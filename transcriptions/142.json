{
  "text": " Bene e benvenuti su Geekbar, nuova settimana e nuovo episodio qua sul nostro bar degli sviluppatori. Ormai le vacanze di Natale ce le siamo lasciate alle porte e siamo pronti per registrare questo nuovissimo episodio che ahimè sarà uno degli episodi più difficili di Geekbar. Abbiamo un ospite, la difficoltà non viene dall'ospite ma viene da me, nel senso che ho mangiato credo il taco più pesante della storia e quindi adesso fare una puntata al microfono con questo taco che mi gira per lo stomaco può essere complesso ma detto questo so che vi interessa poco quello che ho mangiato, vi prometto che cancellerò qualunque tipo di rumore possiate sentire, ahimè l'ospite dovrà subirsi lì. Detto questo io direi che vi ricordo rapidamente i nostri contatti e possiamo iniziare. Info chiocciola Geekbar.it e The Brain Repo su Twitter sono i modi canonici per contattarci, anche quelli che ormai non usa più nessuno. A perchè la mail, no è bella e palle scrivere le mail. B perchè tanto Twitter sta fallendo e non masco meno ci faremo la nostra istanza mastodon. No, abbiamo Telegram, abbiamo il gruppo Telegram, siamo in tanti, siamo belli e belle, siamo freschi quindi se non l'avete ancora fatto iscrivetevi e troverete tante piccole sorprese. Detto questo, anche detto con uno stile che mi ricordo un po' Vanna Marki, io direi che possiamo iniziare, quindi sigla. Benvenuti su Geekbar il podcast dedicato al mondo dei full stack debelter, i mezzo artigiani, i mezzo artisti che ogni giorno infilano le mani nel fango per creare nel modo più efficiente possibile quei prodotti digitali che quotidianamente usiamo. Eccoci qua, eccoci qua, oggi abbiamo un ospite del quale sono super super fiero, l'ho precettato ad Alicante e voi sapete dall'episodio con Michele Riva cosa è successo ad Alicante, no? È successo di tutto e non riusciamo a dare una forma. Era anche lui là con noi, è una delle figure di riferimento dentro Nirform, lo definirei uno dei decani di Nirform. Abbiamo con noi Davide Fiorello, staff engineer a Nirform. Ho detto bene Davide? Sì, ciao a tutti, hai detto benissimo. Grazie Mauro intanto per la presentazione, ho visto che hai ricordato Alicante, ho di recente ascoltato la puntata e ho visto che ha colpito e lasciato nel cuore di molti, di molti, tanti ricordi. Mi compiace molto questa cosa, comunque a parte il meraviglioso evento che abbiamo tutti nel cuore, grazie per l'invito di nuovo e via, vediamo se riesco a essere all'altezza della presentazione che mi hai fatto. Io su questo non ho dubbi Davide, ormai è già da un po' un paio d'anni che ci conosciamo e quindi, insomma, e poi vedo su cosa lavori e come lavori. Abbiamo avuto più di una volta modo di confrontarci e lo faremo anche oggi parlando di un argomento che incendia un po' gli animi, no? Crea un po' di fazioni e parliamo infatti di GraphQL. Qualche anno fa abbiamo fatto una puntata speciale, era il primo compleanno di GitBar, avevamo questo format, se ve lo siete persi andatevelo a recuperare perché è esilarante, lo chiamammo GitFighter ed era un episodio dove io come host tiravo fuori due fazioni, che ne so, tirammo fuori Vim e VS Code e in modo del tutto randomico assegnavamo una fazione a uno sviluppatore che doveva difendere col sangue la posizione e abbiamo visto situazioni dove chi usava Vim doveva difendere Visual Studio Code o chi faceva API REST doveva difendere, chi amava l'API REST barra RESTful doveva difendere GraphQL e là uscì una frase che mi fece particolarmente ridere che diceva così, nel difendere GraphQL si diceva rest in peace per REST, è arrivato la nuova tecnologia disruptive. Davide io so che tu ultimamente hai lavorato pesantemente su GraphQL e so anche che hai lavorato su Mercurius che è una delle librerie JavaScript per fare GraphQL, ricordo bene? Sì ricordi molto bene e sì ho lavorato parecchio su GraphQL di recente e fra una cosa e l'altra ci lavoro da tanti anni, alternata con periodi sì, periodi no, periodi d'odio, periodi di grande amore, adesso sono in un periodo di grande amore fortunatamente e quindi mi sono fatto idee contrastanti, me le sono legate, me le sono sostenute, mi sono un po' tutte, avrei potuto fare una di quelle pintate di cui hai appena citato da solo perché veramente va un po' a periodi, adesso diciamo che dopo tanti anni ho un po' di base per iniziare a, non è insegnabile perché quello ogni volta scopro qualcosa di nuovo, però per iniziare veramente a capire quando sì, quando no, se ne vale la pena, se non ne vale la pena, quindi diciamo che un po' di base me la sono fatta da questo punto di vista. Su quello ci arriviamo brevissimo, però la prima cosa dalla quale vorrei iniziare è una considerazione. Quando per la prima volta approcciamo una tecnologia, in questo caso è GraphQL ma potrei dire la stessa cosa di Rust quest'ultimo periodo, abbiamo un primo momento di spaesamento e se devo portare la mia esperienza, quando per la prima volta ho visto gli schemi di GraphQL, ho detto ma che cazzo è sta roba? Ed era uno dei primi momenti di GraphQL dove la specifica tipo sembrava una ricetta scritta in aramaico, si capiva ben poco, non c'erano tool belli che pronti o erano poco famosi tool come Apollo che poi per un po' di tempo è stato uno dei market leader su questa tecnologia e cazzo mi sono sentito completamente spaesato, ho dovuto ritornarci più di una volta su GraphQL. La mia domanda è, com'è stato il tuo primo contatto, il tuo primo incontro con GraphQL? Allora la prendo un po' lunga, io sono, come hai detto tu, anni che lavoro per mia forma, prima di questa esperienza da Commodore Remote Worker dall'ufficio di casa mia, ho lavorato, ho vissuto due anni in Olanda e ho vissuto e sono andato in Olanda proprio nei due anni della grande rivoluzione del web moderno come lo conosciamo oggi. Arrivo là come sviluppatore angular JS, lavoravo in KLM, l'airline, e il mio tech lead, un pazzo scatenato, decide che dobbiamo usare questa nuovissima tecnologia che si chiamava React, che nessuno conosceva, al che io gli dico, guarda io sinceramente non ho idea di come funziona e lui, neanch'io, però ci mettiamo lì, ce la studiamo e vediamo. Quindi in quell'occasione cosa scopriamo? Che Facebook, che fino alla settimana prima era l'azienda che ci faceva mettere i like alla persona con cui volevamo provarci, perché era uno dei grandi utilizzi di dieci anni fa di Facebook, adesso vai una volta. Penso che lo sia ancora anche se tra i boomer. Esatto, adesso infatti io che sono un boomer ormai non lo uso quasi neanche più, però all'epoca ero un intente molto forte di Facebook e quindi scopriamo insieme che Facebook fa della tecnologia, quindi ho iniziato un po' a seguirlo e quando dopo React, parlo dell'epoca in cui si usavano mixins in React, adesso non so in quanti li abbiano mai visti, e usavamo Fluxore come state management, Redux non esisteva, quindi subito a ruota Facebook è uscito con la presentazione GraphQL, GraphQL che ho iniziato ad usare subito, tant'è che probabilmente c'è ancora il mio nome da qualche parte, nella mia vita a Amsterdam ho aperto il primo meetup di GraphQL, i meetup ad Amsterdam come tutte le città dove ci sono molte start up sono molto vivi, quindi apri con un paio di colleghi questa cosa, quindi il primo approccio è stato lì ed è stato un innamoramento totale, perché era figo, diciamo le cose come stanno, poi soprattutto da persona che ne vedeva solo i pregi, non vedeva i difetti, non avevo in produzione una piattaforma con dei terabyte di dati, avevo semplicemente le mie applicazioni fatte e quelle usavo, quindi si andava di server quello buttato fuori da Facebook, perché Apollo ancora non esisteva, all'epoca Apollo aveva Meteor e hanno avuto un'idea geniale nello switchare verso un GraphQL quando Meteor fondamentalmente era nato, aveva fatto l'esplosione ed era morto praticamente subito, quindi ho avuto l'occasione di lavorarci subito e poi è rimasto una passione, non ho praticamente messo mai niente in produzione, finché dopo sono tornato in Italia, ho iniziato con Airform e la mia prima esperienza lavorativa, che l'ho fatta in Priceline, che è fondamentalmente il padrone di booking, quindi stesso tipo di piattaforma però sul mercato americano, ci hanno lasciato la libertà di scegliere e mi hanno preso nel momento in cui io volevo mettere GraphQL, perché avevo bisogno di provarlo da qualche parte, ho fatto un po' questa follia da un cliente di queste dimensioni. Minchia coraggioso! Ah sì, io sono uno che ha la testa bassa, io se sto sul progetto non ho paura a fare scelte, se devo fare scelte per altri sono sempre molto più tranquillo, ma se sto sul progetto mi prendo proprio le mie responsabilità senza problemi, quindi andiamo là, nel frattempo era già uscito Apollo, e quindi abbiamo messo in piedi la prima applicazione, ti parlo di sei anni fa ormai, la prima applicazione con un serverino, non era particolarmente complesso ma nell'interfaccia ci ha aiutato tantissimo. All'epoca la complessità era nel front-end, ultimamente ho nominato Mercurius, i miei progetti recenti sono molto più sul front-end. Poi su Mercurius ci andiamo perché ho avuto modo di sfagiolare un po' la codebase ancora prima di un refactoring recente che è stato fatto, io gli avevo fatto un po' di cosettine su Mercurius un po' di tempo fa, quindi poi ci andiamo e parliamo di Mercurius, che è un progetto del quale andiamo molto proud a livello aziendale. Assolutamente sì, tra l'altro per rispondere alla vera domanda che mi hai fatto, cioè come mi sono approcciato, tu hai detto che ti sentivi un po' spaisato, io raramente mi sento spaisato nelle tecnologie, ma non perché sono un figo e le capisco subito, perché non capisco un cazzo fondamentalmente, e io vedo solo ed esclusivamente le parti buone e dopo una settimana che ci sono sopra esordisco con la frase che chi mi conosce benissimo sa che la dico ogni 3 persone, che è in questo caso GraphQL, GraphQL per me non ha più segreti, e allora quando dico questa frase vuol dire che in quel momento inizio a capire qualcosina e da lì i dubbi che non avevo, perché fino a quel momento non ne avevo, saltano tutti fuori e poi me l'ho avuto con tutto, abbiamo parlato in pre-meeting di MongoDB, ma con MongoDB è stato lo stesso identico approccio per me, poi dopo ho trovato i vari difetti che aveva, trovo comunque che sia un gran prodotto, ma all'epoca ci facevamo anche il caffè con MongoDB, quindi questo è un po' l'approccio, è il mio approccio alla tecnologia generalmente, a volte sono un po' più coraggioso e la mando in qualche produzione, a volte invece sono un po' più cauto e me la tengo per me, faccio un po' l'evangelistico alle persone che non la conoscono elogiando senza avere poi fondamentalmente delle basi per elogiare, adesso un pochino le ho, ma all'epoca decisamente non c'erano. Sai Davide, hai aperto una cosa che secondo me potremmo per un attimo trattare come una parentesi, ma anzi ti dirò, mi hai dato un'idea, per il prossimo episodio preparo proprio una musichetta che è la musichetta delle parentesi, the endless parentesi, anche con un jingle. La domanda è, tu hai detto, talvolta io mi butto e in produzione magari ci tiro dentro una tecnologia innovativa, anche io l'ho fatto, non l'ho fatto con la tecnologia, l'ho fatto con un, non so, una metodologia, un pattern chiamiamolo così, col functional ddd e tipo mi sono preso dei calci in culo pesanti che venivano da me, venivano dalla complessità che si è inserita nell'applicazione, con l'inserimento di sta roba e quindi mi dico, talvolta siamo portati a spingere una tecnologia senza essere profondamente consapevoli di quella tecnologia, che domande ti fai per non essere vittima di questo entusiasmo? E tipo io me ne faccio alcune e sono curioso di vedere un po' come gestisci tu la cosa. Allora dipende molto dall'entità di quello che butto dentro, ok? Cioè provo a fare un passo indietro. Ti riporto il discorso GraphQL che ho buttato dentro il mio progetto in produzione. La valutazione è stata fatta considerando che non era un progetto ad alte performance, che richiedeva alte performance. Quindi io non lo sapevo quanto GraphQL fosse performante bene. Sicuramente non era una roba che richiedeva delle performance, perché avevamo un, noi gli italiani chiamano FF, front end for back end, era un FF, front end for back end. Quindi era un servizio che chiamava delle API dietro, lentissimi, perché immaginati tu vai a prendere dei dati molto complessi, noi facevamo i pacchetti, quindi tu dicevi dove devi andare, lui automaticamente ti faceva aereo, albergo, automobili. Quindi parlava con Amadeus che era un chiodo di AS400. Sì, non ho idea di cosa ci fosse dietro. È stato l'unico progetto, uno dei pochissimi progetti della mia vita in cui non ho usato un database, perché semplicemente interrogavo API e questi mi davano dei risultati. Quindi la nostra scelta è stata fatta con la coscienza del proviamo, vediamo come va, e poi male che vada io e il mio collega dell'epoca, anche lui di NearForm che eravamo sul progetto, ci siamo guardati in faccia e abbiamo detto se poi non va a tirare su delle rest API per gli altri giorni. Anche il servizietto era già pronto. Ma sì, non è che ci fosse chissà che. Più che altro il vantaggio di partire con GraphQL in quel caso è stato talmente forte che abbiamo deciso di giocarci la carta. Perché non fare delle API per un sistema così complesso? Perché ne avevamo una vagonata di dati, una marea. Quindi erano tre chiamate, quattro chiamate, ma con delle strutture enormi. Quindi allora tanto vale che ci provi e andate bene. Perché alla fine ci è tornato molto bene, non avevamo mai usato Apollo sul frontend e alla fine Apollo ha fatto il suo dovere, alla grande. È stata sicuramente una soluzione che ha pagato. In questo caso ero tranquillo perché sapevo di stare qualche mese sul progetto. Io quando le prendo un po' alla leggera queste cose, quando so che poi ho il tempo di mettere delle pezze se ci sono dei problemi. E non ho paura a dire ho fatto una cazzata, non ho paura a dire adesso scusate datemi una settimana per rimediare a quello che è stato fatto. Alla stessa maniera, come raccontavo all'inizio, noi scegliamo React per fare questo progetto in KPLM quando tutta l'azienda usava Angular. Quindi noi proprio, Valespa, Valdi... ed è stata una scelta giusta perché alla fine abbiamo visto dove era giusto. C'è forse anche da dire questo, nel senso che con un po' di esperienza ce l'abbiamo e sappiamo quando una cosa può funzionare o quando non può funzionare. A puzza di merda la sentiamo da lontano. O almeno abbiamo la presunzione di sentirla da lontano. E proprio in funzione di questo ti chiedo, dalla tua esperienza, quando ha veramente senso usare GraphQL e quando invece il tutto è un po' spinto dall'abbrivio dato dalla moda, dal trend? Allora, ormai direi che trend non è più trend, perché ormai è una tecnologia stabile e funzionante e che tante aziende richiedono. Una cosa che sicuramente mette un punto a favore di GraphQL è la tipologia di applicazione che uno va a fare. Noi lavoriamo, lo dice anche nella sigletta, siamo dei full stack developer che lavorano nel mondo del web, perché noi non siamo più dei full stack developer che fanno delle desktop applications. Noi lavoriamo in un mondo in cui imbarba, ricito una cosa che abbiamo discusso di recente in un altro accolo in inglese che citavamo prima, imbarba a tutti i progetti, questa cosa la si fa così, tempo perso a fare dei grafici e a decidere come fare il database, il software web moderno lo decide il designer che sia un produttore. Punto, non giriamoci attorno, c'è un produttore, parlo ovviamente di una situazione semplice, poi quando si parla di grandissime applicazioni ci sarà un più produttore, un più designer e cose diverse, però un'applicazione che uno commissiona e che uno decide di fare, che fa una certa cosa, ha un designer e una serie di designer che fanno le cose e un produttore che li guida, che ha in mente una feature e semplicemente loro mettono su una feature. Partendo dai design si fa il resto, perché così alla fine si fanno, almeno nelle mie esperienze penso che… No, si fanno le cose. Sì, cioè io penso di aver avuto la fortuna, lavorando in Myrford, di aver avuto la fortuna di lavorare in tanti progetti molto interessanti, clienti, clienti e clienti interessanti e questo è stato il sistema sempre, ovvio che devi avere un'idea di quello che c'è sotto, devi sapere più o meno dove si andrà, di come gestire i dati, certo, ma alla fine il fatto che a un certo punto appare una label con un valore, dal giorno alla notte ti arriva. Quindi GraphQL da questo punto di vista è fenomenale, perché tu soprattutto in fase di prima creazione, mantenimento, sul mantenimento potrei parlare male di GraphQL però… No, ma poi ci arriviamo perché mi sono appuntato un punto proprio su quello. Bravo, però sulla fase di progettazione, sulla fase di progettazione, GraphQL è fenomenale, ogni tool che ci sono in giro, che velocemente da un database o da una struttura dati ti creano, parto dal platformatic di Matteo Collina, cito anche Asura, quindi sono due tipologie di tool che più o meno… Strapi… Strapi per esempio non lo conosco, che però più o meno, però sì, immagino che anche Strapi faccia la stessa cosa, perché lui funziona comunque forniscendo l'API, tu hai la possibilità veramente in maniera veloce di tirar su un servizio che va e con le varie librerie che ti si attaccano a React rendono veramente velocissima la prototipazione, è una roba, è impressionante fare il prototyping in GraphQL, è molto veloce, perché tu non ti devi curare di niente, tu devi semplicemente buttare dentro dei dati, organizzarli, strutturarli, perché bisogna fare le cose per bene, non bisogna prendere una spadellata di attributi e buttarla dentro, cioè, c'hanno le cose per bene, però ti permette intanto di definire già, cioè il bello di GraphQL è che ti permette già di definire la struttura dati, e quando l'hai definita sei a metà dell'opera, perché col sistema di resolver e tutte le cose che hanno, tutto funziona meravigliosamente, poi problematiche di performance, avremo l'intera puntata per descrivere queste cose, ma per partire da zero, sicuramente, che sia una piccola applicazione, che sia una grande applicazione, poi adesso con la federazione, con quello che ti permette di fare la federazione, è tanta roba, cioè si fanno grandi cose. Si, hai detto, ah scusa, vai, vai. No, volevo dire, semmai la riprendiamo dopo, perché questo apre una voragine. Quando io ho parlato con persone e mi dicono, ah ma GraphQL, così, poi quando ci parli per bene, quando ti siedi con loro, quando gli fai vedere la semplicità con cui si fanno certe cose, l'affermazione da parte loro spesso è stata, ah, non avevo capito. Poi non sto dicendo è la migliore tecnologia del mondo, rest funziona da Dio e io le uso indipendentemente tutte e due, perché poi non sono sempre io a scegliere, a volte c'è chi dice no, preferisco stare su quella, beh, allora restiamo su quelle tecnologie. Vero, prima hai detto una cosa interessante sulla quale voglio costruire una piccola analogia. Tu hai detto, le UI, comunque i prodotti, anzi mi correggo, i prodotti sono fatti dal produttore, ok, e dal UX, il designer. Io ho detto designer, ma con designer intendevo tutto quel messione via ruoli che fanno questo. Esatto, e spesso questo non succede nei prodotti digitali, forse perché l'industria non è ancora abbastanza matura, forse perché abbiamo un po' un approccio cinofallico, come diceva qualcuno, spesso perché i tempi sono quelli che, e spesso perché da sviluppatore abbiamo la presunzione di dover dettare una riga, no, di dire no ma questo non si può fare, posto che quello che mi disse il mio produttore è la prossima volta che mi dici non si può fare ti prendo a calcino il culo, nel senso che tutto si può fare, dipende sempre da budget, tempo e risorse, però al di là di quello un po' è quello che succede nel mondo, cioè nel senso che il mio fratello è un architetto, il 90% del mio fratello è da fare i progetti, avere più o meno idea dei calcoli e della parte strutturale, poi lo dà in mano a un ingegnere e gli dice, come, vi faccio questi, prova a costruirvi queste immagini, prende un pezzo di carta, lo appallottola e dice voglio questa struttura del cazzo tutta strana, un po' esceriana, ok, lo dà a un ingegnere e gli dice bene fammela stare in piedi, ecco, dobbiamo un po' ricalibrare il nostro modo, noi dovremmo essere quelli che al ecco fammela stare in piedi, dobbiamo saper dire sì, io te la faccio stare in piedi, questo è quello di cui ho bisogno per fartela stare in piedi, non, non si può fare. Detto questo, aggiungo un'altra parte ricollegandomi al fatto dello schema che hai evidenziato, sto per lanciare una bold opinion sul mondo JavaScript, quindi Davide, so che tu sei un amante di JavaScript, però secondo me la vera innovazione di GraphQL è stata quella di dire, ehi cari amanti di JavaScript, guardate che i tipi e la struttura dei dati che gira all'interno della vostra applicazione non è poco importante, è molto importante, probabilmente è centrale nell'applicazione, quindi secondo me la cosa che ha portato GraphQL è anche una certa disciplina sulla struttura dei dati, in un mondo e in un'industria dove i dati venivano strutturati ad katsum perché bisognava release fast e break things. Cosa ne pensi in merito a questa affermazione? Hai detto che facciamo un'altra puntata JavaScript contro TypeScript? Invitiamo anche il nostro comune amico Matteo in quel caso, ti ricordo una puntata dove Matteo disse che TypeScript era una proiezione mentale del tuo io digital, qualcosa del genere, cioè tipo TypeScript non esiste, sta solo sulla tua mente e dimenticalo! Hai assolutamente ragione, scherzi a parte, secondo me è stato un passo molto importante, poi un passo che generalmente gli sviluppatori un po' attenti già facevano prima, perché io quando lavoro con Fastify non c'è Empoint che non abbia uno schema davanti che mi fa la validazione, diciamo che la validazione e la sanitizzazione dei dati in input è un good practice che va fatta appunto, senza tanto ne quanto sia in entrata che in uscita e l'aver messo nero su bianco il contratto è un passaggio molto importante, nell'unico punto da sviluppatore JavaScript in cui questa cosa è essenziale, la comunicazione con l'esterno, dove tu i dati non li controlli, tu non sai che cosa ti entra, poi in realtà li puoi sanitizzare, puoi validarli, puoi fare un sacco di cose, tra l'altro schema non è che GraphQL permette di fare anche mille validazioni, permette di fare tante cose, su quelle directing fondamentalmente fai tutto quello che fai anche sempre, però è di base, è naturale, ha una dipendazione molto basilaria, che però fa, come dici tu, detta delle regole, detta delle regole che lo sviluppatore JavaScript, in cui motto spesso è quick and dirty, più dirti che quick tavola, ho delle robe nei miei software in giro che veramente fanno i miei ribrivi, la validazione, la stabilizzazione della struttura delle cose si fa con i test, però anche li puoi bisogna saperli fare, fare delle porcate anche con gli unit test è un'altra, e quindi questo è un po' l'approccio, concordo pienamente, ha dato finalmente un linguaggio per comunicare correttamente fra front-end e back-end in maniera buona, questo è essenziale. Facciamo una riflessione, magari mi sbaglio, però provavo a pensare al concetto di business logic, non mi è chiara questa cosa nella mente, quindi te la dico come mi viene e poi ci ragioniamo insieme. Adesso buona parte delle nostre applicazioni sono, come dice Carmine, che a me oggi non c'è, sono il crudino della chiesa, o il crudino dell'azione cattolica, cioè quattro end point che ti fanno il get, il post e fanno il crud, fondamentalmente. Le nostre applicazioni sono piene di crudini, grazie a Dio esistono tool come platformatic, come azura che un po' ci risolvono il problema con i limiti che hanno, però comunque è una roba un po' da programmare, un po' da mechanical turkey. D'altro canto però ci sono dei pezzi di business logic che invece hanno una certa complessità, dove abbiamo certi dati che entrano, abbiamo delle dipendenze esterne che è un database e dei dati risultanti che non necessariamente fittano 100% il database. È da un po' che mi chiedo, in casi dove la business logic è fortemente presente, tu hai parlato di pachettizzazione, di booking, che sono dei casi dove in realtà il calcolo della availability di una certa struttura presuppone un'importante business logic, ecco GraphQL ci può venire d'aiuto, te lo dico perché adesso sto scrivendo un motore di booking e mi sto ponendo proprio questa domanda, mi sto chiedendo, gli end point sono tre end point cagati, ma GraphQL mi può aiutare in qualcosa dove la business logic è importante? Sì, assolutamente, non il linguaggio, cioè non GraphQL inteso come linguaggio, perché tu comunque lo interrovi sempre nella stessa maniera. Gli engine su cui GraphQL gira, con il sistema dei resolver, il sistema dei tipi che vengono forniti dal sistema, permette di fare delle cose molto interessanti, poi c'è un po' di tooling da usare insieme per gestire i problemati di reloading, di caching, tutta una serie di cose. La cosa veramente secondo me figa di GraphQL, da questo punto di vista, è che tu il singolo dato puoi andare a mettere la logica nel singolo dato, ok? E il tutto ti viene tenuto in ordine dal sistema, cioè il sistema ti permette di dire che il prezzo del prodotto X è calcolato seguendo delle logiche e tu puoi andare in quel punto a pensare esclusivamente a quel punto, non devi fare dei salti mortali, non devi andare a organizzare tutta una funzione di 50 righe perché devi ritornare a una struttura che poi verrà ritornata dalla resta. Tu ti preoccupi solo di quello. Ovviamente qualcuno potrebbe dire, eh, ma se tu devi andare a caricare dei dati lì che poi ti servono anche dall'altra parte, con il resto li carichi una volta e li usi dappertutto. Questa cosa con GraphQL si fa nella stessa identica maniera, ci sono dei sistemi, tu hai modo, hai modo tramite i data loader di avere un unico punto d'accesso per il caricamento dei dati, questa cosa ti permette fra l'altro utilizzando anche il sistema di caching un po' di fregartene dell'andare a pescare un dato. Quante volte tu vai a pescare i dati oppure li metti nella query, scusa, li metti nel codice perché non vuoi andarli a richiedere tutte le volte, con un sistema di caching ben fatto. In realtà poi il sistema di caching non fa neanche il resto. Diciamo che GraphQL ti forza un po' a fare certi shape, questo non è un vantaggio perché alla fine è una forzatura, però ti abitui un po' a lavorare con queste cose e quindi faccio un esempio, una tipologia prodotto, tu prendi il prodotto, il prodotto te lo restituisce solo lui di della tipologia e tu hai 20 tipologie in tutto, non ne hai di più, parliamo di un caso abbastanza ristretto. Cosa fai? Vai a chiedere la singola tipologia al sistema? Sì, gliela vai a chiedere, poi te la cachi, tanto la tipologia del prodotto è una roba che non cambia, quindi è una roba che può stare tranquillamente, puoi caricarlo ogni minuto, ogni minuto puoi fare di nuovo la richiesta. Quindi questa cosa ti fa benissimo il resto e la fai dappertutto. Con GraphQL tu puoi tranquillamente nel tuo resolverino fare questa cosa fregandotene delle performance perché tanto un sistema di caching o di preloading o di cose fatto per bene ti dà tutta questa automazione e ti permette di farlo. Poi se devi fare invece una roba complessa perché devi fare dei calcoli e cose così, alla fine è una funzione come un'altra, non è che ci siano delle cose speciali, cioè non ti facilita. Tu hai un ingresso e dei resolver, punto, hai dei dati e ogni dato ha un resolver. Questo resolver ti restituisce uno scalare se uno scalare, ti restituisce una struttura o qualcosa del genere se all'interno di questo attributo è un altro oggetto e così è cascata, tu puoi fare le cose e avere dei risultati che nella mia esperienza funzionano. Poi se tu fai un node che restituisce un JSON statico è più veloce. Tieni presente che devi anche stare attento a cosa usi per serializzare il JSON. Ti apro una parentesi, in un progetto in passato avevamo un Postgres che aveva una colonna JSON, un record che aveva una colonna JSON. Il problema è che una colonna JSON, quando è di 300k, 400k, è un JSON da 300k da deserializzare. Quindi un robo che noi prendevamo e così com'era, lo deserializzavamo e lo mandavamo al frontend senza farci niente di quella deserializzazione, ci creava un sacco di problemi, perché con la deserializzazione costava 50ms. Per uno non è un problema, ma quando tu questo JSON lo carichi, sono 50ms di processore, non sono i 50ms di await. La deserializzazione di JSON e la serializzazione di JSON è sempre facile e molto attento. Noi ci andiamo molto rapidi là sopra, però in azienda ne abbiamo viste di specie quando le performance contano. Sono questi poi i casi in cui veramente si va a fare le pulci su tutto. Però voglio ritornare sulla complessità, perché quello che hai detto è molto interessante, cioè il fatto di concentrarmi nel caso in cui la business logic è importante, sul micro dato all'interno di un albero di dati un po' più complesso, è importantissimo perché ci permette, volendo, di ignorare il contesto. Il contesto, come dici tu, al quale possiamo risalire facilmente, che ne so, risalendo il nodo superiore o andando ad accedere a un context della situazione, però nel contempo io mi concentro su quell'elemento. E una delle rivoluzioni poi in realtà di GraphQL è stata quella di spostare parte di questa complessità di business anche sul frontend, perché il fatto di poter chiedere, quindi parte di questa business logic sul frontend, il fatto di poter chiedere al database, al backend, a me servono questi dati, poi ci penso io, tu non ti preoccupare che ci penso io client side che non ti costa niente a te, vada bravo, dammi i dati come mi servono. In questo approccio quali sono secondo te i vantaggi più grandi e quanto invece i calci nel culo che ne derivano da questa approccia? Parto dagli svantaggi e lo svantaggio l'hai detto tu nella frase dalli a me che a te non costa niente, che è la cosa peggiore in assoluto che uno possa pensare con un server GraphQL. Si tirano giù i server in questa maniera e non sto scherzando. E volevo portarti a parlare proprio di quello. Torno al mitico talk presentato all'epoca da questa donna, non mi ricordo come si chiama, ma è uno degli altri ingegneri di Facebook dell'epoca, non so se lavori ancora per loro, comunque, e ricordo che presentava questa cosa meravigliosa in cui ognuno decideva che dati prendere e quindi tu in quella richiesta hai bisogno di nome, cognome e numero di telefono, nell'altra richiesta hai bisogno anche dell'indirizzo e quindi chiedi l'indirizzo e ti restituisce l'indirizzo. Questo sarebbe il modo perfetto, indipendentemente da cosa costa andare a prendere questi dati, che ne parliamo dopo. Nella pratica le cose non funzionano così. Nella pratica, ho visto abbastanza codice front-end per dirtelo, funziona che io ho la mia entità prodotto, di questa entità prodotto mi faccio un bel fragment che mi restituisce tutti gli attributi del prodotto. Il fragment, per quelli che non lo sapessero, è una sorta di... è il modo che ha GraphQL per definire un seme di attributi e utilizzarli in vari punti in modo da non doverli riscrivere tutti. Possiamo dire un template per porzioni di query GraphQL giusto per semplificarlo. Esatto, c'è una cosa di questo tipo e quindi lo sviluppatore di turno una volta che si è fatto quel fragment, fregandosene di tutto, lo usa ovunque. E quindi hai un fragment che in un punto ha dieci campi di cui ne servono sei, nell'altro ha sempre dieci campi ma di cui ne servono cinque e solo uno di questi è comune perché gli servono quegli altri. E questa è una roba comunissima, le codebase sono piene di casi di questo tipo, che il 50% delle volte non danno problemi, a parte il traffico, non sto neanche considerando il traffico, però il 50% delle volte non danno problemi. Però tu non lo sai perché quello che è un nome, cognome, indirizzo e l'indirizzo dici ma che cosa vuoi che costi fare l'indirizzo, tu non lo sai che cosa sta succedendo là dietro. Potrebbe essere che là dietro c'è qualcuno che per prendere questo indirizzo deve chiedere un servizio esterno perché semmai ha un ID di questo indirizzo. Adesso l'indirizzo ho fatto un caso un po' che è raro, di solito sta dove sta il nome e il cognome. Io ho visto i pi grafici che chiamavano Google per fare reverse geocoding di robe, quindi ho visto fatture di Google esplodere. Sì, perché poi quando dai il potere in mano agli sviluppatori questi lo usano e spesso io, già la parola full stack che hai nella presentazione potrebbe scatenare giorni e giorni di discussione. Ma sta là per quello amico mio. Secondo qualcuno i full stack non esistono, secondo me è perché non li avete mai visti i full stack, perché io di full stack ne ho visti tanti e ho lavorato con tanti full stack. Non vuol dire che sei un fenomeno ovunque, ma non sei un fenomeno neanche se sei un frontender, perché ci sarà sempre quella parte del frontend che conosci di meno e così vale anche per i full stack e vale per i back end. Io che ho avuto la fortuna negli anni di lavorare un po' a destra, un po' a sinistra, o a sopra e sotto, prendendo i front e i back, quando io lavoro a front end faccio molta attenzione a che cosa vuol dire il dato dietro. Non li vado mai a prendere con leggerezza questi dati. Purtroppo chi non ha esperienza dietro e poca esperienza in generale, perché poi quando i front end di esperienza sanno benissimo il costo dei dati, anche se non si occupano loro di andarli a prendere, spesso dicono ma si vado a prendere poi ci penserò e queste ha dei costi trementi, serializzazione e serializzazione, richiesta di database. Se tu hai un DynamoDB dietro, che costa a richiesta, è ovvio, se tu hai un DynamoDB è molto cheap se tu fai poche richieste, ma se la tua query invece ne fa 5 e di qui ne hai 500.000 all'ora, iniziano ad essere dei soldi. Avere 5 e avere 10 fa la differenza. Quindi ci sono delle attenzioni a cui bisogna stare attenti. Si sbaglia tutti, io per primo faccio del castronato, ogni tanto, che è veramente sbattere la testa contro il muro. Però sul costo dei dati, il costo inteso come che cosa vuol dire andare a prendere, GraphQL da questo punto di vista non lo nasconde. Dandoti la possibilità di fare picking ti dà spesso il diritto di prendere. Se tu chiami una REST API, ti frega, tu chiami una REST API, sai già che tanto i dati verranno ristituiti tutti. Questo è un altro dei grandi vantaggi di GraphQL, che spesso ci sono nelle REST API. Quante volte hai usato delle REST API per il parametro full response o qualcosa? Si, o complete. Allora usa GraphQL. Nel momento in cui devi dire che tipologia di dati vuoi dietro, usa uno strumento che è fatto per queste cose. Si, assolutamente. Domanda invece, perché poi tra un po' voglio arrivare a un punto saliente sul quale hai lavorato pesantemente e voglio trattare perché è una delle cose che di GraphQL si conosce poco, è male e invece secondo me ha senso approfondire. Però prima voglio farti una domanda. Apollo. Come vedi la posizione Apollo nell'ecosistema GraphQL e soprattutto a questo punto in che modo Mercurius ribalta i giochi? Allora, partiamo da Apollo. Partiamo dal fatto che è un po' che non uso Apollo, però leggendo un po' in giro e soprattutto essendo dalla parte di Mercurius sicuramente c'è una differenza di performance. È un sistema più pesante per come è fatto, per come... non le so le motivazioni esatte. Conosco molto bene Mercurius, so che è fatto da un maniaco delle performance e quindi a questo punto di vista funziona molto bene. Apollo è il leader del mercato. Non giriamoci attorno, è nel mondo JavaScript e GraphQL lo fa in JavaScript perché è fatto per essere fatto in JavaScript. Io se devo pensare di fare un GraphQL con un C Sharp o con un Java, con tutto quello che comporta la tipizzazione, non è tanto la tipizzazione perché TypeScript è tipizzato, però ti dà delle libertà TypeScript che anche se è tipizzato tu riesci a essere molto più veloce a fare code. C Sharp, dico C Sharp perché durante le vacanze di Natale, nelle mie due settimane di vacanze, ho lavorato per Unity a full time. Sto facendo un giochino per i fatti miei perché mi rilassa e quindi ho dovuto lavorare molto in C Sharp che non conosco. L'ho programmato all'inizio degli anni 2000, ma ovviamente adesso ho un altro linguaggio. Quindi queste complessità nel fare le cose più banali, nel caricare un JSON, capisci? Io il JSON lo carico e lo uso in JavaScript e in TypeScript. In C Sharp devo fare un serviziatore, una serie di robe che vanno fatte e poi verrò smentito da gente che C Sharp lo conosce, perché io c'è quel fatto, non lo conosco. Però così a naso mi viene da dire che GraphQL è nato per essere fatto dal punto di vista di comodità in JavaScript. Anche perché il 90% è consumato da JavaScript. Esatto, in Rust è sicuramente più performante. Però detto questo, mi sono perso, dovrebbe che non mi ricordo più cosa stavo dicendo. No, la posizione di Apollo. Nel mondo JavaScript Apollo è dominante, è utile che ci geriamo attorno. Se tu vai sui trend di download, si vede la differenza. Sono due prodotti diversi. Diversi dal punto di vista dell'utilizzo. Poi Apollo è amato ovviamente da chi entra nel mondo Node, perché se fai una ricerca ti viene Apollo e il mondo Node. Anche perché è un prodotto commerciale supportato da... Tutta una serie di cose. Però, adesso butto l'acqua un po' al mio di mulino. Se dovessimo usare un server, a questo punto useremmo Express se ragionassimo con questo sistema. La differenza è che Apollo ha un'azienda, dietro Express no. Per quanto sia proprietario dell'IBM, fra una serie e tutta una serie, che l'hanno vinto a carte giocando non mi ricordo qual è il motivo. Adesso Express dovrebbe essere sotto l'ala dell'IBM. Non so proprietà, non proprietà, non so. Però a un certo punto mi sembra che fosse sotto un prodotto che l'IBM ha comprato. Comunque, dettagli. Però se noi dovessimo fare un server useremmo Express da questo punto di vista. Quindi invece Fastify, che se non conoscete la community di Fastify, adesso a parte Matteo e io... La conoscono, la conoscono. Rompo le balle io. No, è un prodotto fenomenale dal punto di vista della community. Cioè è una cosa mostruosa. Mercurius non è attivo come Fastify perché è più di nicchia in generale come prodotto, perché Grafico L è più di nicchia rispetto a un server rest. Poi per usare Apollo c'è Fastify sotto... Scusate, Mercurius c'è Fastify. Non è che non usate Fastify se usate Mercurius. Però è un prodotto ben supportato. Fatto bene, fatto che funzioni. Ha i suoi bug, ma ce li hanno tutti. Però una volta che si trova un bug poi si va a correggere. Io l'ho usato in produzione in un progetto molto grosso, siamo contentissimi. Il cliente è stato contentissimo. Un progetto complesso, federazione, con dieci nodi. Non sto parlando di un serverino che serve tra entità. Parliamo di una roba... Abbiamo fatto una migrazione da Apollo a Mercurius e il passaggio... Abbiamo tolto due cose, Sequelize, che era una versione vecchia, e Mercurius. Sequelize e Apollo. La rimozione di Sequelize, che era una versione vecchia che usava ancora Bluebird. Oh mio Dio. Esatto. Tappava non poco. Il passaggio Mercurius, che ci ha permesso di andarci un po' più spinti con il sistema di cache, con queste cose, ha fatto la differenza. Il sistema era oggettivamente più veloce, ma molto più veloce. Da questo punto di vista io voto Mercurius. Se si devono sentire più sicuri, avere un'azienda supportata che vuole un servizio di pagamento di Apollo, loro te lo danno. Quindi si può fare. È anche più supportata dal punto di vista del tooling di Apollo, c'è molte più cose. Il tooling fra l'altro è uno dei grandi problemi di GraphQL. Se vuoi dopo parliamo di che cosa vuol dire secondo me lavorare in GraphQL. No, io so su cos'hai lavorato e quindi voglio arrivarci su quello. Perché tu hai ultimamente lavorato su Mercurius anche in progetti abbastanza grossi. È uno dei topic... hai lavorato anche su il core di Mercurius se non mi sbaglio. Io ho fatto refactoring. Io ho estrapolato il Gateway e la Federale. Hai diviso il gemello Siamese. Ho diviso il gemello Siamese. Hai tirato fuori tutta la parte di Federation perché era strettamente accopiata a Mercurius. Sì, era un unico progetto. Adesso se vuoi usare la Federazione e vuoi costruire un Gateway, devi usare un modulo a parte. Il codice è quello, non è che l'ho riscritto. Soprattutto sulla Federazione, come ne male, è quello. Sul Gateway l'ho estratto e ho dovuto fare un po' di ritocchi perché non era sufficiente estrarlo. Cioè ci sono andato un po' a fondo, ma è stato molto utile perché questo mi ha permesso di andare nel core. Infatti dopo che l'ho fatta, che sapevo esattamente che cosa succedeva lì dentro, ho iniziato a prendere a mano un po' di issue aperte sulla Federazione e a chiuderle perché poi sapevo esattamente che il problema era da risolvere. Dopo ho iniziato a chiudere un po' di task. A fare un po' di pulizia. Tra l'altro io ci ho lavorato per un progetto, ci tengo ancora a ricordarlo, era cercare di utilizzare Mercurius come Gateway per diverse API, tra cui un API non federata di Strapi, quindi probabilmente conosci quel ticket che hai girato in azienda per un po', forse è il nostro amico Simone al quale mandiamo un grande abbraccio, è stato l'ultimo a cui ci ha lavorato, è stata un po' una follia. Però di lì io per esempio ho approcciato al mondo della Federazione GraphQL. Ce lo vuoi raccontare in breve? Allora io l'ho scoperto in questo ultimo progetto che ho fatto, molto grosso, non sapevo neanche certo poi come si poteva fare perché io era poi un po' che non lavoravo con GraphQL, perché appunto lavorando nella consulenza e lavorando un anno, un anno e mezzo sui progetti finisci su un progetto che non lo usa e tu semplicemente sei fuori completamente da quella consulenza. Quindi per me era una novità abbastanza, quindi ho avuto occasione di rimetterci le mani e la federazione sì ne avevo sentito parlare ma non l'avevo mai fatta. Che cosa vuol dire fare la federazione? Vuol dire che tu invece di dichiarare un unico schema, un server, tu hai ogni server, chiamiamolo service visto che è il termine utilizzato, ogni service ha il suo schema, qualcuno di questi oggetti nello schema hanno delle chiavi dichiarate esterne o delle entità che sono dichiarate come esterne e che lui si limita a sapere che esistono, nulla più, e a eventualmente fornirne l'identificativo e poi c'è un sistema di gateway che altro non fa che interrogare tutti questi servizi che tu dichiari all'inizio e lui uno per uno prende questo schema, lo mergea in un'unica grande schema e fa vedere all'utente finale un'unica macchina che lo usa appunto come se fosse un'unica macchina. E queste cose ti permette di fare grandi cose, ti permette di avere un bel storm di nodi su Kubernetes per esempio in cui i due servizi che hanno bisogno di potenza hanno 24 o 36 nodi e il servizetto che non viene usato poca niente, ogni tanto, gli mette solo uno senza dover necessariamente replicare un'unica macchina con una serie di vantaggi mostruosa, perché di nuovo quando si parla di progetti di un certo tipo si parla di team, non è che tu lavori su un prodotto, non è che se tu vai, cito Booking perché l'abbiamo citato prima, non è che Booking è un team dietro, Booking avrà 10 team, 15 team, 20 team che fanno cose. Io quando lavoravo in KPLM, la home page di KPLM all'epoca ci saranno stati almeno 6 team, 7 team che lavoravano per buttare fuori quei dati di quella home page, fra back end, fra front end, c'era un gruppo che faccia solo l'header, c'era tutta una serie di cose e questa è la normalità, quindi nel back end avere la possibilità ognuno di lavorare sul proprio porticello fa una bella differenza e non è solo una questione di dimensione del progetto. Quando tu hai 10 team di 3 persone, 1, 4, stiamo sul piccolo back end, quando tu hai 3 per 10, 30 persone che lavorano sulla stessa home page, poi puoi fare le merge, si diverte, c'è tutta una serie di lanciare i test di una code base in cui lavorano 10 team, aspetti un'ora e mezza per finirne tutti, quindi ci sono tutta una serie di vantaggi nello spezzettare e quindi la federazione ci dà quello che è fondamentalmente un sistema di microservizi su GraphQL. Mercurius la fa bene, è ai livelli di completezza di quello di Apollo? No, non lo è, perché l'hanno inventata quelli di Apollo, cioè le specifiche della federazione. Mercurius non implementa la federazione, implementa la federazione V1 di Apollo, quindi l'hanno inventata quelli di Apollo. Ero finito delle specifiche per capire come cazzo funziona, ed è vero che stavano nel sito di Apollo. E ci sono cose, per esempio c'è qualcosina che non viene implementata, serve, io non ne ho avuto bisogno, le due volte in cui avevo bisogno di fare una roba che non era implementata, ce l'ha sia implementata. No, no, no, ci ho girato attorno senza problemi, ancora all'epoca non avevo questa conoscenza della code base per metterci le mani, adesso probabilmente andrei a sistemare le cose. Però, per esempio, Mercurius fa la federazione sulle subscription, che Apollo non faceva, adesso non so se nel frattempo l'hanno implementata. Però tu puoi fare federazione sulle subscription usando Mercurius. Rapidamente, giusto per chi non lo sapesse, quando si parla di GraphQL si parla di query e mutation. Con le query accedi ai dati, con le mutation istruisci il server GraphQL per fare delle azioni. Non sto dicendo con la zappa, però fondamentalmente questo fanno. In più c'è un terzo elemento che è un query, possiamo vederlo come un query, che in qualche modo parla attraverso un altro protocollo, possiamo immaginarlo come WebSocket, e qui ti permette di rimanere in ascolto di qualcosa che poi ti arriva e non è strettamente legato a un processo di richiesta risposta. L'ho detto con la zappa, Davide. Esatto, è un sistema di push che si usa con GraphQL. Tu ti metti in ascolto e lui ti manda i dati quando questi arrivano, quando vengono modificati. Esatto, è molto figo, devo dire che io l'ho utilizzato. Non mi è, in tutta sincerità, ancora ben chiaro come farlo scalare, però su quello potremmo aprirci un capitolo. Io non ho una grande esperienza su quelle, perché al solito, non so, sai un po' la teoria, ma poi i progetti in cui l'hai usato non l'hai usato e quindi… Esatto, non l'ho visto utilizzato in tanti progetti grossi in produzione, però forse è un mio limit e quindi la mia domanda è sempre quella, sì vabbè, ma se scala, come scala e quanto scala? Ma questo forse è un po' una nostra deformazione professionale. Prima di fare una cosa ci facciamo questo tipo di domande. Però voglio arrivare a un'altra cosa. Abbiamo parlato di GraphQL, abbiamo parlato di REST, ma il nuovo arrivato nella stanza è di RCP. Quindi il nuovo protocollo RCP tipizzato per TypeScript per fare la chiamata remota di funzioni con i tipi scerati tra back-end e front-end. Non voglio affrontare il problema perché lo faremo in una prossima puntata, però una cosa che io ho sempre sentito a pelle dei protocolli RCP è che nascondono tutta la parte di operazioni sulla rete che nel meccanismo di comunicazione inseriscono della complessità, della complessità dettata dal fatto che qualunque tipo di comunicazione sulla rete non è solo ti chiedo e mi rispondi, ma ci sono una serie di variabili che non controlliamo e che possono finire per darci dei problemi. È GraphQL un po' questo tipo di complessità, specie con la federazione, quindi dove ho un nodo GraphQL a cui chiedo che poi si occupa in modo quasi da reverse proxy possiamo immaginarlo, di distribuire queste richieste per porzioni nei nodi che mi devono rispondere un po' astrae, nasconde tutta la complessità anche diretta che c'è tra i vari nodi. E quindi alla fine la mia domanda è come possiamo osservare quello che succede in un ecosistema GraphQL federato? Cioè come possiamo essere consapevoli delle merdate che da sistemisti possiamo aver fatto o dei lentezze dirette o dei problemi di questo tipo? Allora lì ci sono un po' di cose. Intanto bisogna avere dei DevOps coi controcasti, che è una cosa che spesso si sottovaluta, ma quello del DevOps bravo è un ruolo essenziale e quando hai un DevOps bravo tu tutte queste cose le sai perché c'è qualcuno che ti ha messo su un sistema come si deve. Poi sta nello sviluppatore andare a vedergli questi dati perché usando tool come Datadog, si chiama, Grafana, c'è soprattutto una serie di tool che fanno tante cose, ma sul sistema AWS stesso con Ray, come si chiama, qualcosa di realistico. Non lo so, da un anno che uso Azure, quindi posso dire. Sono anche certificato AWS, il nome proprio non me lo ricordo. Vabbè frega, ci sono più di 300 servizi AWS. Esatto, però tramite CloudWatch, c'è un sistema che si chiama Ray, è qualcosa che ti permette di vedere il giro che ha fatto la richiesta per i vari servizi. Comunque con Datadog, con Celsion, sono tutti una serie di servizi che ti danno le risposte. Quindi con Datadog tu puoi andare a vedere, dato una richiesta, qual è l'acquiri MySQL, se il sistema ha fatto bene, che ti ha rallentato con la richiesta. Può fare anche App Insight. Esatto, immagino che lo facciano. Cioè, sono alla base queste cose. Quindi le metriche, tutta una roba essenziale. Adesso sto lavorando, lo ricito, con Simone Sanfratello, che è molto bravo da questo punto di vista. Cioè lui su metriche, sull'andare a dire a questi sistemi che cosa sta succedendo dentro il codice, è proprio una cosa che gli piace, si vede, gli brillano gli occhi quando ti racconta queste cose. Tanto sarà ospite prossimamentissimo, quindi lo sentirete tra pochissimo. Quindi lui è proprio bravo. Io per esempio sono più sull'architettura, su queste cose ad alto livello, lui sul basso livello è un fenomeno. Quando metti i dati come si deve, tu recuperi tutte le informazioni che ti servono. Quindi sai esattamente le cose come funzionano. Poi dove non ci arriva l'infrastruttura, inteso come la VU, cioè l'infrastruttura, dove non ci arriva il tooling fatto da altri, ci si smazza e si inventano cose. Io ho finito il progetto dopo un anno in cui sono passato sul nuovo progetto, nei due mesi di pausa fra un progetto e l'altro, ho provato, anzi l'abbiamo fatto perché l'abbiamo costruito, ho provato a fare un po' di tooling proprio per Mercurius per andare incontro a queste cose. Quindi insieme ad altri ragazzi di NearForm, noi l'azienda abbiamo questa grande possibilità di fare un sacco di open source fra un progetto e l'altro. Abbiamo sviluppato questi due plugin, uno che è dato una federazione e ti racconta com'è fatta, cosa che in Mercurius mancava, non so se c'è in Apollo, ma adesso abbiamo un sistema che è dato una federazione e ti dice il dato X in quale servizio è sviluppato. Sembra una scematta, ma per chi ci lavora è essenziale, soprattutto quando hai cinque team che lavorano ai nodi. Tu vedi una cosa e dici ma chi è che lavora a questo nodo, dà un'occhiata che lo chiedo a tizio. Quella è una roba che tu se vedi solo lo schema finale non sai dove questa cosa viene fatta. Questa è la prima parte, prima cosa che abbiamo fatto e l'altra, una cosa essenziale, è che abbiamo fatto un plugin che ti restituisce le metriche della query che hai fatto. Quindi tu fai una query e gli dici esattamente che cosa è successo all'interno del sistema, che il resolver X ci ha messo 200 millisecondi ed è stato chiamato cinque volte. Tu dici come mai è stato chiamato cinque volte e vai a vedere e dici che qualcosa non è andato. Allora vai un po' a sistemare e ti rendi conto che spesso i problemi sono nel codice. Oppure vedi che il resolver ci mette troppo tempo e viene chiamato troppo spesso e quindi sai che quel resolver deve andare a sistemare le cose. Torniamo a quello che ti avevo detto inizialmente su GraphQL, il fatto che tu puoi concentrare su un singolo resolver è una grande cosa da questo punto di vista perché riduci la complessità. Esatto, riduci la complessità. Poi a volte risolvi con una bella cache, tutti i sanity ti aiutano, quindi Redis è un sidecar che deve essere sempre lì. Tra l'altro un saluto a Salvatore che ci sta ascoltando. Io ricordo ancora una frase di Matteo Collina che disse se io dovessi dire su cosa è basato il 90% della mia carriera in termini di performance rispondere con una sola parola, Redis. Ha scritto anche un tweet su questo. Io l'ho sempre usato molto, molto, diciamo di paro dei Redis. Un po' come scelta imposta, come sistema di cache. L'ultimo anno, sempre per il discorso che la nostra esperienza ce la facciamo poi in base al progetto in cui lavoriamo, perché è lì che si fa la differenza. L'ultimo progetto in cui ho fatto questa federazione, in cui ho lavorato a stretto contatto con Matteo per un bel po', il progetto era gigante e a Redis ho imparato, grazie a Matteo che mi ha spinto un po' a fare certe cose, ho imparato a usarlo molto bene e non a usarlo nel senso che non so usare Redis, ho imparato a sfruttarlo molto bene, questa è la parte importante. E' una cosa che dovrebbe far parte di una, cioè Redis dovrebbe esserci non il progetto. Un progetto che non ha un Redis è un progetto che non ha bisogno di performance, perché performance non intendo che deve restituire in 20 millisecondi una pagina, però spesso e volentieri... Se c'è complessità c'è fai dati, bom bom bom, un po' di carico probabilmente. Esatto, avere un luogo in cui tu dici io non mi preoccupo di quanto ci mette a dare questo dato, perché tanto so che mi refresha ogni mezz'ora e io quindi posso permettermi ogni 10 minuti di andarlo a prendere da Redis e me ne frego di quanto costi il reale peso. Ed è una cosa che ti fa programmare con molto più leggerezza da questo punto di vista. Con il cash bisogna sempre invalidarla nelle problematiche, però non è sempre necessario invalidarla, perché spesso sono label, spesso sono cose, ci sono altri controlli successivamente che nel caso il valore non sia giusto ti risolvono il problema. Tanto c'è sempre un caso in cui per quanto ci mettiamo non potremo mai... Va tutto in vacca! L'utente carica la pagina, va a mangiare, torna e c'ha una pagina piena di dati che non funzionano più, perché nel frattempo sono cambiati e committa una forma di qualcosa che dietro nel frattempo è cambiato. Ma se hai usato le GraphQL Subscriptions, qualcosa sotto il culo, i dati sotto il culo ti sono cambiati! Anche poi lavorando nell'ambiente le scopri queste cose. Io l'idea di avere una scheda di un browser aperta per dei giorni non l'avevo neanche mai considerata, ma scopri che poi in realtà ci sono e dici ma come ha fatto questo a mandarmi dentro questi dati che sono già tre giorni e che non ci sono più? Ma un utente interno dell'azienda, uno a cui abbiamo chiesto, e lui ci ha detto che la pagina era riaperta da una settimana e io semplicemente a un certo punto ho deciso di dire ok. Guarda, caso di stamattina, giusto per farci una risata insieme, caso di stamattina io sto lavorando un sistema e non posso dire praticamente niente su quello di cui sto lavorando, però in breve, sto lavorando un sistema che ti dà, c'è una pagina prodotto, immaginate una pagina prodotto anche se non è un prodotto e tu devi fare tipo un booking su quel prodotto e cribio ho iniziato a vedere tutti i 404, 404, 404, 404, non capivo perché ed era praticamente, cos'era successo? Una persona si era aperto, 50 prodotti, ok 50, 100 tab, uno con ogni prodotto, che non erano prodotti, questi prodotti sono scaduti, quindi non erano più presenti e lui ha praticamente lanciato, non so se fosse uno script o qualcosa, uno schedula call per tutti i prodotti e quindi io guardando le app in site ho detto ma questo è un coglione, invece no, se tu pensi a livello di business, io mi immagino, sai ho avuto un'azienda turistica per un po' di tempo e io mi ricordo la mia socia quando organizzava delle vacanze importanti, teneva anche per una o due settimane le pagine degli hotel aperti o le pagine dei servizi da bookare aperti, bookare non credo si dica neppure, ma facciamo finire. No, bookie, boo boo boo e bookare. Prenottare, prenottare. Ormai abbiamo perso l'uso delle lingue. Esatto, no però è una cosa più che verosimile e ormai l'abbiamo detto, quindi comunque questi edge case ci sono e ci saranno sempre. Guarda, io direi che potrei rimanere ore a parlare di questo, l'abbiamo anche già fatto. Vi faccio giusto l'ultima domanda sul topic perché è una domanda che mi sta molto a cuore. Qualche tempo fa ho lavorato su un API GraphQL, ero già in Irform ed era un grosso e-commerce, medio grosso e-commerce che faceva parecchi ordini al secondo e avevo un API GraphQL. E una delle cose più rompi balle in questo API GraphQL era cercare di stabilire un contratto tra front-end e back-end per la comunicazione dell'errore. GraphQL ha il suo modo di comunicare l'errore che ti dice error e poi c'hai del data non tipizzato. Ok? E mi dico ma cazzo, stiamo tipizzando tutto. Ok? Allora perché non tipizzare anche gli errori di dominio e dare un modo più agevole per la comunicazione di questi errori di dominio? Tanto sempre 200 ti risponde e già questo poco mi piace. Però come posso dirlo? Cioè ad oggi nell'API GraphQL vedo una forte distinzione tra gli errori di platform, quindi legati al framework utilizzato, al server, all'architecture e gli errori di dominio. Secondo te hai mai vissuto questo tipo di problema? Te lo dico perché noi abbiamo dovuto fare un wrapper, abbiamo dovuto hackare il sistema degli errori di Apollo per fargli rispondere degli errori in grazia di Dio dove potevi querare GraphQL anche dentro l'errore. Sì, il problema non è tanto la struttura dell'errore, il problema è che se c'è un errore quel dato ti torna nullo, questo è uno dei grandi problemi. E l'errore non è lì, ce l'hai separato, ce l'hai da un'altra parte e quindi è un problema. Perché se anche tu lo restituissi lì, col fatto che non hai messo nello schema di darti indietro l'errore, lui non te lo dà indietro l'errore su quel punto. Perché l'unica cosa sarebbe probabilmente quella di non so neanche se si possa fare a dire il vero. Però sì, hai ragione, non c'è una soluzione, che io sappia, non c'è una soluzione. Ti dico come avevamo fatto noi, avevamo messo tutto dentro un bel try and catch, avevamo tipizzato tutti gli errori, li avevamo messi nello schema e quindi c'era il return 200, il return positivo, però di un tipo errore che poi ti dovevi andare lato front end. E così abbiamo risolto. Lo mettevate nella struttura? Sì. Gli errori di dominio erano tipizzati, sì. Ma ti faccio una domanda, tu chiedi una lista di 10 prodotti, il terzo prodotto della lista ti ha dato un errore e quindi è nullo. Perché si è rotto e quindi non c'è nessun errore. Tu hai una lista di oggetti che possono essere un prodotto o un errore. Ah, avete fatto un bel hack per fare una roba di questo tipo. Sì, era una taffazzata. Però capisci che, guarda, io è un po' che ragiono col concetto di result, dove c'hai un left side che è il dato, un right side che è l'errore. E anche in JavaScript lo uso spesso, ho una classe stupida con queste due proprietà e poi io faccio, nel caso di Rust, faccio un match o faccio una if in JavaScript in modo da validare questi due casi, in modo che anche l'errore diventa parte del dominio, specie in contesti dove il dominio è importante. Certo, sul crude della chiesa non lo farei manco morto, però mi immagino un sistema di booking che ti dice, immagina che stai facendo un pacchetto e ti dice il servizio non è raggiungibile. Ecco, quello cos'è? Un errore di piattaforma o un errore di dominio, nel caso del pacchetto? Se è un errore di dominio io devo poterlo trattare come oggetto di dominio, quindi devo anche poter cuerare, bruttissima parola, dentro quell'errore. Ecco, una cosa che ho visto dei tool GraphQL è quella. Secondo me c'è un po' di lavoro a livello di specifica da fare. Allora, faccio un piccolo passo indietro. Da questo punto di vista, la community. Diciamo che le specifiche vanno molto a rilento su GraphQL, non c'è una grande pressione di farlo evolvere, questo è vero. Alla fine ci sono cose nuove, Apollo ha portato delle grandi cose, le subscription inizialmente erano un'idea, non esistevano, poi lentamente le hanno definite e buttate dentro. Non è velocissimo come sistema, quindi, come specifiche. Quindi aggiungere, fare la roba che richiedi implica che tu la devi mettere in specifiche, cioè il fatto che se un'entità non viene restituita tu potresti avere l'errore sul posto. Questa è appesante come affermazione. È bisogno pensarla bene. E me la devi restituire indipendentemente dal fatto che io te l'abbia chiesta o non te l'abbia chiesta. Perché è quello il problema. Perché se tu ci pensi, dovrebbe essere abbastanza semplice, tu per ogni entità restituisci eventualmente l'errore, io nel caso se ho un errore, ti mostro l'errore, si fa. Vuoi fare un hacking da questo punto di vista, lo fai. Il problema è che se tu non lo richiedi, quell'errore lui non ti da indietro. Invece, appunto, lo metto tu, di base ti da una strutturina separata con un elenco degli errori in cui ti dice c'è stato un errore X in questo punto, cioè ti dice proprio c'è stato un errore X in questo punto, quindi tu devi anche andarti a ricostruire dove c'è stato questo errore, che il 90% delle volte te ne freghi e dici c'è stato un errore e non mostri i dati. Ma se sei federato ti puoi anche sparare in bocca. Quello è un altro problema. Però bene o male gli errori vengono riportati. Davide non ti sento più, è successo qualcosa. Non ti sento. Sei sparito completamente. Allora datemi giusto un secondo, prima di andare in chiusura, perché abbiamo un momentino che non possiamo sboccare. È il momento di ringraziare i donatori della settimana, le persone che ci prendono sulle spalle e ci accompagnano facendo in modo che ogni settimana possiamo portare del contenuto fresco. Abbiamo questa settimana alcuni donatori, questo perché sono un po' di più del solito perché ci siamo fermati qualche settimana per Natale. Il primo è Giovanni Italiano che ha replicato una donazione facendo una donazione di 5 birre con un messaggio. Auguri di buone feste e grazie per il podcast. Grazie a te Giovanni. Abbiamo anche Leonardo Sabato o Sabato, sbaglio tutti gli accenti, li metto in modo randomico, abbi pazienza di me Leonardo. Grazie per tutto il tempo che togliete a voi per dedicarlo al podcast e soprattutto grazie per la condivisione. Bravi tutti, grazie a te perché fai in modo che noi possiamo veramente arrivare alle vostre orecchie ogni settimana. Ma abbiamo un altro donatore, 3 birre Andrea Quintino che insieme appunto a Leonardo e a Giovanni hanno fatto in modo che noi questa settimana abbiamo potuto pubblicare l'episodio. Dicevamo, nel frattempo è esploso tutto, però parlavamo appunto della gestione degli errori. Secondo me comunque è uno di quegli ambiti dove si può aprire una bella discussione. Sì, assolutamente sì. Cioè è uno di quegli ambiti in cui andrebbero affrontati un po' presi per le corne, trovare una soluzione comune per tutti perché poi una volta che hai la soluzione comune, anche il tooling dopo ce l'hai che ti funziona out of the box senza dover di nuovo reinventare la ruota tutte le volte perché poi la problematica è lì. Cioè che poi detto tra mette, secondo me la soluzione è anche abbastanza semplice, cioè restituisci un tipo che può essere i dati o l'errore, ok? Puoi definire l'errore se tu non lo definisci all'interno del nodo no? Di GraphQL. Se tu non lo definisci in caso d'errore hai o un errore generico o null. E alla fine così lo risolvi. Però diciamo che posticipiamo questa discussione magari in uno dei nostri working group su GraphQL. Assolutamente. Abbiamo trovato il topic per il prossimo working group. Io guardavo l'orologio, siamo avvantissimo con l'orario, non abbiamo dei nuovi donatori ahimè, però vi ricordo che abbiamo la parte nel sito di supportaci se vi va buttateci un occhio e se vi va di supportarci fatelo pure e arriviamo dritti dritti dritti al momento tipico e topico del nostro podcast. Il momento, il Paese dei Balocchi. Ed è questo il momento nel quale io chiedo a Davide se ha qualcosa da condividere con noi. Allora, ci ho pensato un po' perché sono andato un po' a guardarmi le vecchie puntate e ho scoperto all'ultimo di questa cosa quindi me la sono dovuta un po' improvvisare, ma ciò il regalo giusto. Il regalo giusto che è un libro che secondo me ogni sviluppatore e ogni persona che lavora nel nostro campo dovrebbe aver letto o quantomeno aver sentito illuminare sperando che non l'abbia già regalato qualcuno prima di me, che è Quality Land. È la prima volta che lo sento. È un libro, potrei dire un tipo, la guida intergalattica, ma non c'entra assolutamente nulla e non è neanche di quella brillantezza. Però, aspetta che lo cerco per bene, Quality Land di Mark Kling che esiste in due versioni, per ottimisti e per pessimisti. Il libro è lo stesso, non cambia molto, cambiano gli intermezzi che ci sono fra un capitale e l'altro, che in quello per ottimisti sono più ottimisti e quello per pessimisti sono un po' più pessimistici, che racconta il mondo che stiamo vivendo, il mondo dei social, il mondo dei sistemi di ricerca, il mondo della delivery, portato all'estremo, all'estremo dove si può arrivare. Faccio un esempio, non fai neanche più le ricerche sull'Amazon di turno, che non si chiama Amazon, il loro servizio, perché loro sanno già che cosa spedirti a casa, perché ti conoscono talmente bene che lui viene lasciato, porto un esempio, dalla ragazza e dopo tre minuti gli suona il campanello con uno che gli consegna sei birre. Ed è molto divertente, è un librino non troppo impegnativo, che ho veramente divorato, su un mondo distopico, però divertente, perché tipicamente il mondo distopico è un mondo triste, invece questo è un mondo molto allegro, molto divertente, quindi consiglio a tutti la lettura e per noi del settore ci sono sorrisi in più, perché è un mondo che conosciamo anche da dietro, non solo da davanti. Esatto, no, questo me lo recupero, è super interessante, soprattutto ero alla ricerca di una lettura leggera. Io invece, ahimè, butto al cesso tutta la leggerezza che ci ha portato Davide, per invece condividere con voi una roba molto figa. Allora, voi sapete, vi ho già rotto le balle su questo che sto provando con mediocre successo, devo dire, a studiare Rust per uno sviluppatore che viene dal mondo JavaScript e prima ancora dal mondo PHP, probabilmente tipo Rust è, non lo so, l'uomo nero dei racconti da bambino, cioè una roba che ti spaventa a morte e siccome la paura non è mai troppa, ho beccato questa fonte, questa risorsa che è molto figa. Allora, sto studiando Rust perché voglio avvicinarmi alla system programming, però nel contempo io ho fatto alcune parti dell'università un po' alla cazzo di cane, speciale a parte di algoritmi, nel senso, l'esame di APAL ho passato, ma se mi chiedete come, non lo so, non ne ho minimamente idea. Quindi ha senso per me riprendere in mano gli algoritmi e siccome sto studiando Rust, come vi dicevo, ho beccato questo repository che è una figata, pazzesca. Si chiama The Algorithms, esistono delle versioni in linguaggi diversi, ma la versione che vi voglio raccontare oggi è la versione degli algoritmi sviluppati in Rust e ci sono gli algoritmi di ordinamento, quelli sui grafi, gli algoritmi di dynamic programming, ci sono le data structure, ci sono gli algoritmi di ricerca, c'è veramente un gozziliardo. Se avete bisogno di una lettura un pochino più pesante, ecco, qua probabilmente avete il repository che fa per voi. Io comunque consiglio sempre il libro di Davide che mi sembra molto più interessante del mio balocco o almeno molto più divertente, però insomma, volevo condividere con voi... Anche lì dipende comunque... volevo condividere esatto con voi questa cosa. Davide, mi ha fatto superissimo piacere averti qua con noi, veramente un iper iper iper piacere. Anche a me mi sono proprio divertito un sacco, poi come già ben mi conosci, mi piace parlare, quindi saremmo potuti stare qualche altra ora qua in chiacchiere. Grazie di tutto e poi ci rivedremo presto, intanto con te spero presto di persona e poi adesso seguirò assiduamente il podcast per vedere un po' le persone che passano e che sono passate. Grazie, grazie di nuovo. Noi immagino che ci vedremo prestissimo, grazie davvero di essere venuti qua. Ricordo rapidamente abbiamo avuto con noi il super Davide Fiorello, il decano di noi nearformisti o nearformer, un abbraccio grandissimo Davide. Ringraziando di nuovo Davide, io vi ricordo rapidamente i nostri contatti, infochiocciola.it, brandrepo in modo canonico, il classico, l'immancabile gruppo telegram. Se avete del tempo aprite iTunes, andate tra i podcast e metteteci una bella stellina e se vi va anche lasciateci una recensione. Potrebbe non essere importante, ma per noi lo è, nel senso che grazie a questo riusciamo a tenere saldo la nostra posizione nelle classifiche di iTunes per il podcasting, che non è una questione di orgoglio o di dire noi siamo più fighi degli altri, ma è questione di raggiungere più orecchie possibili. Detto questo, appuntamento alla prossima settimana. Ciao! Sottotitoli e revisione a cura di QTSS",
  "segments": [
    {
      "id": 0,
      "seek": 0,
      "start": 0,
      "end": 9.5,
      "text": " Bene e benvenuti su Geekbar, nuova settimana e nuovo episodio qua sul nostro bar degli",
      "tokens": [
        27702,
        308,
        3271,
        553,
        29161,
        459,
        2876,
        916,
        5356,
        11,
        3822,
        27924,
        5584,
        36497,
        308,
        49348,
        39200,
        1004,
        24159,
        17603,
        35779,
        2159,
        32079
      ],
      "temperature": 0,
      "avg_logprob": -0.2576652666844359,
      "compression_ratio": 1.5663716814159292,
      "no_speech_prob": 0.6064186096191406
    },
    {
      "id": 1,
      "seek": 0,
      "start": 9.5,
      "end": 14.9,
      "text": " sviluppatori. Ormai le vacanze di Natale ce le siamo lasciate alle porte e siamo pronti",
      "tokens": [
        17342,
        388,
        10504,
        39842,
        13,
        1610,
        76,
        1301,
        476,
        2842,
        282,
        1381,
        1026,
        6821,
        1220,
        1769,
        476,
        33459,
        48451,
        473,
        5430,
        26658,
        308,
        33459,
        582,
        896,
        72
      ],
      "temperature": 0,
      "avg_logprob": -0.2576652666844359,
      "compression_ratio": 1.5663716814159292,
      "no_speech_prob": 0.6064186096191406
    },
    {
      "id": 2,
      "seek": 0,
      "start": 14.9,
      "end": 21.72,
      "text": " per registrare questo nuovissimo episodio che ahimè sarà uno degli episodi più difficili",
      "tokens": [
        680,
        11376,
        35559,
        10263,
        3822,
        5179,
        34966,
        39200,
        1004,
        947,
        3716,
        332,
        1462,
        41338,
        8526,
        32079,
        2927,
        30727,
        10589,
        2204,
        2312
      ],
      "temperature": 0,
      "avg_logprob": -0.2576652666844359,
      "compression_ratio": 1.5663716814159292,
      "no_speech_prob": 0.6064186096191406
    },
    {
      "id": 3,
      "seek": 0,
      "start": 21.72,
      "end": 26.72,
      "text": " di Geekbar. Abbiamo un ospite, la difficoltà non viene dall'ospite ma viene da me, nel",
      "tokens": [
        1026,
        2876,
        916,
        5356,
        13,
        32673,
        7415,
        517,
        3003,
        79,
        642,
        11,
        635,
        2204,
        4837,
        1467,
        2107,
        19561,
        43351,
        6,
        2763,
        642,
        463,
        19561,
        1120,
        385,
        11,
        15373
      ],
      "temperature": 0,
      "avg_logprob": -0.2576652666844359,
      "compression_ratio": 1.5663716814159292,
      "no_speech_prob": 0.6064186096191406
    },
    {
      "id": 4,
      "seek": 2672,
      "start": 26.72,
      "end": 34.56,
      "text": " senso che ho mangiato credo il taco più pesante della storia e quindi adesso fare una puntata",
      "tokens": [
        3151,
        539,
        947,
        1106,
        587,
        7834,
        2513,
        3864,
        78,
        1930,
        34101,
        10589,
        9262,
        2879,
        11618,
        5967,
        654,
        308,
        15727,
        39552,
        11994,
        2002,
        18212,
        3274
      ],
      "temperature": 0,
      "avg_logprob": -0.2341912709749662,
      "compression_ratio": 1.5909090909090908,
      "no_speech_prob": 3.8070129448897205e-7
    },
    {
      "id": 5,
      "seek": 2672,
      "start": 34.56,
      "end": 42.56,
      "text": " al microfono con questo taco che mi gira per lo stomaco può essere complesso ma detto",
      "tokens": [
        419,
        42763,
        8957,
        416,
        10263,
        34101,
        947,
        2752,
        290,
        4271,
        680,
        450,
        9036,
        11428,
        26526,
        19799,
        1209,
        5557,
        463,
        41031
      ],
      "temperature": 0,
      "avg_logprob": -0.2341912709749662,
      "compression_ratio": 1.5909090909090908,
      "no_speech_prob": 3.8070129448897205e-7
    },
    {
      "id": 6,
      "seek": 2672,
      "start": 42.56,
      "end": 48.92,
      "text": " questo so che vi interessa poco quello che ho mangiato, vi prometto che cancellerò",
      "tokens": [
        10263,
        370,
        947,
        1932,
        728,
        8391,
        10639,
        22813,
        947,
        1106,
        587,
        7834,
        2513,
        11,
        1932,
        37786,
        1353,
        947,
        19114,
        260,
        4293
      ],
      "temperature": 0,
      "avg_logprob": -0.2341912709749662,
      "compression_ratio": 1.5909090909090908,
      "no_speech_prob": 3.8070129448897205e-7
    },
    {
      "id": 7,
      "seek": 2672,
      "start": 48.92,
      "end": 54.4,
      "text": " qualunque tipo di rumore possiate sentire, ahimè l'ospite dovrà subirsi lì. Detto",
      "tokens": [
        4101,
        409,
        1077,
        9746,
        1026,
        8347,
        418,
        1402,
        13024,
        2279,
        621,
        11,
        3716,
        332,
        1462,
        287,
        6,
        2763,
        642,
        30870,
        39212,
        1422,
        347,
        7691,
        287,
        4749,
        13,
        4237,
        1353
      ],
      "temperature": 0,
      "avg_logprob": -0.2341912709749662,
      "compression_ratio": 1.5909090909090908,
      "no_speech_prob": 3.8070129448897205e-7
    },
    {
      "id": 8,
      "seek": 5440,
      "start": 54.4,
      "end": 58.699999999999996,
      "text": " questo io direi che vi ricordo rapidamente i nostri contatti e possiamo iniziare. Info",
      "tokens": [
        10263,
        19785,
        1264,
        72,
        947,
        1932,
        21040,
        23872,
        7558,
        3439,
        741,
        10397,
        470,
        660,
        21515,
        308,
        44758,
        294,
        24300,
        543,
        13,
        11537,
        78
      ],
      "temperature": 0,
      "avg_logprob": -0.368328012738909,
      "compression_ratio": 1.6457564575645756,
      "no_speech_prob": 0.0000010030134944827296
    },
    {
      "id": 9,
      "seek": 5440,
      "start": 58.699999999999996,
      "end": 65.28,
      "text": " chiocciola Geekbar.it e The Brain Repo su Twitter sono i modi canonici per contattarci,",
      "tokens": [
        417,
        1004,
        66,
        537,
        4711,
        2876,
        916,
        5356,
        13,
        270,
        308,
        314,
        675,
        29783,
        3696,
        78,
        459,
        5794,
        9259,
        741,
        1072,
        72,
        21985,
        8787,
        680,
        660,
        1591,
        289,
        537,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.368328012738909,
      "compression_ratio": 1.6457564575645756,
      "no_speech_prob": 0.0000010030134944827296
    },
    {
      "id": 10,
      "seek": 5440,
      "start": 65.28,
      "end": 72.08,
      "text": " anche quelli che ormai non usa più nessuno. A perchè la mail, no è bella e palle scrivere",
      "tokens": [
        11585,
        631,
        16320,
        947,
        420,
        76,
        1301,
        2107,
        29909,
        10589,
        39787,
        12638,
        13,
        316,
        29240,
        1462,
        635,
        10071,
        11,
        572,
        4873,
        312,
        3505,
        308,
        280,
        11780,
        5545,
        5887
      ],
      "temperature": 0,
      "avg_logprob": -0.368328012738909,
      "compression_ratio": 1.6457564575645756,
      "no_speech_prob": 0.0000010030134944827296
    },
    {
      "id": 11,
      "seek": 5440,
      "start": 72.08,
      "end": 78.08,
      "text": " le mail. B perchè tanto Twitter sta fallendo e non masco meno ci faremo la nostra istanza",
      "tokens": [
        476,
        10071,
        13,
        363,
        29240,
        1462,
        10331,
        5794,
        11135,
        2100,
        3999,
        308,
        2107,
        2300,
        1291,
        40236,
        6983,
        11994,
        3280,
        635,
        34311,
        1418,
        20030
      ],
      "temperature": 0,
      "avg_logprob": -0.368328012738909,
      "compression_ratio": 1.6457564575645756,
      "no_speech_prob": 0.0000010030134944827296
    },
    {
      "id": 12,
      "seek": 5440,
      "start": 78.08,
      "end": 83.75999999999999,
      "text": " mastodon. No, abbiamo Telegram, abbiamo il gruppo Telegram, siamo in tanti, siamo belli",
      "tokens": [
        27055,
        378,
        266,
        13,
        883,
        11,
        22815,
        14889,
        1342,
        11,
        22815,
        1930,
        47477,
        78,
        14889,
        1342,
        11,
        33459,
        294,
        256,
        11520,
        11,
        33459,
        48006
      ],
      "temperature": 0,
      "avg_logprob": -0.368328012738909,
      "compression_ratio": 1.6457564575645756,
      "no_speech_prob": 0.0000010030134944827296
    },
    {
      "id": 13,
      "seek": 8376,
      "start": 83.76,
      "end": 89.36,
      "text": " e belle, siamo freschi quindi se non l'avete ancora fatto iscrivetevi e troverete tante",
      "tokens": [
        308,
        28770,
        11,
        33459,
        25235,
        8036,
        15727,
        369,
        2107,
        287,
        6,
        706,
        3498,
        30656,
        23228,
        307,
        1142,
        85,
        3498,
        4917,
        308,
        4495,
        331,
        3498,
        256,
        2879
      ],
      "temperature": 0,
      "avg_logprob": -0.2965714931488037,
      "compression_ratio": 1.550660792951542,
      "no_speech_prob": 0.0000013081750012133853
    },
    {
      "id": 14,
      "seek": 8376,
      "start": 89.36,
      "end": 95.48,
      "text": " piccole sorprese. Detto questo, anche detto con uno stile che mi ricordo un po' Vanna",
      "tokens": [
        13363,
        27247,
        9359,
        3712,
        405,
        13,
        4237,
        1353,
        10263,
        11,
        11585,
        41031,
        416,
        8526,
        342,
        794,
        947,
        2752,
        21040,
        23872,
        517,
        714,
        6,
        8979,
        629
      ],
      "temperature": 0,
      "avg_logprob": -0.2965714931488037,
      "compression_ratio": 1.550660792951542,
      "no_speech_prob": 0.0000013081750012133853
    },
    {
      "id": 15,
      "seek": 8376,
      "start": 95.48,
      "end": 104.24000000000001,
      "text": " Marki, io direi che possiamo iniziare, quindi sigla. Benvenuti su Geekbar il podcast dedicato",
      "tokens": [
        2039,
        2984,
        11,
        19785,
        1264,
        72,
        947,
        44758,
        294,
        24300,
        543,
        11,
        15727,
        4556,
        875,
        13,
        3964,
        553,
        29161,
        459,
        2876,
        916,
        5356,
        1930,
        7367,
        37071,
        2513
      ],
      "temperature": 0,
      "avg_logprob": -0.2965714931488037,
      "compression_ratio": 1.550660792951542,
      "no_speech_prob": 0.0000013081750012133853
    },
    {
      "id": 16,
      "seek": 8376,
      "start": 104.24000000000001,
      "end": 109.48,
      "text": " al mondo dei full stack debelter, i mezzo artigiani, i mezzo artisti che ogni giorno",
      "tokens": [
        419,
        40499,
        13874,
        1577,
        8630,
        3001,
        338,
        391,
        11,
        741,
        28966,
        4765,
        1523,
        328,
        21309,
        11,
        741,
        28966,
        4765,
        5748,
        72,
        947,
        33189,
        42202
      ],
      "temperature": 0,
      "avg_logprob": -0.2965714931488037,
      "compression_ratio": 1.550660792951542,
      "no_speech_prob": 0.0000013081750012133853
    },
    {
      "id": 17,
      "seek": 10948,
      "start": 109.48,
      "end": 114.10000000000001,
      "text": " infilano le mani nel fango per creare nel modo più efficiente possibile quei prodotti",
      "tokens": [
        1536,
        388,
        3730,
        476,
        587,
        72,
        15373,
        283,
        17150,
        680,
        1197,
        543,
        15373,
        16664,
        10589,
        7148,
        68,
        50184,
        631,
        72,
        15792,
        37514
      ],
      "temperature": 0,
      "avg_logprob": -0.18731932920568128,
      "compression_ratio": 1.4775280898876404,
      "no_speech_prob": 0.000008801080184639432
    },
    {
      "id": 18,
      "seek": 10948,
      "start": 114.10000000000001,
      "end": 128.76,
      "text": " digitali che quotidianamente usiamo. Eccoci qua, eccoci qua, oggi abbiamo un ospite del",
      "tokens": [
        4562,
        72,
        947,
        9641,
        34681,
        3439,
        505,
        7415,
        13,
        28993,
        1291,
        537,
        24159,
        11,
        11437,
        1291,
        537,
        24159,
        11,
        34768,
        22815,
        517,
        3003,
        79,
        642,
        1103
      ],
      "temperature": 0,
      "avg_logprob": -0.18731932920568128,
      "compression_ratio": 1.4775280898876404,
      "no_speech_prob": 0.000008801080184639432
    },
    {
      "id": 19,
      "seek": 10948,
      "start": 128.76,
      "end": 137.4,
      "text": " quale sono super super fiero, l'ho precettato ad Alicante e voi sapete dall'episodio con",
      "tokens": [
        421,
        1220,
        9259,
        1687,
        1687,
        283,
        12030,
        11,
        287,
        6,
        1289,
        4346,
        3093,
        2513,
        614,
        967,
        299,
        2879,
        308,
        20931,
        18985,
        3498,
        43351,
        6,
        595,
        271,
        378,
        1004,
        416
      ],
      "temperature": 0,
      "avg_logprob": -0.18731932920568128,
      "compression_ratio": 1.4775280898876404,
      "no_speech_prob": 0.000008801080184639432
    },
    {
      "id": 20,
      "seek": 13740,
      "start": 137.4,
      "end": 142.32,
      "text": " Michele Riva cosa è successo ad Alicante, no? È successo di tutto e non riusciamo a",
      "tokens": [
        3392,
        16884,
        497,
        5931,
        10163,
        4873,
        2245,
        78,
        614,
        967,
        299,
        2879,
        11,
        572,
        30,
        34495,
        2245,
        78,
        1026,
        23048,
        308,
        2107,
        367,
        4872,
        42052,
        257
      ],
      "temperature": 0,
      "avg_logprob": -0.29820774719778415,
      "compression_ratio": 1.521551724137931,
      "no_speech_prob": 4.618430082814484e-8
    },
    {
      "id": 21,
      "seek": 13740,
      "start": 142.32,
      "end": 148.72,
      "text": " dare una forma. Era anche lui là con noi, è una delle figure di riferimento dentro",
      "tokens": [
        8955,
        2002,
        8366,
        13,
        23071,
        11585,
        8783,
        3684,
        416,
        22447,
        11,
        4873,
        2002,
        16485,
        2573,
        1026,
        367,
        9361,
        10030,
        10856
      ],
      "temperature": 0,
      "avg_logprob": -0.29820774719778415,
      "compression_ratio": 1.521551724137931,
      "no_speech_prob": 4.618430082814484e-8
    },
    {
      "id": 22,
      "seek": 13740,
      "start": 148.72,
      "end": 157.4,
      "text": " Nirform, lo definirei uno dei decani di Nirform. Abbiamo con noi Davide Fiorello, staff engineer",
      "tokens": [
        426,
        347,
        2994,
        76,
        11,
        450,
        1561,
        621,
        72,
        8526,
        13874,
        979,
        3782,
        1026,
        426,
        347,
        837,
        13,
        32673,
        7415,
        416,
        22447,
        3724,
        482,
        38245,
        418,
        1913,
        11,
        3525,
        11403
      ],
      "temperature": 0,
      "avg_logprob": -0.29820774719778415,
      "compression_ratio": 1.521551724137931,
      "no_speech_prob": 4.618430082814484e-8
    },
    {
      "id": 23,
      "seek": 13740,
      "start": 157.4,
      "end": 165.84,
      "text": " a Nirform. Ho detto bene Davide? Sì, ciao a tutti, hai detto benissimo. Grazie Mauro",
      "tokens": [
        257,
        426,
        347,
        837,
        13,
        3631,
        41031,
        2537,
        3724,
        482,
        30,
        318,
        4749,
        11,
        42860,
        257,
        19822,
        11,
        21822,
        41031,
        3271,
        34966,
        13,
        8985,
        3283,
        32858,
        340
      ],
      "temperature": 0,
      "avg_logprob": -0.29820774719778415,
      "compression_ratio": 1.521551724137931,
      "no_speech_prob": 4.618430082814484e-8
    },
    {
      "id": 24,
      "seek": 16584,
      "start": 165.84,
      "end": 171.68,
      "text": " intanto per la presentazione, ho visto che hai ricordato Alicante, ho di recente ascoltato",
      "tokens": [
        560,
        5857,
        680,
        635,
        1974,
        12928,
        11,
        1106,
        17558,
        947,
        21822,
        21040,
        765,
        2513,
        967,
        299,
        2879,
        11,
        1106,
        1026,
        850,
        1576,
        15526,
        4837,
        2513
      ],
      "temperature": 0,
      "avg_logprob": -0.2421626222544703,
      "compression_ratio": 1.6807511737089202,
      "no_speech_prob": 2.873645712497819e-7
    },
    {
      "id": 25,
      "seek": 16584,
      "start": 171.68,
      "end": 179.12,
      "text": " la puntata e ho visto che ha colpito e lasciato nel cuore di molti, di molti, tanti ricordi.",
      "tokens": [
        635,
        18212,
        3274,
        308,
        1106,
        17558,
        947,
        324,
        1173,
        79,
        3528,
        308,
        48451,
        2513,
        15373,
        2702,
        418,
        1026,
        10739,
        72,
        11,
        1026,
        10739,
        72,
        11,
        256,
        11520,
        21040,
        765,
        72,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.2421626222544703,
      "compression_ratio": 1.6807511737089202,
      "no_speech_prob": 2.873645712497819e-7
    },
    {
      "id": 26,
      "seek": 16584,
      "start": 179.12,
      "end": 184.96,
      "text": " Mi compiace molto questa cosa, comunque a parte il meraviglioso evento che abbiamo tutti",
      "tokens": [
        10204,
        715,
        72,
        617,
        16394,
        16540,
        10163,
        11,
        45736,
        257,
        6975,
        1930,
        3551,
        706,
        328,
        2081,
        9869,
        40655,
        947,
        22815,
        19822
      ],
      "temperature": 0,
      "avg_logprob": -0.2421626222544703,
      "compression_ratio": 1.6807511737089202,
      "no_speech_prob": 2.873645712497819e-7
    },
    {
      "id": 27,
      "seek": 16584,
      "start": 184.96,
      "end": 193.16,
      "text": " nel cuore, grazie per l'invito di nuovo e via, vediamo se riesco a essere all'altezza",
      "tokens": [
        15373,
        2702,
        418,
        11,
        1295,
        3283,
        680,
        287,
        6,
        259,
        85,
        3528,
        1026,
        49348,
        308,
        5766,
        11,
        14267,
        7415,
        369,
        23932,
        1291,
        257,
        19799,
        439,
        6,
        304,
        975,
        26786
      ],
      "temperature": 0,
      "avg_logprob": -0.2421626222544703,
      "compression_ratio": 1.6807511737089202,
      "no_speech_prob": 2.873645712497819e-7
    },
    {
      "id": 28,
      "seek": 19316,
      "start": 193.16,
      "end": 199.52,
      "text": " della presentazione che mi hai fatto. Io su questo non ho dubbi Davide, ormai è già",
      "tokens": [
        11618,
        1974,
        12928,
        947,
        2752,
        21822,
        23228,
        13,
        19239,
        459,
        10263,
        2107,
        1106,
        18540,
        5614,
        3724,
        482,
        11,
        420,
        76,
        1301,
        4873,
        30469
      ],
      "temperature": 0,
      "avg_logprob": -0.20582236230900858,
      "compression_ratio": 1.51528384279476,
      "no_speech_prob": 1.3232033246879382e-8
    },
    {
      "id": 29,
      "seek": 19316,
      "start": 199.52,
      "end": 205.28,
      "text": " da un po' un paio d'anni che ci conosciamo e quindi, insomma, e poi vedo su cosa lavori",
      "tokens": [
        1120,
        517,
        714,
        6,
        517,
        2502,
        1004,
        274,
        6,
        35832,
        947,
        6983,
        49892,
        42052,
        308,
        15727,
        11,
        1028,
        30243,
        11,
        308,
        19260,
        14267,
        78,
        459,
        10163,
        20923,
        7386
      ],
      "temperature": 0,
      "avg_logprob": -0.20582236230900858,
      "compression_ratio": 1.51528384279476,
      "no_speech_prob": 1.3232033246879382e-8
    },
    {
      "id": 30,
      "seek": 19316,
      "start": 205.28,
      "end": 211.56,
      "text": " e come lavori. Abbiamo avuto più di una volta modo di confrontarci e lo faremo anche",
      "tokens": [
        308,
        808,
        20923,
        7386,
        13,
        32673,
        7415,
        1305,
        8262,
        10589,
        1026,
        2002,
        18765,
        16664,
        1026,
        12422,
        289,
        537,
        308,
        450,
        11994,
        3280,
        11585
      ],
      "temperature": 0,
      "avg_logprob": -0.20582236230900858,
      "compression_ratio": 1.51528384279476,
      "no_speech_prob": 1.3232033246879382e-8
    },
    {
      "id": 31,
      "seek": 19316,
      "start": 211.56,
      "end": 220.12,
      "text": " oggi parlando di un argomento che incendia un po' gli animi, no? Crea un po' di fazioni",
      "tokens": [
        34768,
        971,
        16201,
        1026,
        517,
        3882,
        298,
        15467,
        947,
        834,
        521,
        654,
        517,
        714,
        6,
        17161,
        2383,
        72,
        11,
        572,
        30,
        9549,
        64,
        517,
        714,
        6,
        1026,
        4375,
        15273
      ],
      "temperature": 0,
      "avg_logprob": -0.20582236230900858,
      "compression_ratio": 1.51528384279476,
      "no_speech_prob": 1.3232033246879382e-8
    },
    {
      "id": 32,
      "seek": 22012,
      "start": 220.12,
      "end": 229.92000000000002,
      "text": " e parliamo infatti di GraphQL. Qualche anno fa abbiamo fatto una puntata speciale, era",
      "tokens": [
        308,
        971,
        49926,
        1536,
        21515,
        1026,
        21884,
        13695,
        13,
        13616,
        1876,
        46277,
        2050,
        22815,
        23228,
        2002,
        18212,
        3274,
        2121,
        68,
        11,
        4249
      ],
      "temperature": 0,
      "avg_logprob": -0.21773383617401124,
      "compression_ratio": 1.4426229508196722,
      "no_speech_prob": 6.893605473123898e-7
    },
    {
      "id": 33,
      "seek": 22012,
      "start": 229.92000000000002,
      "end": 234.8,
      "text": " il primo compleanno di GitBar, avevamo questo format, se ve lo siete persi andatevelo a",
      "tokens": [
        1930,
        38671,
        44424,
        13484,
        1026,
        16939,
        38666,
        11,
        3472,
        85,
        10502,
        10263,
        7877,
        11,
        369,
        1241,
        450,
        40719,
        868,
        72,
        293,
        473,
        303,
        752,
        257
      ],
      "temperature": 0,
      "avg_logprob": -0.21773383617401124,
      "compression_ratio": 1.4426229508196722,
      "no_speech_prob": 6.893605473123898e-7
    },
    {
      "id": 34,
      "seek": 22012,
      "start": 234.8,
      "end": 242.6,
      "text": " recuperare perché è esilarante, lo chiamammo GitFighter ed era un episodio dove io come",
      "tokens": [
        25692,
        543,
        14303,
        4873,
        785,
        2202,
        2879,
        11,
        450,
        417,
        2918,
        335,
        3280,
        16939,
        37,
        397,
        260,
        1257,
        4249,
        517,
        39200,
        1004,
        23287,
        19785,
        808
      ],
      "temperature": 0,
      "avg_logprob": -0.21773383617401124,
      "compression_ratio": 1.4426229508196722,
      "no_speech_prob": 6.893605473123898e-7
    },
    {
      "id": 35,
      "seek": 24260,
      "start": 242.6,
      "end": 250.2,
      "text": " host tiravo fuori due fazioni, che ne so, tirammo fuori Vim e VS Code e in modo del tutto",
      "tokens": [
        3975,
        13807,
        25713,
        8536,
        7386,
        3462,
        4375,
        15273,
        11,
        947,
        408,
        370,
        11,
        13807,
        335,
        3280,
        8536,
        7386,
        691,
        332,
        308,
        25091,
        15549,
        308,
        294,
        16664,
        1103,
        23048
      ],
      "temperature": 0,
      "avg_logprob": -0.22918860932700655,
      "compression_ratio": 1.7524271844660195,
      "no_speech_prob": 2.1815937500946347e-8
    },
    {
      "id": 36,
      "seek": 24260,
      "start": 250.2,
      "end": 256.32,
      "text": " randomico assegnavamo una fazione a uno sviluppatore che doveva difendere col sangue la posizione",
      "tokens": [
        4974,
        2789,
        5907,
        70,
        629,
        85,
        10502,
        2002,
        4375,
        5328,
        257,
        8526,
        17342,
        388,
        10504,
        43148,
        947,
        23287,
        2757,
        679,
        521,
        323,
        1173,
        9980,
        622,
        635,
        1366,
        35740
      ],
      "temperature": 0,
      "avg_logprob": -0.22918860932700655,
      "compression_ratio": 1.7524271844660195,
      "no_speech_prob": 2.1815937500946347e-8
    },
    {
      "id": 37,
      "seek": 24260,
      "start": 256.32,
      "end": 262.24,
      "text": " e abbiamo visto situazioni dove chi usava Vim doveva difendere Visual Studio Code o",
      "tokens": [
        308,
        22815,
        17558,
        2054,
        27569,
        23287,
        13228,
        505,
        4061,
        691,
        332,
        23287,
        2757,
        679,
        521,
        323,
        23187,
        13500,
        15549,
        277
      ],
      "temperature": 0,
      "avg_logprob": -0.22918860932700655,
      "compression_ratio": 1.7524271844660195,
      "no_speech_prob": 2.1815937500946347e-8
    },
    {
      "id": 38,
      "seek": 24260,
      "start": 262.24,
      "end": 269.6,
      "text": " chi faceva API REST doveva difendere, chi amava l'API REST barra RESTful doveva difendere",
      "tokens": [
        13228,
        1851,
        2757,
        9362,
        497,
        14497,
        23287,
        2757,
        679,
        521,
        323,
        11,
        13228,
        669,
        4061,
        287,
        6,
        4715,
        40,
        497,
        14497,
        2159,
        424,
        497,
        14497,
        906,
        23287,
        2757,
        679,
        521,
        323
      ],
      "temperature": 0,
      "avg_logprob": -0.22918860932700655,
      "compression_ratio": 1.7524271844660195,
      "no_speech_prob": 2.1815937500946347e-8
    },
    {
      "id": 39,
      "seek": 26960,
      "start": 269.6,
      "end": 277.84000000000003,
      "text": " GraphQL e là uscì una frase che mi fece particolarmente ridere che diceva così, nel",
      "tokens": [
        21884,
        13695,
        308,
        3684,
        505,
        66,
        4749,
        2002,
        38406,
        947,
        2752,
        579,
        384,
        1276,
        15276,
        4082,
        3973,
        323,
        947,
        10313,
        2757,
        23278,
        11,
        15373
      ],
      "temperature": 0,
      "avg_logprob": -0.2669713120711477,
      "compression_ratio": 1.4301675977653632,
      "no_speech_prob": 4.075751647292236e-8
    },
    {
      "id": 40,
      "seek": 26960,
      "start": 277.84000000000003,
      "end": 287.52000000000004,
      "text": " difendere GraphQL si diceva rest in peace per REST, è arrivato la nuova tecnologia",
      "tokens": [
        679,
        521,
        323,
        21884,
        13695,
        1511,
        10313,
        2757,
        1472,
        294,
        4336,
        680,
        497,
        14497,
        11,
        4873,
        30697,
        2513,
        635,
        3822,
        27924,
        44905
      ],
      "temperature": 0,
      "avg_logprob": -0.2669713120711477,
      "compression_ratio": 1.4301675977653632,
      "no_speech_prob": 4.075751647292236e-8
    },
    {
      "id": 41,
      "seek": 26960,
      "start": 287.52000000000004,
      "end": 297.04,
      "text": " disruptive. Davide io so che tu ultimamente hai lavorato pesantemente su GraphQL e so",
      "tokens": [
        37865,
        13,
        3724,
        482,
        19785,
        370,
        947,
        2604,
        3725,
        332,
        3439,
        21822,
        29241,
        2513,
        9262,
        394,
        16288,
        459,
        21884,
        13695,
        308,
        370
      ],
      "temperature": 0,
      "avg_logprob": -0.2669713120711477,
      "compression_ratio": 1.4301675977653632,
      "no_speech_prob": 4.075751647292236e-8
    },
    {
      "id": 42,
      "seek": 29704,
      "start": 297.04,
      "end": 303.88,
      "text": " anche che hai lavorato su Mercurius che è una delle librerie JavaScript per fare GraphQL,",
      "tokens": [
        11585,
        947,
        21822,
        29241,
        2513,
        459,
        18897,
        374,
        4872,
        947,
        4873,
        2002,
        16485,
        4939,
        17487,
        15778,
        680,
        11994,
        21884,
        13695,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.272269082069397,
      "compression_ratio": 1.5113636363636365,
      "no_speech_prob": 5.9301825672264386e-8
    },
    {
      "id": 43,
      "seek": 29704,
      "start": 303.88,
      "end": 313.04,
      "text": " ricordo bene? Sì ricordi molto bene e sì ho lavorato parecchio su GraphQL di recente",
      "tokens": [
        21040,
        23872,
        2537,
        30,
        318,
        4749,
        21040,
        765,
        72,
        16394,
        2537,
        308,
        49267,
        1106,
        29241,
        2513,
        7448,
        66,
        31033,
        459,
        21884,
        13695,
        1026,
        850,
        1576
      ],
      "temperature": 0,
      "avg_logprob": -0.272269082069397,
      "compression_ratio": 1.5113636363636365,
      "no_speech_prob": 5.9301825672264386e-8
    },
    {
      "id": 44,
      "seek": 29704,
      "start": 313.04,
      "end": 321.52000000000004,
      "text": " e fra una cosa e l'altra ci lavoro da tanti anni, alternata con periodi sì, periodi no,",
      "tokens": [
        308,
        6600,
        2002,
        10163,
        308,
        287,
        6,
        38865,
        6983,
        42060,
        1120,
        256,
        11520,
        31164,
        11,
        5400,
        3274,
        416,
        2896,
        72,
        49267,
        11,
        2896,
        72,
        572,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.272269082069397,
      "compression_ratio": 1.5113636363636365,
      "no_speech_prob": 5.9301825672264386e-8
    },
    {
      "id": 45,
      "seek": 32152,
      "start": 321.52,
      "end": 328.71999999999997,
      "text": " periodi d'odio, periodi di grande amore, adesso sono in un periodo di grande amore fortunatamente",
      "tokens": [
        2896,
        72,
        274,
        6,
        378,
        1004,
        11,
        2896,
        72,
        1026,
        8883,
        669,
        418,
        11,
        39552,
        9259,
        294,
        517,
        2896,
        78,
        1026,
        8883,
        669,
        418,
        10506,
        25354
      ],
      "temperature": 0,
      "avg_logprob": -0.262818066578991,
      "compression_ratio": 1.8131313131313131,
      "no_speech_prob": 1.9252484051435204e-8
    },
    {
      "id": 46,
      "seek": 32152,
      "start": 328.71999999999997,
      "end": 334.79999999999995,
      "text": " e quindi mi sono fatto idee contrastanti, me le sono legate, me le sono sostenute, mi",
      "tokens": [
        308,
        15727,
        2752,
        9259,
        23228,
        49742,
        8712,
        11520,
        11,
        385,
        476,
        9259,
        1676,
        473,
        11,
        385,
        476,
        9259,
        262,
        18946,
        1169,
        11,
        2752
      ],
      "temperature": 0,
      "avg_logprob": -0.262818066578991,
      "compression_ratio": 1.8131313131313131,
      "no_speech_prob": 1.9252484051435204e-8
    },
    {
      "id": 47,
      "seek": 32152,
      "start": 334.79999999999995,
      "end": 340,
      "text": " sono un po' tutte, avrei potuto fare una di quelle pintate di cui hai appena citato",
      "tokens": [
        9259,
        517,
        714,
        6,
        38632,
        11,
        1305,
        10271,
        1847,
        8262,
        11994,
        2002,
        1026,
        29237,
        23924,
        473,
        1026,
        22929,
        21822,
        724,
        4118,
        4814,
        2513
      ],
      "temperature": 0,
      "avg_logprob": -0.262818066578991,
      "compression_ratio": 1.8131313131313131,
      "no_speech_prob": 1.9252484051435204e-8
    },
    {
      "id": 48,
      "seek": 32152,
      "start": 340,
      "end": 347.52,
      "text": " da solo perché veramente va un po' a periodi, adesso diciamo che dopo tanti anni ho un po'",
      "tokens": [
        1120,
        6944,
        14303,
        50079,
        2773,
        517,
        714,
        6,
        257,
        2896,
        72,
        11,
        39552,
        14285,
        7415,
        947,
        35196,
        256,
        11520,
        31164,
        1106,
        517,
        714,
        6
      ],
      "temperature": 0,
      "avg_logprob": -0.262818066578991,
      "compression_ratio": 1.8131313131313131,
      "no_speech_prob": 1.9252484051435204e-8
    },
    {
      "id": 49,
      "seek": 34752,
      "start": 347.52,
      "end": 354.68,
      "text": " di base per iniziare a, non è insegnabile perché quello ogni volta scopro qualcosa",
      "tokens": [
        1026,
        3096,
        680,
        294,
        24300,
        543,
        257,
        11,
        2107,
        4873,
        33874,
        4568,
        33288,
        14303,
        22813,
        33189,
        18765,
        795,
        404,
        340,
        42400
      ],
      "temperature": 0,
      "avg_logprob": -0.2759587371233598,
      "compression_ratio": 1.6543778801843319,
      "no_speech_prob": 8.990940614239662e-7
    },
    {
      "id": 50,
      "seek": 34752,
      "start": 354.68,
      "end": 361.96,
      "text": " di nuovo, però per iniziare veramente a capire quando sì, quando no, se ne vale la pena,",
      "tokens": [
        1026,
        49348,
        11,
        12673,
        680,
        294,
        24300,
        543,
        50079,
        257,
        1410,
        621,
        7770,
        49267,
        11,
        7770,
        572,
        11,
        369,
        408,
        15474,
        635,
        29222,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.2759587371233598,
      "compression_ratio": 1.6543778801843319,
      "no_speech_prob": 8.990940614239662e-7
    },
    {
      "id": 51,
      "seek": 34752,
      "start": 361.96,
      "end": 366.47999999999996,
      "text": " se non ne vale la pena, quindi diciamo che un po' di base me la sono fatta da questo",
      "tokens": [
        369,
        2107,
        408,
        15474,
        635,
        29222,
        11,
        15727,
        14285,
        7415,
        947,
        517,
        714,
        6,
        1026,
        3096,
        385,
        635,
        9259,
        4046,
        1328,
        1120,
        10263
      ],
      "temperature": 0,
      "avg_logprob": -0.2759587371233598,
      "compression_ratio": 1.6543778801843319,
      "no_speech_prob": 8.990940614239662e-7
    },
    {
      "id": 52,
      "seek": 34752,
      "start": 366.47999999999996,
      "end": 373.91999999999996,
      "text": " punto di vista. Su quello ci arriviamo brevissimo, però la prima cosa dalla quale vorrei iniziare",
      "tokens": [
        14326,
        1026,
        22553,
        13,
        2746,
        22813,
        6983,
        30697,
        7415,
        1403,
        85,
        34966,
        11,
        12673,
        635,
        19507,
        10163,
        35566,
        421,
        1220,
        4245,
        10271,
        294,
        24300,
        543
      ],
      "temperature": 0,
      "avg_logprob": -0.2759587371233598,
      "compression_ratio": 1.6543778801843319,
      "no_speech_prob": 8.990940614239662e-7
    },
    {
      "id": 53,
      "seek": 37392,
      "start": 373.92,
      "end": 378.84000000000003,
      "text": " è una considerazione. Quando per la prima volta approcciamo una tecnologia, in questo",
      "tokens": [
        4873,
        2002,
        1949,
        12928,
        13,
        18725,
        680,
        635,
        19507,
        18765,
        2075,
        66,
        42052,
        2002,
        44905,
        11,
        294,
        10263
      ],
      "temperature": 0,
      "avg_logprob": -0.286650562286377,
      "compression_ratio": 1.5972850678733033,
      "no_speech_prob": 4.3818309336529637e-7
    },
    {
      "id": 54,
      "seek": 37392,
      "start": 378.84000000000003,
      "end": 384.68,
      "text": " caso è GraphQL ma potrei dire la stessa cosa di Rust quest'ultimo periodo, abbiamo un",
      "tokens": [
        9666,
        4873,
        21884,
        13695,
        463,
        1847,
        10271,
        1264,
        635,
        342,
        8391,
        10163,
        1026,
        34952,
        866,
        6,
        723,
        6934,
        2896,
        78,
        11,
        22815,
        517
      ],
      "temperature": 0,
      "avg_logprob": -0.286650562286377,
      "compression_ratio": 1.5972850678733033,
      "no_speech_prob": 4.3818309336529637e-7
    },
    {
      "id": 55,
      "seek": 37392,
      "start": 384.68,
      "end": 391.12,
      "text": " primo momento di spaesamento e se devo portare la mia esperienza, quando per la prima volta",
      "tokens": [
        38671,
        9333,
        1026,
        637,
        64,
        279,
        8824,
        308,
        369,
        49717,
        2436,
        543,
        635,
        21290,
        10045,
        42331,
        11,
        7770,
        680,
        635,
        19507,
        18765
      ],
      "temperature": 0,
      "avg_logprob": -0.286650562286377,
      "compression_ratio": 1.5972850678733033,
      "no_speech_prob": 4.3818309336529637e-7
    },
    {
      "id": 56,
      "seek": 37392,
      "start": 391.12,
      "end": 402.52000000000004,
      "text": " ho visto gli schemi di GraphQL, ho detto ma che cazzo è sta roba? Ed era uno dei primi",
      "tokens": [
        1106,
        17558,
        17161,
        956,
        13372,
        1026,
        21884,
        13695,
        11,
        1106,
        41031,
        463,
        947,
        269,
        921,
        4765,
        4873,
        11135,
        3870,
        64,
        30,
        3977,
        4249,
        8526,
        13874,
        2886,
        72
      ],
      "temperature": 0,
      "avg_logprob": -0.286650562286377,
      "compression_ratio": 1.5972850678733033,
      "no_speech_prob": 4.3818309336529637e-7
    },
    {
      "id": 57,
      "seek": 40252,
      "start": 402.52,
      "end": 410.47999999999996,
      "text": " momenti di GraphQL dove la specifica tipo sembrava una ricetta scritta in aramaico,",
      "tokens": [
        1623,
        72,
        1026,
        21884,
        13695,
        23287,
        635,
        2685,
        64,
        9746,
        20775,
        424,
        2757,
        2002,
        21040,
        16593,
        5918,
        21870,
        294,
        594,
        2404,
        2789,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.23247995132055038,
      "compression_ratio": 1.4555555555555555,
      "no_speech_prob": 1.8266241852415988e-7
    },
    {
      "id": 58,
      "seek": 40252,
      "start": 410.47999999999996,
      "end": 418.88,
      "text": " si capiva ben poco, non c'erano tool belli che pronti o erano poco famosi tool come Apollo",
      "tokens": [
        1511,
        1410,
        5931,
        3271,
        10639,
        11,
        2107,
        269,
        6,
        260,
        3730,
        2290,
        48006,
        947,
        582,
        896,
        72,
        277,
        1189,
        3730,
        10639,
        1087,
        21521,
        2290,
        808,
        25187
      ],
      "temperature": 0,
      "avg_logprob": -0.23247995132055038,
      "compression_ratio": 1.4555555555555555,
      "no_speech_prob": 1.8266241852415988e-7
    },
    {
      "id": 59,
      "seek": 40252,
      "start": 418.88,
      "end": 427.15999999999997,
      "text": " che poi per un po' di tempo è stato uno dei market leader su questa tecnologia e cazzo",
      "tokens": [
        947,
        19260,
        680,
        517,
        714,
        6,
        1026,
        8972,
        4873,
        29657,
        8526,
        13874,
        2142,
        5263,
        459,
        16540,
        44905,
        308,
        269,
        921,
        4765
      ],
      "temperature": 0,
      "avg_logprob": -0.23247995132055038,
      "compression_ratio": 1.4555555555555555,
      "no_speech_prob": 1.8266241852415988e-7
    },
    {
      "id": 60,
      "seek": 42716,
      "start": 427.16,
      "end": 433.24,
      "text": " mi sono sentito completamente spaesato, ho dovuto ritornarci più di una volta su GraphQL.",
      "tokens": [
        2752,
        9259,
        2279,
        3528,
        28381,
        32543,
        279,
        2513,
        11,
        1106,
        30870,
        8262,
        11289,
        1865,
        289,
        537,
        10589,
        1026,
        2002,
        18765,
        459,
        21884,
        13695,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.33351101194109234,
      "compression_ratio": 1.4673913043478262,
      "no_speech_prob": 1.538171829906787e-7
    },
    {
      "id": 61,
      "seek": 42716,
      "start": 433.24,
      "end": 439.12,
      "text": " La mia domanda è, com'è stato il tuo primo contatto, il tuo primo incontro con GraphQL?",
      "tokens": [
        2369,
        21290,
        3285,
        5575,
        4873,
        11,
        395,
        6,
        1462,
        29657,
        1930,
        45352,
        38671,
        660,
        37491,
        11,
        1930,
        45352,
        38671,
        834,
        896,
        340,
        416,
        21884,
        13695,
        30
      ],
      "temperature": 0,
      "avg_logprob": -0.33351101194109234,
      "compression_ratio": 1.4673913043478262,
      "no_speech_prob": 1.538171829906787e-7
    },
    {
      "id": 62,
      "seek": 42716,
      "start": 439.12,
      "end": 449.36,
      "text": " Allora la prendo un po' lunga, io sono, come hai detto tu, anni che lavoro per mia forma,",
      "tokens": [
        1057,
        3252,
        635,
        9866,
        78,
        517,
        714,
        6,
        16730,
        64,
        11,
        19785,
        9259,
        11,
        808,
        21822,
        41031,
        2604,
        11,
        31164,
        947,
        42060,
        680,
        21290,
        8366,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.33351101194109234,
      "compression_ratio": 1.4673913043478262,
      "no_speech_prob": 1.538171829906787e-7
    },
    {
      "id": 63,
      "seek": 44936,
      "start": 449.36,
      "end": 459.36,
      "text": " prima di questa esperienza da Commodore Remote Worker dall'ufficio di casa mia, ho lavorato,",
      "tokens": [
        19507,
        1026,
        16540,
        10045,
        42331,
        1120,
        3046,
        34239,
        44858,
        6603,
        260,
        43351,
        6,
        1245,
        18322,
        1026,
        9022,
        21290,
        11,
        1106,
        29241,
        2513,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.24178698004745855,
      "compression_ratio": 1.4864864864864864,
      "no_speech_prob": 5.315743578648835e-8
    },
    {
      "id": 64,
      "seek": 44936,
      "start": 459.36,
      "end": 467.04,
      "text": " ho vissuto due anni in Olanda e ho vissuto e sono andato in Olanda proprio nei due anni",
      "tokens": [
        1106,
        371,
        891,
        8262,
        3462,
        31164,
        294,
        422,
        1661,
        64,
        308,
        1106,
        371,
        891,
        8262,
        308,
        9259,
        293,
        2513,
        294,
        422,
        1661,
        64,
        28203,
        34517,
        3462,
        31164
      ],
      "temperature": 0,
      "avg_logprob": -0.24178698004745855,
      "compression_ratio": 1.4864864864864864,
      "no_speech_prob": 5.315743578648835e-8
    },
    {
      "id": 65,
      "seek": 44936,
      "start": 467.04,
      "end": 474.92,
      "text": " della grande rivoluzione del web moderno come lo conosciamo oggi. Arrivo là come sviluppatore",
      "tokens": [
        11618,
        8883,
        367,
        21356,
        3334,
        5328,
        1103,
        3670,
        4363,
        78,
        808,
        450,
        49892,
        42052,
        34768,
        13,
        45188,
        3080,
        3684,
        808,
        17342,
        388,
        10504,
        43148
      ],
      "temperature": 0,
      "avg_logprob": -0.24178698004745855,
      "compression_ratio": 1.4864864864864864,
      "no_speech_prob": 5.315743578648835e-8
    },
    {
      "id": 66,
      "seek": 47492,
      "start": 474.92,
      "end": 485.56,
      "text": " angular JS, lavoravo in KLM, l'airline, e il mio tech lead, un pazzo scatenato, decide",
      "tokens": [
        24413,
        33063,
        11,
        29241,
        25713,
        294,
        591,
        43,
        44,
        11,
        287,
        6,
        1246,
        1889,
        11,
        308,
        1930,
        29908,
        7553,
        1477,
        11,
        517,
        30032,
        4765,
        795,
        7186,
        2513,
        11,
        4536
      ],
      "temperature": 0,
      "avg_logprob": -0.2877870354770629,
      "compression_ratio": 1.4795081967213115,
      "no_speech_prob": 7.224378464343317e-7
    },
    {
      "id": 67,
      "seek": 47492,
      "start": 485.56,
      "end": 491.12,
      "text": " che dobbiamo usare questa nuovissima tecnologia che si chiamava React, che nessuno conosceva,",
      "tokens": [
        947,
        360,
        6692,
        7415,
        505,
        543,
        16540,
        3822,
        5179,
        891,
        4775,
        44905,
        947,
        1511,
        417,
        2918,
        4061,
        30644,
        11,
        947,
        39787,
        12638,
        49892,
        384,
        2757,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.2877870354770629,
      "compression_ratio": 1.4795081967213115,
      "no_speech_prob": 7.224378464343317e-7
    },
    {
      "id": 68,
      "seek": 47492,
      "start": 491.12,
      "end": 496.20000000000005,
      "text": " al che io gli dico, guarda io sinceramente non ho idea di come funziona e lui, neanch'io,",
      "tokens": [
        419,
        947,
        19785,
        17161,
        274,
        2789,
        11,
        6290,
        64,
        19785,
        30220,
        3439,
        2107,
        1106,
        1558,
        1026,
        808,
        49345,
        21758,
        308,
        8783,
        11,
        408,
        4778,
        6,
        1004,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.2877870354770629,
      "compression_ratio": 1.4795081967213115,
      "no_speech_prob": 7.224378464343317e-7
    },
    {
      "id": 69,
      "seek": 47492,
      "start": 496.20000000000005,
      "end": 501.6,
      "text": " però ci mettiamo lì, ce la studiamo e vediamo. Quindi in quell'occasione cosa scopriamo?",
      "tokens": [
        12673,
        6983,
        27812,
        7415,
        287,
        4749,
        11,
        1769,
        635,
        972,
        7415,
        308,
        14267,
        7415,
        13,
        32534,
        294,
        631,
        285,
        6,
        905,
        16369,
        5328,
        10163,
        795,
        404,
        470,
        10502,
        30
      ],
      "temperature": 0,
      "avg_logprob": -0.2877870354770629,
      "compression_ratio": 1.4795081967213115,
      "no_speech_prob": 7.224378464343317e-7
    },
    {
      "id": 70,
      "seek": 50160,
      "start": 501.6,
      "end": 510.96000000000004,
      "text": " Che Facebook, che fino alla settimana prima era l'azienda che ci faceva mettere i like",
      "tokens": [
        3351,
        4384,
        11,
        947,
        42560,
        11591,
        5584,
        36497,
        19507,
        4249,
        287,
        6,
        921,
        30498,
        947,
        6983,
        1851,
        2757,
        27812,
        323,
        741,
        411
      ],
      "temperature": 0,
      "avg_logprob": -0.36370222909109934,
      "compression_ratio": 1.4619565217391304,
      "no_speech_prob": 9.476383411310962e-8
    },
    {
      "id": 71,
      "seek": 50160,
      "start": 510.96000000000004,
      "end": 519.96,
      "text": " alla persona con cui volevamo provarci, perché era uno dei grandi utilizzi di dieci anni",
      "tokens": [
        11591,
        12184,
        416,
        22929,
        1650,
        28316,
        10502,
        1439,
        289,
        537,
        11,
        14303,
        4249,
        8526,
        13874,
        45155,
        19906,
        3992,
        1026,
        978,
        537,
        31164
      ],
      "temperature": 0,
      "avg_logprob": -0.36370222909109934,
      "compression_ratio": 1.4619565217391304,
      "no_speech_prob": 9.476383411310962e-8
    },
    {
      "id": 72,
      "seek": 50160,
      "start": 519.96,
      "end": 527.0400000000001,
      "text": " fa di Facebook, adesso vai una volta. Penso che lo sia ancora anche se tra i boomer. Esatto,",
      "tokens": [
        2050,
        1026,
        4384,
        11,
        39552,
        4405,
        2002,
        18765,
        13,
        10571,
        539,
        947,
        450,
        25176,
        30656,
        11585,
        369,
        944,
        741,
        9351,
        260,
        13,
        2313,
        37491,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.36370222909109934,
      "compression_ratio": 1.4619565217391304,
      "no_speech_prob": 9.476383411310962e-8
    },
    {
      "id": 73,
      "seek": 52704,
      "start": 527.04,
      "end": 535.5999999999999,
      "text": " adesso infatti io che sono un boomer ormai non lo uso quasi neanche più, però all'epoca",
      "tokens": [
        39552,
        1536,
        21515,
        19785,
        947,
        9259,
        517,
        9351,
        260,
        420,
        76,
        1301,
        2107,
        450,
        22728,
        20954,
        408,
        22806,
        10589,
        11,
        12673,
        439,
        6,
        595,
        24035
      ],
      "temperature": 0,
      "avg_logprob": -0.29468688510713126,
      "compression_ratio": 1.5,
      "no_speech_prob": 4.9937138868472175e-8
    },
    {
      "id": 74,
      "seek": 52704,
      "start": 535.5999999999999,
      "end": 545.3199999999999,
      "text": " ero un intente molto forte di Facebook e quindi scopriamo insieme che Facebook fa della tecnologia,",
      "tokens": [
        1189,
        78,
        517,
        560,
        1576,
        16394,
        23235,
        1026,
        4384,
        308,
        15727,
        795,
        404,
        470,
        10502,
        1028,
        44940,
        947,
        4384,
        2050,
        11618,
        44905,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.29468688510713126,
      "compression_ratio": 1.5,
      "no_speech_prob": 4.9937138868472175e-8
    },
    {
      "id": 75,
      "seek": 52704,
      "start": 545.3199999999999,
      "end": 552.88,
      "text": " quindi ho iniziato un po' a seguirlo e quando dopo React, parlo dell'epoca in cui si usavano",
      "tokens": [
        15727,
        1106,
        294,
        24300,
        2513,
        517,
        714,
        6,
        257,
        18584,
        752,
        308,
        7770,
        35196,
        30644,
        11,
        971,
        752,
        19781,
        6,
        595,
        24035,
        294,
        22929,
        1511,
        505,
        706,
        3730
      ],
      "temperature": 0,
      "avg_logprob": -0.29468688510713126,
      "compression_ratio": 1.5,
      "no_speech_prob": 4.9937138868472175e-8
    },
    {
      "id": 76,
      "seek": 55288,
      "start": 552.88,
      "end": 559.68,
      "text": " mixins in React, adesso non so in quanti li abbiano mai visti, e usavamo Fluxore come",
      "tokens": [
        2890,
        1292,
        294,
        30644,
        11,
        39552,
        2107,
        370,
        294,
        4426,
        72,
        375,
        16903,
        6254,
        12698,
        40247,
        72,
        11,
        308,
        505,
        706,
        10502,
        3235,
        2449,
        418,
        808
      ],
      "temperature": 0,
      "avg_logprob": -0.3082224073864165,
      "compression_ratio": 1.436842105263158,
      "no_speech_prob": 0.000001018796410789946
    },
    {
      "id": 77,
      "seek": 55288,
      "start": 559.68,
      "end": 572.12,
      "text": " state management, Redux non esisteva, quindi subito a ruota Facebook è uscito con la presentazione",
      "tokens": [
        1785,
        4592,
        11,
        4477,
        2449,
        2107,
        785,
        8375,
        2757,
        11,
        15727,
        1422,
        3528,
        257,
        5420,
        5377,
        4384,
        4873,
        505,
        32030,
        416,
        635,
        1974,
        12928
      ],
      "temperature": 0,
      "avg_logprob": -0.3082224073864165,
      "compression_ratio": 1.436842105263158,
      "no_speech_prob": 0.000001018796410789946
    },
    {
      "id": 78,
      "seek": 55288,
      "start": 572.12,
      "end": 578,
      "text": " GraphQL, GraphQL che ho iniziato ad usare subito, tant'è che probabilmente c'è ancora",
      "tokens": [
        21884,
        13695,
        11,
        21884,
        13695,
        947,
        1106,
        294,
        24300,
        2513,
        614,
        505,
        543,
        1422,
        3528,
        11,
        12095,
        6,
        1462,
        947,
        31959,
        4082,
        269,
        6,
        1462,
        30656
      ],
      "temperature": 0,
      "avg_logprob": -0.3082224073864165,
      "compression_ratio": 1.436842105263158,
      "no_speech_prob": 0.000001018796410789946
    },
    {
      "id": 79,
      "seek": 57800,
      "start": 578,
      "end": 583.8,
      "text": " il mio nome da qualche parte, nella mia vita a Amsterdam ho aperto il primo meetup di GraphQL,",
      "tokens": [
        1930,
        29908,
        19003,
        1120,
        38737,
        6975,
        11,
        23878,
        21290,
        32712,
        257,
        28291,
        1106,
        43139,
        1353,
        1930,
        38671,
        1677,
        1010,
        1026,
        21884,
        13695,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.30326980810899,
      "compression_ratio": 1.634703196347032,
      "no_speech_prob": 4.812505949303159e-7
    },
    {
      "id": 80,
      "seek": 57800,
      "start": 583.8,
      "end": 590.52,
      "text": " i meetup ad Amsterdam come tutte le città dove ci sono molte start up sono molto vivi,",
      "tokens": [
        741,
        1677,
        1010,
        614,
        28291,
        808,
        38632,
        476,
        269,
        593,
        1467,
        23287,
        6983,
        9259,
        8015,
        975,
        722,
        493,
        9259,
        16394,
        11005,
        72,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.30326980810899,
      "compression_ratio": 1.634703196347032,
      "no_speech_prob": 4.812505949303159e-7
    },
    {
      "id": 81,
      "seek": 57800,
      "start": 590.52,
      "end": 595.44,
      "text": " quindi apri con un paio di colleghi questa cosa, quindi il primo approccio è stato lì",
      "tokens": [
        15727,
        1882,
        470,
        416,
        517,
        2502,
        1004,
        1026,
        13300,
        4954,
        16540,
        10163,
        11,
        15727,
        1930,
        38671,
        2075,
        66,
        8529,
        4873,
        29657,
        287,
        4749
      ],
      "temperature": 0,
      "avg_logprob": -0.30326980810899,
      "compression_ratio": 1.634703196347032,
      "no_speech_prob": 4.812505949303159e-7
    },
    {
      "id": 82,
      "seek": 57800,
      "start": 595.44,
      "end": 602.88,
      "text": " ed è stato un innamoramento totale, perché era figo, diciamo le cose come stanno, poi",
      "tokens": [
        1257,
        4873,
        29657,
        517,
        294,
        5378,
        284,
        8824,
        1993,
        1220,
        11,
        14303,
        4249,
        2147,
        78,
        11,
        14285,
        7415,
        476,
        30261,
        808,
        342,
        13484,
        11,
        19260
      ],
      "temperature": 0,
      "avg_logprob": -0.30326980810899,
      "compression_ratio": 1.634703196347032,
      "no_speech_prob": 4.812505949303159e-7
    },
    {
      "id": 83,
      "seek": 60288,
      "start": 602.88,
      "end": 609.6,
      "text": " soprattutto da persona che ne vedeva solo i pregi, non vedeva i difetti, non avevo in",
      "tokens": [
        50002,
        1120,
        12184,
        947,
        408,
        371,
        4858,
        2757,
        6944,
        741,
        659,
        7834,
        11,
        2107,
        371,
        4858,
        2757,
        741,
        679,
        12495,
        11,
        2107,
        3472,
        3080,
        294
      ],
      "temperature": 0,
      "avg_logprob": -0.24553376209886768,
      "compression_ratio": 1.5197740112994351,
      "no_speech_prob": 1.167722807338123e-8
    },
    {
      "id": 84,
      "seek": 60288,
      "start": 609.6,
      "end": 617.04,
      "text": " produzione una piattaforma con dei terabyte di dati, avevo semplicemente le mie applicazioni",
      "tokens": [
        1082,
        19706,
        2002,
        3895,
        18405,
        837,
        64,
        416,
        13874,
        1796,
        34529,
        1026,
        1137,
        72,
        11,
        3472,
        3080,
        4361,
        4770,
        16288,
        476,
        12597,
        2580,
        27569
      ],
      "temperature": 0,
      "avg_logprob": -0.24553376209886768,
      "compression_ratio": 1.5197740112994351,
      "no_speech_prob": 1.167722807338123e-8
    },
    {
      "id": 85,
      "seek": 60288,
      "start": 617.04,
      "end": 626.08,
      "text": " fatte e quelle usavo, quindi si andava di server quello buttato fuori da Facebook, perché",
      "tokens": [
        4046,
        975,
        308,
        29237,
        505,
        25713,
        11,
        15727,
        1511,
        293,
        4061,
        1026,
        7154,
        22813,
        6660,
        2513,
        8536,
        7386,
        1120,
        4384,
        11,
        14303
      ],
      "temperature": 0,
      "avg_logprob": -0.24553376209886768,
      "compression_ratio": 1.5197740112994351,
      "no_speech_prob": 1.167722807338123e-8
    },
    {
      "id": 86,
      "seek": 62608,
      "start": 626.08,
      "end": 635.1600000000001,
      "text": " Apollo ancora non esisteva, all'epoca Apollo aveva Meteor e hanno avuto un'idea geniale",
      "tokens": [
        25187,
        30656,
        2107,
        785,
        8375,
        2757,
        11,
        439,
        6,
        595,
        24035,
        25187,
        3472,
        2757,
        43328,
        284,
        308,
        26595,
        1305,
        8262,
        517,
        6,
        482,
        64,
        1049,
        25051
      ],
      "temperature": 0,
      "avg_logprob": -0.2212430780584162,
      "compression_ratio": 1.6177777777777778,
      "no_speech_prob": 3.2562897445131966e-7
    },
    {
      "id": 87,
      "seek": 62608,
      "start": 635.1600000000001,
      "end": 642.88,
      "text": " nello switchare verso un GraphQL quando Meteor fondamentalmente era nato, aveva fatto l'esplosione",
      "tokens": [
        408,
        1913,
        3679,
        543,
        49786,
        517,
        21884,
        13695,
        7770,
        43328,
        284,
        9557,
        44538,
        4082,
        4249,
        2249,
        78,
        11,
        3472,
        2757,
        23228,
        287,
        6,
        279,
        564,
        329,
        5328
      ],
      "temperature": 0,
      "avg_logprob": -0.2212430780584162,
      "compression_ratio": 1.6177777777777778,
      "no_speech_prob": 3.2562897445131966e-7
    },
    {
      "id": 88,
      "seek": 62608,
      "start": 642.88,
      "end": 649.64,
      "text": " ed era morto praticamente subito, quindi ho avuto l'occasione di lavorarci subito e poi",
      "tokens": [
        1257,
        4249,
        6599,
        78,
        45734,
        1422,
        3528,
        11,
        15727,
        1106,
        1305,
        8262,
        287,
        6,
        905,
        16369,
        5328,
        1026,
        29241,
        289,
        537,
        1422,
        3528,
        308,
        19260
      ],
      "temperature": 0,
      "avg_logprob": -0.2212430780584162,
      "compression_ratio": 1.6177777777777778,
      "no_speech_prob": 3.2562897445131966e-7
    },
    {
      "id": 89,
      "seek": 62608,
      "start": 649.64,
      "end": 655.4000000000001,
      "text": " è rimasto una passione, non ho praticamente messo mai niente in produzione, finché dopo",
      "tokens": [
        4873,
        15982,
        33869,
        2002,
        1320,
        5328,
        11,
        2107,
        1106,
        45734,
        2082,
        78,
        12698,
        297,
        8413,
        294,
        1082,
        19706,
        11,
        962,
        11131,
        35196
      ],
      "temperature": 0,
      "avg_logprob": -0.2212430780584162,
      "compression_ratio": 1.6177777777777778,
      "no_speech_prob": 3.2562897445131966e-7
    },
    {
      "id": 90,
      "seek": 65540,
      "start": 655.4,
      "end": 661.6,
      "text": " sono tornato in Italia, ho iniziato con Airform e la mia prima esperienza lavorativa, che l'ho",
      "tokens": [
        9259,
        10885,
        2513,
        294,
        41355,
        11,
        1106,
        294,
        24300,
        2513,
        416,
        5774,
        837,
        308,
        635,
        21290,
        19507,
        10045,
        42331,
        29241,
        18740,
        11,
        947,
        287,
        6,
        1289
      ],
      "temperature": 0,
      "avg_logprob": -0.25209630452669585,
      "compression_ratio": 1.4915966386554622,
      "no_speech_prob": 1.6893493182124075e-7
    },
    {
      "id": 91,
      "seek": 65540,
      "start": 661.6,
      "end": 670.64,
      "text": " fatta in Priceline, che è fondamentalmente il padrone di booking, quindi stesso tipo",
      "tokens": [
        4046,
        1328,
        294,
        430,
        1341,
        5440,
        11,
        947,
        4873,
        9557,
        44538,
        4082,
        1930,
        6887,
        26446,
        1026,
        34424,
        11,
        15727,
        44413,
        9746
      ],
      "temperature": 0,
      "avg_logprob": -0.25209630452669585,
      "compression_ratio": 1.4915966386554622,
      "no_speech_prob": 1.6893493182124075e-7
    },
    {
      "id": 92,
      "seek": 65540,
      "start": 670.64,
      "end": 676.8,
      "text": " di piattaforma però sul mercato americano, ci hanno lasciato la libertà di scegliere",
      "tokens": [
        1026,
        3895,
        18405,
        837,
        64,
        12673,
        17603,
        10811,
        2513,
        16116,
        38028,
        11,
        6983,
        26595,
        48451,
        2513,
        635,
        18058,
        1467,
        1026,
        262,
        384,
        41443,
        323
      ],
      "temperature": 0,
      "avg_logprob": -0.25209630452669585,
      "compression_ratio": 1.4915966386554622,
      "no_speech_prob": 1.6893493182124075e-7
    },
    {
      "id": 93,
      "seek": 65540,
      "start": 676.8,
      "end": 680.24,
      "text": " e mi hanno preso nel momento in cui io volevo mettere GraphQL, perché avevo bisogno di",
      "tokens": [
        308,
        2752,
        26595,
        1183,
        78,
        15373,
        9333,
        294,
        22929,
        19785,
        49877,
        3080,
        27812,
        323,
        21884,
        13695,
        11,
        14303,
        3472,
        3080,
        40505,
        1771,
        1026
      ],
      "temperature": 0,
      "avg_logprob": -0.25209630452669585,
      "compression_ratio": 1.4915966386554622,
      "no_speech_prob": 1.6893493182124075e-7
    },
    {
      "id": 94,
      "seek": 68024,
      "start": 680.24,
      "end": 685.4,
      "text": " provarlo da qualche parte, ho fatto un po' questa follia da un cliente di queste dimensioni.",
      "tokens": [
        1439,
        19457,
        1120,
        38737,
        6975,
        11,
        1106,
        23228,
        517,
        714,
        6,
        16540,
        25483,
        654,
        1120,
        517,
        6423,
        68,
        1026,
        35455,
        10139,
        72,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.30710446632514565,
      "compression_ratio": 1.5909090909090908,
      "no_speech_prob": 6.312630063121105e-8
    },
    {
      "id": 95,
      "seek": 68024,
      "start": 685.4,
      "end": 686.4,
      "text": " Minchia coraggioso!",
      "tokens": [
        2829,
        339,
        654,
        1181,
        559,
        70,
        23540,
        0
      ],
      "temperature": 0,
      "avg_logprob": -0.30710446632514565,
      "compression_ratio": 1.5909090909090908,
      "no_speech_prob": 6.312630063121105e-8
    },
    {
      "id": 96,
      "seek": 68024,
      "start": 686.4,
      "end": 693.36,
      "text": " Ah sì, io sono uno che ha la testa bassa, io se sto sul progetto non ho paura a fare",
      "tokens": [
        2438,
        49267,
        11,
        19785,
        9259,
        8526,
        947,
        324,
        635,
        1500,
        64,
        10136,
        64,
        11,
        19785,
        369,
        22784,
        17603,
        447,
        847,
        1353,
        2107,
        1106,
        2502,
        2991,
        257,
        11994
      ],
      "temperature": 0,
      "avg_logprob": -0.30710446632514565,
      "compression_ratio": 1.5909090909090908,
      "no_speech_prob": 6.312630063121105e-8
    },
    {
      "id": 97,
      "seek": 68024,
      "start": 693.36,
      "end": 700.96,
      "text": " scelte, se devo fare scelte per altri sono sempre molto più tranquillo, ma se sto sul",
      "tokens": [
        795,
        338,
        975,
        11,
        369,
        49717,
        11994,
        795,
        338,
        975,
        680,
        33707,
        9259,
        9553,
        16394,
        10589,
        17640,
        15831,
        11,
        463,
        369,
        22784,
        17603
      ],
      "temperature": 0,
      "avg_logprob": -0.30710446632514565,
      "compression_ratio": 1.5909090909090908,
      "no_speech_prob": 6.312630063121105e-8
    },
    {
      "id": 98,
      "seek": 68024,
      "start": 700.96,
      "end": 705.6800000000001,
      "text": " progetto mi prendo proprio le mie responsabilità senza problemi, quindi andiamo là, nel frattempo",
      "tokens": [
        447,
        847,
        1353,
        2752,
        9866,
        78,
        28203,
        476,
        12597,
        29829,
        12445,
        36208,
        1154,
        72,
        11,
        15727,
        293,
        7415,
        3684,
        11,
        15373,
        431,
        1591,
        443,
        2259
      ],
      "temperature": 0,
      "avg_logprob": -0.30710446632514565,
      "compression_ratio": 1.5909090909090908,
      "no_speech_prob": 6.312630063121105e-8
    },
    {
      "id": 99,
      "seek": 70568,
      "start": 705.68,
      "end": 710.64,
      "text": " era già uscito Apollo, e quindi abbiamo messo in piedi la prima applicazione, ti parlo",
      "tokens": [
        4249,
        30469,
        505,
        32030,
        25187,
        11,
        308,
        15727,
        22815,
        2082,
        78,
        294,
        24186,
        72,
        635,
        19507,
        2580,
        12928,
        11,
        8757,
        971,
        752
      ],
      "temperature": 0,
      "avg_logprob": -0.2976111306084527,
      "compression_ratio": 1.5829596412556053,
      "no_speech_prob": 1.0407762118802566e-7
    },
    {
      "id": 100,
      "seek": 70568,
      "start": 710.64,
      "end": 718.8,
      "text": " di sei anni fa ormai, la prima applicazione con un serverino, non era particolarmente",
      "tokens": [
        1026,
        10842,
        31164,
        2050,
        420,
        76,
        1301,
        11,
        635,
        19507,
        2580,
        12928,
        416,
        517,
        7154,
        2982,
        11,
        2107,
        4249,
        1276,
        15276,
        4082
      ],
      "temperature": 0,
      "avg_logprob": -0.2976111306084527,
      "compression_ratio": 1.5829596412556053,
      "no_speech_prob": 1.0407762118802566e-7
    },
    {
      "id": 101,
      "seek": 70568,
      "start": 718.8,
      "end": 724.68,
      "text": " complesso ma nell'interfaccia ci ha aiutato tantissimo. All'epoca la complessità era",
      "tokens": [
        1209,
        5557,
        463,
        44666,
        6,
        5106,
        69,
        326,
        2755,
        6983,
        324,
        9783,
        325,
        2513,
        12095,
        34966,
        13,
        1057,
        6,
        595,
        24035,
        635,
        1209,
        442,
        12445,
        4249
      ],
      "temperature": 0,
      "avg_logprob": -0.2976111306084527,
      "compression_ratio": 1.5829596412556053,
      "no_speech_prob": 1.0407762118802566e-7
    },
    {
      "id": 102,
      "seek": 70568,
      "start": 724.68,
      "end": 731.4799999999999,
      "text": " nel front-end, ultimamente ho nominato Mercurius, i miei progetti recenti sono molto più sul",
      "tokens": [
        15373,
        1868,
        12,
        521,
        11,
        3725,
        332,
        3439,
        1106,
        5369,
        259,
        2513,
        18897,
        374,
        4872,
        11,
        741,
        12597,
        72,
        447,
        847,
        7317,
        5162,
        72,
        9259,
        16394,
        10589,
        17603
      ],
      "temperature": 0,
      "avg_logprob": -0.2976111306084527,
      "compression_ratio": 1.5829596412556053,
      "no_speech_prob": 1.0407762118802566e-7
    },
    {
      "id": 103,
      "seek": 73148,
      "start": 731.48,
      "end": 740.52,
      "text": " front-end. Poi su Mercurius ci andiamo perché ho avuto modo di sfagiolare un po' la codebase",
      "tokens": [
        1868,
        12,
        521,
        13,
        430,
        4869,
        459,
        18897,
        374,
        4872,
        6983,
        293,
        7415,
        14303,
        1106,
        1305,
        8262,
        16664,
        1026,
        47095,
        20291,
        43141,
        517,
        714,
        6,
        635,
        3089,
        17429
      ],
      "temperature": 0,
      "avg_logprob": -0.2889527514972518,
      "compression_ratio": 1.6289592760180995,
      "no_speech_prob": 3.307587519429944e-7
    },
    {
      "id": 104,
      "seek": 73148,
      "start": 740.52,
      "end": 746.36,
      "text": " ancora prima di un refactoring recente che è stato fatto, io gli avevo fatto un po'",
      "tokens": [
        30656,
        19507,
        1026,
        517,
        1895,
        578,
        3662,
        850,
        1576,
        947,
        4873,
        29657,
        23228,
        11,
        19785,
        17161,
        3472,
        3080,
        23228,
        517,
        714,
        6
      ],
      "temperature": 0,
      "avg_logprob": -0.2889527514972518,
      "compression_ratio": 1.6289592760180995,
      "no_speech_prob": 3.307587519429944e-7
    },
    {
      "id": 105,
      "seek": 73148,
      "start": 746.36,
      "end": 752,
      "text": " di cosettine su Mercurius un po' di tempo fa, quindi poi ci andiamo e parliamo di Mercurius,",
      "tokens": [
        1026,
        3792,
        3093,
        533,
        459,
        18897,
        374,
        4872,
        517,
        714,
        6,
        1026,
        8972,
        2050,
        11,
        15727,
        19260,
        6983,
        293,
        7415,
        308,
        971,
        49926,
        1026,
        18897,
        374,
        4872,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.2889527514972518,
      "compression_ratio": 1.6289592760180995,
      "no_speech_prob": 3.307587519429944e-7
    },
    {
      "id": 106,
      "seek": 73148,
      "start": 752,
      "end": 757.76,
      "text": " che è un progetto del quale andiamo molto proud a livello aziendale. Assolutamente sì,",
      "tokens": [
        947,
        4873,
        517,
        447,
        847,
        1353,
        1103,
        421,
        1220,
        293,
        7415,
        16394,
        4570,
        257,
        1621,
        1913,
        7883,
        1174,
        1220,
        13,
        6281,
        2308,
        3439,
        49267,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.2889527514972518,
      "compression_ratio": 1.6289592760180995,
      "no_speech_prob": 3.307587519429944e-7
    },
    {
      "id": 107,
      "seek": 75776,
      "start": 757.76,
      "end": 762.92,
      "text": " tra l'altro per rispondere alla vera domanda che mi hai fatto, cioè come mi sono approcciato,",
      "tokens": [
        944,
        287,
        6,
        47484,
        680,
        2253,
        79,
        33447,
        11591,
        1306,
        64,
        3285,
        5575,
        947,
        2752,
        21822,
        23228,
        11,
        41827,
        808,
        2752,
        9259,
        2075,
        66,
        537,
        2513,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.25669236768756,
      "compression_ratio": 1.6787330316742082,
      "no_speech_prob": 0.000001963798013093765
    },
    {
      "id": 108,
      "seek": 75776,
      "start": 762.92,
      "end": 769.56,
      "text": " tu hai detto che ti sentivi un po' spaisato, io raramente mi sento spaisato nelle tecnologie,",
      "tokens": [
        2604,
        21822,
        41031,
        947,
        8757,
        2279,
        33448,
        517,
        714,
        6,
        637,
        1527,
        2513,
        11,
        19785,
        367,
        289,
        3439,
        2752,
        2279,
        78,
        637,
        1527,
        2513,
        46350,
        20105,
        20121,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.25669236768756,
      "compression_ratio": 1.6787330316742082,
      "no_speech_prob": 0.000001963798013093765
    },
    {
      "id": 109,
      "seek": 75776,
      "start": 769.56,
      "end": 775.96,
      "text": " ma non perché sono un figo e le capisco subito, perché non capisco un cazzo fondamentalmente,",
      "tokens": [
        463,
        2107,
        14303,
        9259,
        517,
        2147,
        78,
        308,
        476,
        1410,
        8610,
        1422,
        3528,
        11,
        14303,
        2107,
        1410,
        8610,
        517,
        269,
        921,
        4765,
        9557,
        44538,
        4082,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.25669236768756,
      "compression_ratio": 1.6787330316742082,
      "no_speech_prob": 0.000001963798013093765
    },
    {
      "id": 110,
      "seek": 75776,
      "start": 775.96,
      "end": 783.48,
      "text": " e io vedo solo ed esclusivamente le parti buone e dopo una settimana che ci sono sopra",
      "tokens": [
        308,
        19785,
        14267,
        78,
        6944,
        1257,
        4721,
        3063,
        23957,
        476,
        24408,
        758,
        546,
        308,
        35196,
        2002,
        5584,
        36497,
        947,
        6983,
        9259,
        370,
        43255
      ],
      "temperature": 0,
      "avg_logprob": -0.25669236768756,
      "compression_ratio": 1.6787330316742082,
      "no_speech_prob": 0.000001963798013093765
    },
    {
      "id": 111,
      "seek": 78348,
      "start": 783.48,
      "end": 790.64,
      "text": " esordisco con la frase che chi mi conosce benissimo sa che la dico ogni 3 persone, che",
      "tokens": [
        785,
        765,
        8610,
        416,
        635,
        38406,
        947,
        13228,
        2752,
        49892,
        384,
        3271,
        34966,
        601,
        947,
        635,
        274,
        2789,
        33189,
        805,
        29944,
        11,
        947
      ],
      "temperature": 0,
      "avg_logprob": -0.28958287605872524,
      "compression_ratio": 1.5806451612903225,
      "no_speech_prob": 0.0000022252718281379202
    },
    {
      "id": 112,
      "seek": 78348,
      "start": 790.64,
      "end": 796.28,
      "text": " è in questo caso GraphQL, GraphQL per me non ha più segreti, e allora quando dico",
      "tokens": [
        4873,
        294,
        10263,
        9666,
        21884,
        13695,
        11,
        21884,
        13695,
        680,
        385,
        2107,
        324,
        10589,
        3896,
        1505,
        72,
        11,
        308,
        44141,
        7770,
        274,
        2789
      ],
      "temperature": 0,
      "avg_logprob": -0.28958287605872524,
      "compression_ratio": 1.5806451612903225,
      "no_speech_prob": 0.0000022252718281379202
    },
    {
      "id": 113,
      "seek": 78348,
      "start": 796.28,
      "end": 803.02,
      "text": " questa frase vuol dire che in quel momento inizio a capire qualcosina e da lì i dubbi",
      "tokens": [
        16540,
        38406,
        9732,
        401,
        1264,
        947,
        294,
        7178,
        9333,
        294,
        590,
        1004,
        257,
        1410,
        621,
        4101,
        6877,
        1426,
        308,
        1120,
        287,
        4749,
        741,
        18540,
        5614
      ],
      "temperature": 0,
      "avg_logprob": -0.28958287605872524,
      "compression_ratio": 1.5806451612903225,
      "no_speech_prob": 0.0000022252718281379202
    },
    {
      "id": 114,
      "seek": 78348,
      "start": 803.02,
      "end": 808.48,
      "text": " che non avevo, perché fino a quel momento non ne avevo, saltano tutti fuori e poi me",
      "tokens": [
        947,
        2107,
        3472,
        3080,
        11,
        14303,
        42560,
        257,
        7178,
        9333,
        2107,
        408,
        3472,
        3080,
        11,
        5139,
        3730,
        19822,
        8536,
        7386,
        308,
        19260,
        385
      ],
      "temperature": 0,
      "avg_logprob": -0.28958287605872524,
      "compression_ratio": 1.5806451612903225,
      "no_speech_prob": 0.0000022252718281379202
    },
    {
      "id": 115,
      "seek": 80848,
      "start": 808.48,
      "end": 814.5600000000001,
      "text": " l'ho avuto con tutto, abbiamo parlato in pre-meeting di MongoDB, ma con MongoDB è stato lo stesso",
      "tokens": [
        287,
        6,
        1289,
        1305,
        8262,
        416,
        23048,
        11,
        22815,
        13734,
        2513,
        294,
        659,
        12,
        1398,
        9880,
        1026,
        48380,
        27735,
        11,
        463,
        416,
        48380,
        27735,
        4873,
        29657,
        450,
        44413
      ],
      "temperature": 0,
      "avg_logprob": -0.24326187741439953,
      "compression_ratio": 1.6123348017621146,
      "no_speech_prob": 2.457979064729443e-7
    },
    {
      "id": 116,
      "seek": 80848,
      "start": 814.5600000000001,
      "end": 821.12,
      "text": " identico approccio per me, poi dopo ho trovato i vari difetti che aveva, trovo comunque che",
      "tokens": [
        2473,
        2789,
        2075,
        66,
        8529,
        680,
        385,
        11,
        19260,
        35196,
        1106,
        35449,
        2513,
        741,
        3034,
        679,
        12495,
        947,
        3472,
        2757,
        11,
        4495,
        3080,
        45736,
        947
      ],
      "temperature": 0,
      "avg_logprob": -0.24326187741439953,
      "compression_ratio": 1.6123348017621146,
      "no_speech_prob": 2.457979064729443e-7
    },
    {
      "id": 117,
      "seek": 80848,
      "start": 821.12,
      "end": 827.5600000000001,
      "text": " sia un gran prodotto, ma all'epoca ci facevamo anche il caffè con MongoDB, quindi questo",
      "tokens": [
        25176,
        517,
        9370,
        15792,
        18838,
        11,
        463,
        439,
        6,
        595,
        24035,
        6983,
        1851,
        85,
        10502,
        11585,
        1930,
        1335,
        602,
        1462,
        416,
        48380,
        27735,
        11,
        15727,
        10263
      ],
      "temperature": 0,
      "avg_logprob": -0.24326187741439953,
      "compression_ratio": 1.6123348017621146,
      "no_speech_prob": 2.457979064729443e-7
    },
    {
      "id": 118,
      "seek": 80848,
      "start": 827.5600000000001,
      "end": 832.72,
      "text": " è un po' l'approccio, è il mio approccio alla tecnologia generalmente, a volte sono",
      "tokens": [
        4873,
        517,
        714,
        6,
        287,
        6,
        1746,
        24174,
        8529,
        11,
        4873,
        1930,
        29908,
        2075,
        66,
        8529,
        11591,
        44905,
        2674,
        4082,
        11,
        257,
        37801,
        9259
      ],
      "temperature": 0,
      "avg_logprob": -0.24326187741439953,
      "compression_ratio": 1.6123348017621146,
      "no_speech_prob": 2.457979064729443e-7
    },
    {
      "id": 119,
      "seek": 83272,
      "start": 832.72,
      "end": 839.24,
      "text": " un po' più coraggioso e la mando in qualche produzione, a volte invece sono un po' più",
      "tokens": [
        517,
        714,
        6,
        10589,
        1181,
        559,
        70,
        23540,
        308,
        635,
        7411,
        78,
        294,
        38737,
        1082,
        19706,
        11,
        257,
        37801,
        36344,
        9259,
        517,
        714,
        6,
        10589
      ],
      "temperature": 0,
      "avg_logprob": -0.2781690762737605,
      "compression_ratio": 1.6487603305785123,
      "no_speech_prob": 1.0246434101190971e-7
    },
    {
      "id": 120,
      "seek": 83272,
      "start": 839.24,
      "end": 845.52,
      "text": " cauto e me la tengo per me, faccio un po' l'evangelistico alle persone che non la conoscono",
      "tokens": [
        1335,
        8262,
        308,
        385,
        635,
        13989,
        680,
        385,
        11,
        1915,
        8529,
        517,
        714,
        6,
        287,
        6,
        13379,
        14282,
        468,
        2789,
        5430,
        29944,
        947,
        2107,
        635,
        49892,
        45846
      ],
      "temperature": 0,
      "avg_logprob": -0.2781690762737605,
      "compression_ratio": 1.6487603305785123,
      "no_speech_prob": 1.0246434101190971e-7
    },
    {
      "id": 121,
      "seek": 83272,
      "start": 845.52,
      "end": 851.76,
      "text": " elogiando senza avere poi fondamentalmente delle basi per elogiare, adesso un pochino",
      "tokens": [
        806,
        27202,
        1806,
        36208,
        37914,
        19260,
        9557,
        44538,
        4082,
        16485,
        987,
        72,
        680,
        806,
        27202,
        543,
        11,
        39552,
        517,
        714,
        339,
        2982
      ],
      "temperature": 0,
      "avg_logprob": -0.2781690762737605,
      "compression_ratio": 1.6487603305785123,
      "no_speech_prob": 1.0246434101190971e-7
    },
    {
      "id": 122,
      "seek": 83272,
      "start": 851.76,
      "end": 854.72,
      "text": " le ho, ma all'epoca decisamente non c'erano.",
      "tokens": [
        476,
        1106,
        11,
        463,
        439,
        6,
        595,
        24035,
        18206,
        3439,
        2107,
        269,
        6,
        260,
        3730,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.2781690762737605,
      "compression_ratio": 1.6487603305785123,
      "no_speech_prob": 1.0246434101190971e-7
    },
    {
      "id": 123,
      "seek": 83272,
      "start": 854.72,
      "end": 860.72,
      "text": " Sai Davide, hai aperto una cosa che secondo me potremmo per un attimo trattare come una",
      "tokens": [
        27987,
        3724,
        482,
        11,
        21822,
        43139,
        1353,
        2002,
        10163,
        947,
        41601,
        385,
        1847,
        265,
        2174,
        78,
        680,
        517,
        951,
        6934,
        504,
        1591,
        543,
        808,
        2002
      ],
      "temperature": 0,
      "avg_logprob": -0.2781690762737605,
      "compression_ratio": 1.6487603305785123,
      "no_speech_prob": 1.0246434101190971e-7
    },
    {
      "id": 124,
      "seek": 86072,
      "start": 860.72,
      "end": 867.9200000000001,
      "text": " parentesi, ma anzi ti dirò, mi hai dato un'idea, per il prossimo episodio preparo proprio una",
      "tokens": [
        2596,
        21181,
        11,
        463,
        364,
        3992,
        8757,
        4746,
        4293,
        11,
        2752,
        21822,
        46971,
        517,
        6,
        482,
        64,
        11,
        680,
        1930,
        48794,
        6934,
        39200,
        1004,
        8231,
        78,
        28203,
        2002
      ],
      "temperature": 0,
      "avg_logprob": -0.3320521389145449,
      "compression_ratio": 1.5625,
      "no_speech_prob": 4.181183612672612e-7
    },
    {
      "id": 125,
      "seek": 86072,
      "start": 867.9200000000001,
      "end": 874.96,
      "text": " musichetta che è la musichetta delle parentesi, the endless parentesi, anche con un jingle.",
      "tokens": [
        1038,
        480,
        16593,
        947,
        4873,
        635,
        1038,
        480,
        16593,
        16485,
        2596,
        21181,
        11,
        264,
        16144,
        2596,
        21181,
        11,
        11585,
        416,
        517,
        49495,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.3320521389145449,
      "compression_ratio": 1.5625,
      "no_speech_prob": 4.181183612672612e-7
    },
    {
      "id": 126,
      "seek": 86072,
      "start": 874.96,
      "end": 885.12,
      "text": " La domanda è, tu hai detto, talvolta io mi butto e in produzione magari ci tiro dentro",
      "tokens": [
        2369,
        3285,
        5575,
        4873,
        11,
        2604,
        21822,
        41031,
        11,
        4023,
        9646,
        1328,
        19785,
        2752,
        457,
        1353,
        308,
        294,
        1082,
        19706,
        49932,
        6983,
        44188,
        10856
      ],
      "temperature": 0,
      "avg_logprob": -0.3320521389145449,
      "compression_ratio": 1.5625,
      "no_speech_prob": 4.181183612672612e-7
    },
    {
      "id": 127,
      "seek": 88512,
      "start": 885.12,
      "end": 891.64,
      "text": " una tecnologia innovativa, anche io l'ho fatto, non l'ho fatto con la tecnologia, l'ho fatto",
      "tokens": [
        2002,
        44905,
        5083,
        18740,
        11,
        11585,
        19785,
        287,
        6,
        1289,
        23228,
        11,
        2107,
        287,
        6,
        1289,
        23228,
        416,
        635,
        44905,
        11,
        287,
        6,
        1289,
        23228
      ],
      "temperature": 0,
      "avg_logprob": -0.2486500967116583,
      "compression_ratio": 1.6604938271604939,
      "no_speech_prob": 3.6534789416009517e-8
    },
    {
      "id": 128,
      "seek": 88512,
      "start": 891.64,
      "end": 901.96,
      "text": " con un, non so, una metodologia, un pattern chiamiamolo così, col functional ddd e tipo",
      "tokens": [
        416,
        517,
        11,
        2107,
        370,
        11,
        2002,
        1131,
        378,
        24103,
        11,
        517,
        5102,
        417,
        2918,
        2918,
        7902,
        23278,
        11,
        1173,
        11745,
        274,
        24810,
        308,
        9746
      ],
      "temperature": 0,
      "avg_logprob": -0.2486500967116583,
      "compression_ratio": 1.6604938271604939,
      "no_speech_prob": 3.6534789416009517e-8
    },
    {
      "id": 129,
      "seek": 88512,
      "start": 901.96,
      "end": 907.44,
      "text": " mi sono preso dei calci in culo pesanti che venivano da me, venivano dalla complessità",
      "tokens": [
        2752,
        9259,
        1183,
        78,
        13874,
        2104,
        537,
        294,
        11021,
        78,
        9262,
        11520,
        947,
        6138,
        592,
        3730,
        1120,
        385,
        11,
        6138,
        592,
        3730,
        35566,
        1209,
        442,
        12445
      ],
      "temperature": 0,
      "avg_logprob": -0.2486500967116583,
      "compression_ratio": 1.6604938271604939,
      "no_speech_prob": 3.6534789416009517e-8
    },
    {
      "id": 130,
      "seek": 90744,
      "start": 907.44,
      "end": 915.6400000000001,
      "text": " che si è inserita nell'applicazione, con l'inserimento di sta roba e quindi mi dico,",
      "tokens": [
        947,
        1511,
        4873,
        1028,
        260,
        2786,
        44666,
        6,
        1746,
        1050,
        12928,
        11,
        416,
        287,
        6,
        1292,
        260,
        10030,
        1026,
        11135,
        3870,
        64,
        308,
        15727,
        2752,
        274,
        2789,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.22854161262512207,
      "compression_ratio": 1.5146198830409356,
      "no_speech_prob": 1.0907273662041916e-7
    },
    {
      "id": 131,
      "seek": 90744,
      "start": 915.6400000000001,
      "end": 922.12,
      "text": " talvolta siamo portati a spingere una tecnologia senza essere profondamente consapevoli di",
      "tokens": [
        4023,
        9646,
        1328,
        33459,
        2436,
        6908,
        257,
        637,
        278,
        323,
        2002,
        44905,
        36208,
        19799,
        1740,
        684,
        3439,
        1014,
        41153,
        85,
        9384,
        1026
      ],
      "temperature": 0,
      "avg_logprob": -0.22854161262512207,
      "compression_ratio": 1.5146198830409356,
      "no_speech_prob": 1.0907273662041916e-7
    },
    {
      "id": 132,
      "seek": 90744,
      "start": 922.12,
      "end": 931.1600000000001,
      "text": " quella tecnologia, che domande ti fai per non essere vittima di questo entusiasmo?",
      "tokens": [
        32234,
        44905,
        11,
        947,
        3285,
        11123,
        8757,
        283,
        1301,
        680,
        2107,
        19799,
        371,
        593,
        4775,
        1026,
        10263,
        948,
        301,
        4609,
        3280,
        30
      ],
      "temperature": 0,
      "avg_logprob": -0.22854161262512207,
      "compression_ratio": 1.5146198830409356,
      "no_speech_prob": 1.0907273662041916e-7
    },
    {
      "id": 133,
      "seek": 93116,
      "start": 931.16,
      "end": 939.24,
      "text": " E tipo io me ne faccio alcune e sono curioso di vedere un po' come gestisci tu la cosa.",
      "tokens": [
        462,
        9746,
        19785,
        385,
        408,
        1915,
        8529,
        20005,
        2613,
        308,
        9259,
        13625,
        78,
        1026,
        35373,
        517,
        714,
        6,
        808,
        7219,
        271,
        537,
        2604,
        635,
        10163,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.3146299786037869,
      "compression_ratio": 1.3959390862944163,
      "no_speech_prob": 6.179363936098525e-7
    },
    {
      "id": 134,
      "seek": 93116,
      "start": 939.24,
      "end": 947.28,
      "text": " Allora dipende molto dall'entità di quello che butto dentro, ok?",
      "tokens": [
        1057,
        3252,
        10460,
        5445,
        16394,
        43351,
        6,
        317,
        12445,
        1026,
        22813,
        947,
        457,
        1353,
        10856,
        11,
        3133,
        30
      ],
      "temperature": 0,
      "avg_logprob": -0.3146299786037869,
      "compression_ratio": 1.3959390862944163,
      "no_speech_prob": 6.179363936098525e-7
    },
    {
      "id": 135,
      "seek": 93116,
      "start": 947.28,
      "end": 950.76,
      "text": " Cioè provo a fare un passo indietro.",
      "tokens": [
        383,
        35983,
        1439,
        78,
        257,
        11994,
        517,
        38159,
        1016,
        1684,
        340,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.3146299786037869,
      "compression_ratio": 1.3959390862944163,
      "no_speech_prob": 6.179363936098525e-7
    },
    {
      "id": 136,
      "seek": 93116,
      "start": 950.76,
      "end": 959.4,
      "text": " Ti riporto il discorso GraphQL che ho buttato dentro il mio progetto in produzione.",
      "tokens": [
        20456,
        12782,
        477,
        78,
        1930,
        2983,
        284,
        539,
        21884,
        13695,
        947,
        1106,
        6660,
        2513,
        10856,
        1930,
        29908,
        447,
        847,
        1353,
        294,
        1082,
        19706,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.3146299786037869,
      "compression_ratio": 1.3959390862944163,
      "no_speech_prob": 6.179363936098525e-7
    },
    {
      "id": 137,
      "seek": 95940,
      "start": 959.4,
      "end": 966.84,
      "text": " La valutazione è stata fatta considerando che non era un progetto ad alte performance,",
      "tokens": [
        2369,
        1323,
        325,
        12928,
        4873,
        49554,
        4046,
        1328,
        1949,
        1806,
        947,
        2107,
        4249,
        517,
        447,
        847,
        1353,
        614,
        38973,
        3389,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.45477309784331876,
      "compression_ratio": 1.5976331360946745,
      "no_speech_prob": 0.000001308168180003122
    },
    {
      "id": 138,
      "seek": 95940,
      "start": 966.84,
      "end": 968.72,
      "text": " che richiedeva alte performance.",
      "tokens": [
        947,
        4593,
        1091,
        68,
        2757,
        38973,
        3389,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.45477309784331876,
      "compression_ratio": 1.5976331360946745,
      "no_speech_prob": 0.000001308168180003122
    },
    {
      "id": 139,
      "seek": 95940,
      "start": 968.72,
      "end": 973.84,
      "text": " Quindi io non lo sapevo quanto GraphQL fosse performante bene.",
      "tokens": [
        32534,
        19785,
        2107,
        450,
        601,
        494,
        3080,
        17820,
        21884,
        13695,
        24528,
        2042,
        2879,
        2537,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.45477309784331876,
      "compression_ratio": 1.5976331360946745,
      "no_speech_prob": 0.000001308168180003122
    },
    {
      "id": 140,
      "seek": 95940,
      "start": 973.84,
      "end": 981.84,
      "text": " Sicuramente non era una roba che richiedeva delle performance, perché avevamo un, noi",
      "tokens": [
        39155,
        374,
        3439,
        2107,
        4249,
        2002,
        3870,
        64,
        947,
        4593,
        1091,
        68,
        2757,
        16485,
        3389,
        11,
        14303,
        3472,
        85,
        10502,
        517,
        11,
        22447
      ],
      "temperature": 0,
      "avg_logprob": -0.45477309784331876,
      "compression_ratio": 1.5976331360946745,
      "no_speech_prob": 0.000001308168180003122
    },
    {
      "id": 141,
      "seek": 98184,
      "start": 981.84,
      "end": 990.44,
      "text": " gli italiani chiamano FF, front end for back end, era un FF, front end for back end.",
      "tokens": [
        290,
        2081,
        22366,
        21309,
        417,
        2918,
        3730,
        479,
        37,
        11,
        1868,
        917,
        337,
        646,
        917,
        11,
        4249,
        517,
        479,
        37,
        11,
        1868,
        917,
        337,
        646,
        917,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.4375038146972656,
      "compression_ratio": 1.5058823529411764,
      "no_speech_prob": 1.6893481813440303e-7
    },
    {
      "id": 142,
      "seek": 98184,
      "start": 990.44,
      "end": 996.36,
      "text": " Quindi era un servizio che chiamava delle API dietro, lentissimi, perché immaginati",
      "tokens": [
        32534,
        4249,
        517,
        1658,
        590,
        1004,
        947,
        417,
        2918,
        4061,
        16485,
        9362,
        6339,
        340,
        11,
        23556,
        891,
        10121,
        11,
        14303,
        3397,
        559,
        259,
        6908
      ],
      "temperature": 0,
      "avg_logprob": -0.4375038146972656,
      "compression_ratio": 1.5058823529411764,
      "no_speech_prob": 1.6893481813440303e-7
    },
    {
      "id": 143,
      "seek": 98184,
      "start": 996.36,
      "end": 1010.2,
      "text": " tu vai a prendere dei dati molto complessi, noi facevamo i pacchetti, quindi tu dicevi",
      "tokens": [
        2604,
        4405,
        257,
        9866,
        323,
        13874,
        1137,
        72,
        16394,
        1209,
        442,
        72,
        11,
        22447,
        1851,
        85,
        10502,
        741,
        15165,
        339,
        12495,
        11,
        15727,
        2604,
        10313,
        4917
      ],
      "temperature": 0,
      "avg_logprob": -0.4375038146972656,
      "compression_ratio": 1.5058823529411764,
      "no_speech_prob": 1.6893481813440303e-7
    },
    {
      "id": 144,
      "seek": 101020,
      "start": 1010.2,
      "end": 1015.9200000000001,
      "text": " dove devi andare, lui automaticamente ti faceva aereo, albergo, automobili.",
      "tokens": [
        23287,
        31219,
        42742,
        11,
        8783,
        12509,
        3439,
        8757,
        1851,
        2757,
        257,
        323,
        78,
        11,
        419,
        607,
        1571,
        11,
        3553,
        996,
        2312,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.32454724993024553,
      "compression_ratio": 1.55,
      "no_speech_prob": 4.247022786785237e-7
    },
    {
      "id": 145,
      "seek": 101020,
      "start": 1015.9200000000001,
      "end": 1020.2800000000001,
      "text": " Quindi parlava con Amadeus che era un chiodo di AS400.",
      "tokens": [
        32534,
        13734,
        4061,
        416,
        2012,
        762,
        301,
        947,
        4249,
        517,
        417,
        2695,
        78,
        1026,
        7469,
        13741,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.32454724993024553,
      "compression_ratio": 1.55,
      "no_speech_prob": 4.247022786785237e-7
    },
    {
      "id": 146,
      "seek": 101020,
      "start": 1020.2800000000001,
      "end": 1023.6,
      "text": " Sì, non ho idea di cosa ci fosse dietro.",
      "tokens": [
        318,
        4749,
        11,
        2107,
        1106,
        1558,
        1026,
        10163,
        6983,
        24528,
        6339,
        340,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.32454724993024553,
      "compression_ratio": 1.55,
      "no_speech_prob": 4.247022786785237e-7
    },
    {
      "id": 147,
      "seek": 101020,
      "start": 1023.6,
      "end": 1028.76,
      "text": " È stato l'unico progetto, uno dei pochissimi progetti della mia vita in cui non ho usato",
      "tokens": [
        34495,
        29657,
        287,
        6,
        409,
        2789,
        447,
        847,
        1353,
        11,
        8526,
        13874,
        714,
        339,
        891,
        10121,
        447,
        847,
        7317,
        11618,
        21290,
        32712,
        294,
        22929,
        2107,
        1106,
        505,
        2513
      ],
      "temperature": 0,
      "avg_logprob": -0.32454724993024553,
      "compression_ratio": 1.55,
      "no_speech_prob": 4.247022786785237e-7
    },
    {
      "id": 148,
      "seek": 101020,
      "start": 1028.76,
      "end": 1034.2,
      "text": " un database, perché semplicemente interrogavo API e questi mi davano dei risultati.",
      "tokens": [
        517,
        8149,
        11,
        14303,
        4361,
        4770,
        16288,
        728,
        340,
        70,
        25713,
        9362,
        308,
        29729,
        2752,
        11753,
        3730,
        13874,
        2253,
        723,
        6908,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.32454724993024553,
      "compression_ratio": 1.55,
      "no_speech_prob": 4.247022786785237e-7
    },
    {
      "id": 149,
      "seek": 101020,
      "start": 1034.2,
      "end": 1038.96,
      "text": " Quindi la nostra scelta è stata fatta con la coscienza del proviamo, vediamo come va,",
      "tokens": [
        32534,
        635,
        34311,
        795,
        338,
        1328,
        4873,
        49554,
        4046,
        1328,
        416,
        635,
        3792,
        537,
        23691,
        1103,
        1439,
        7415,
        11,
        14267,
        7415,
        808,
        2773,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.32454724993024553,
      "compression_ratio": 1.55,
      "no_speech_prob": 4.247022786785237e-7
    },
    {
      "id": 150,
      "seek": 103896,
      "start": 1038.96,
      "end": 1045.44,
      "text": " e poi male che vada io e il mio collega dell'epoca, anche lui di NearForm che eravamo sul progetto,",
      "tokens": [
        308,
        19260,
        7133,
        947,
        371,
        1538,
        19785,
        308,
        1930,
        29908,
        1263,
        6335,
        19781,
        6,
        595,
        24035,
        11,
        11585,
        8783,
        1026,
        22200,
        49855,
        947,
        1189,
        706,
        10502,
        17603,
        447,
        847,
        1353,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.3364878356001759,
      "compression_ratio": 1.544776119402985,
      "no_speech_prob": 3.1741961947773234e-8
    },
    {
      "id": 151,
      "seek": 103896,
      "start": 1045.44,
      "end": 1050.32,
      "text": " ci siamo guardati in faccia e abbiamo detto se poi non va a tirare su delle rest API per",
      "tokens": [
        6983,
        33459,
        6290,
        6908,
        294,
        1915,
        2755,
        308,
        22815,
        41031,
        369,
        19260,
        2107,
        2773,
        257,
        13807,
        543,
        459,
        16485,
        1472,
        9362,
        680
      ],
      "temperature": 0,
      "avg_logprob": -0.3364878356001759,
      "compression_ratio": 1.544776119402985,
      "no_speech_prob": 3.1741961947773234e-8
    },
    {
      "id": 152,
      "seek": 103896,
      "start": 1050.32,
      "end": 1051.32,
      "text": " gli altri giorni.",
      "tokens": [
        17161,
        33707,
        36937,
        72,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.3364878356001759,
      "compression_ratio": 1.544776119402985,
      "no_speech_prob": 3.1741961947773234e-8
    },
    {
      "id": 153,
      "seek": 103896,
      "start": 1051.32,
      "end": 1054.32,
      "text": " Anche il servizietto era già pronto.",
      "tokens": [
        1107,
        1876,
        1930,
        1658,
        590,
        1684,
        1353,
        4249,
        30469,
        26194,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.3364878356001759,
      "compression_ratio": 1.544776119402985,
      "no_speech_prob": 3.1741961947773234e-8
    },
    {
      "id": 154,
      "seek": 103896,
      "start": 1054.32,
      "end": 1056.3600000000001,
      "text": " Ma sì, non è che ci fosse chissà che.",
      "tokens": [
        4042,
        49267,
        11,
        2107,
        4873,
        947,
        6983,
        24528,
        417,
        891,
        1467,
        947,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.3364878356001759,
      "compression_ratio": 1.544776119402985,
      "no_speech_prob": 3.1741961947773234e-8
    },
    {
      "id": 155,
      "seek": 103896,
      "start": 1056.3600000000001,
      "end": 1061.76,
      "text": " Più che altro il vantaggio di partire con GraphQL in quel caso è stato talmente forte",
      "tokens": [
        17741,
        5035,
        947,
        40924,
        1930,
        371,
        394,
        30763,
        1026,
        644,
        621,
        416,
        21884,
        13695,
        294,
        7178,
        9666,
        4873,
        29657,
        4023,
        4082,
        23235
      ],
      "temperature": 0,
      "avg_logprob": -0.3364878356001759,
      "compression_ratio": 1.544776119402985,
      "no_speech_prob": 3.1741961947773234e-8
    },
    {
      "id": 156,
      "seek": 103896,
      "start": 1061.76,
      "end": 1065.24,
      "text": " che abbiamo deciso di giocarci la carta.",
      "tokens": [
        947,
        22815,
        18206,
        78,
        1026,
        1735,
        47993,
        537,
        635,
        41815,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.3364878356001759,
      "compression_ratio": 1.544776119402985,
      "no_speech_prob": 3.1741961947773234e-8
    },
    {
      "id": 157,
      "seek": 106524,
      "start": 1065.24,
      "end": 1069,
      "text": " Perché non fare delle API per un sistema così complesso?",
      "tokens": [
        47978,
        2107,
        11994,
        16485,
        9362,
        680,
        517,
        13245,
        23278,
        1209,
        5557,
        30
      ],
      "temperature": 0,
      "avg_logprob": -0.36959994772206184,
      "compression_ratio": 1.6493506493506493,
      "no_speech_prob": 3.3075866667786613e-7
    },
    {
      "id": 158,
      "seek": 106524,
      "start": 1069,
      "end": 1072.6,
      "text": " Perché ne avevamo una vagonata di dati, una marea.",
      "tokens": [
        47978,
        408,
        3472,
        85,
        10502,
        2002,
        371,
        6709,
        3274,
        1026,
        1137,
        72,
        11,
        2002,
        463,
        12057,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.36959994772206184,
      "compression_ratio": 1.6493506493506493,
      "no_speech_prob": 3.3075866667786613e-7
    },
    {
      "id": 159,
      "seek": 106524,
      "start": 1072.6,
      "end": 1078.64,
      "text": " Quindi erano tre chiamate, quattro chiamate, ma con delle strutture enormi.",
      "tokens": [
        32534,
        1189,
        3730,
        2192,
        417,
        2918,
        473,
        11,
        421,
        1591,
        340,
        417,
        2918,
        473,
        11,
        463,
        416,
        16485,
        1056,
        13478,
        540,
        8473,
        72,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.36959994772206184,
      "compression_ratio": 1.6493506493506493,
      "no_speech_prob": 3.3075866667786613e-7
    },
    {
      "id": 160,
      "seek": 106524,
      "start": 1078.64,
      "end": 1082.68,
      "text": " Quindi allora tanto vale che ci provi e andate bene.",
      "tokens": [
        32534,
        44141,
        10331,
        15474,
        947,
        6983,
        1439,
        72,
        308,
        293,
        473,
        2537,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.36959994772206184,
      "compression_ratio": 1.6493506493506493,
      "no_speech_prob": 3.3075866667786613e-7
    },
    {
      "id": 161,
      "seek": 106524,
      "start": 1082.68,
      "end": 1089.52,
      "text": " Perché alla fine ci è tornato molto bene, non avevamo mai usato Apollo sul frontend",
      "tokens": [
        47978,
        11591,
        2489,
        6983,
        4873,
        10885,
        2513,
        16394,
        2537,
        11,
        2107,
        3472,
        85,
        10502,
        12698,
        505,
        2513,
        25187,
        17603,
        1868,
        521
      ],
      "temperature": 0,
      "avg_logprob": -0.36959994772206184,
      "compression_ratio": 1.6493506493506493,
      "no_speech_prob": 3.3075866667786613e-7
    },
    {
      "id": 162,
      "seek": 106524,
      "start": 1089.52,
      "end": 1093.52,
      "text": " e alla fine Apollo ha fatto il suo dovere, alla grande.",
      "tokens": [
        308,
        11591,
        2489,
        25187,
        324,
        23228,
        1930,
        34197,
        360,
        5887,
        11,
        11591,
        8883,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.36959994772206184,
      "compression_ratio": 1.6493506493506493,
      "no_speech_prob": 3.3075866667786613e-7
    },
    {
      "id": 163,
      "seek": 109352,
      "start": 1093.52,
      "end": 1096.8,
      "text": " È stata sicuramente una soluzione che ha pagato.",
      "tokens": [
        34495,
        49554,
        33579,
        374,
        3439,
        2002,
        1404,
        3334,
        5328,
        947,
        324,
        11812,
        2513,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.2842645732634658,
      "compression_ratio": 1.5772727272727274,
      "no_speech_prob": 0.0000010676969850464957
    },
    {
      "id": 164,
      "seek": 109352,
      "start": 1096.8,
      "end": 1101.8799999999999,
      "text": " In questo caso ero tranquillo perché sapevo di stare qualche mese sul progetto.",
      "tokens": [
        682,
        10263,
        9666,
        1189,
        78,
        17640,
        15831,
        14303,
        601,
        494,
        3080,
        1026,
        22432,
        38737,
        275,
        1130,
        17603,
        447,
        847,
        1353,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.2842645732634658,
      "compression_ratio": 1.5772727272727274,
      "no_speech_prob": 0.0000010676969850464957
    },
    {
      "id": 165,
      "seek": 109352,
      "start": 1101.8799999999999,
      "end": 1109.44,
      "text": " Io quando le prendo un po' alla leggera queste cose, quando so che poi ho il tempo di mettere",
      "tokens": [
        19239,
        7770,
        476,
        9866,
        78,
        517,
        714,
        6,
        11591,
        1676,
        1321,
        64,
        35455,
        30261,
        11,
        7770,
        370,
        947,
        19260,
        1106,
        1930,
        8972,
        1026,
        27812,
        323
      ],
      "temperature": 0,
      "avg_logprob": -0.2842645732634658,
      "compression_ratio": 1.5772727272727274,
      "no_speech_prob": 0.0000010676969850464957
    },
    {
      "id": 166,
      "seek": 109352,
      "start": 1109.44,
      "end": 1114.16,
      "text": " delle pezze se ci sono dei problemi.",
      "tokens": [
        16485,
        520,
        89,
        1381,
        369,
        6983,
        9259,
        13874,
        1154,
        72,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.2842645732634658,
      "compression_ratio": 1.5772727272727274,
      "no_speech_prob": 0.0000010676969850464957
    },
    {
      "id": 167,
      "seek": 109352,
      "start": 1114.16,
      "end": 1121.4,
      "text": " E non ho paura a dire ho fatto una cazzata, non ho paura a dire adesso scusate datemi",
      "tokens": [
        462,
        2107,
        1106,
        2502,
        2991,
        257,
        1264,
        1106,
        23228,
        2002,
        269,
        9112,
        3274,
        11,
        2107,
        1106,
        2502,
        2991,
        257,
        1264,
        39552,
        795,
        301,
        473,
        1137,
        13372
      ],
      "temperature": 0,
      "avg_logprob": -0.2842645732634658,
      "compression_ratio": 1.5772727272727274,
      "no_speech_prob": 0.0000010676969850464957
    },
    {
      "id": 168,
      "seek": 112140,
      "start": 1121.4,
      "end": 1125.64,
      "text": " una settimana per rimediare a quello che è stato fatto.",
      "tokens": [
        2002,
        5584,
        36497,
        680,
        367,
        332,
        10323,
        543,
        257,
        22813,
        947,
        4873,
        29657,
        23228,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.31618162643077763,
      "compression_ratio": 1.5307692307692307,
      "no_speech_prob": 1.4225729216832406e-7
    },
    {
      "id": 169,
      "seek": 112140,
      "start": 1125.64,
      "end": 1131.8400000000001,
      "text": " Alla stessa maniera, come raccontavo all'inizio, noi scegliamo React per fare questo progetto",
      "tokens": [
        1057,
        64,
        342,
        8391,
        587,
        10609,
        11,
        808,
        4129,
        9000,
        25713,
        439,
        6,
        9328,
        1004,
        11,
        22447,
        262,
        384,
        41443,
        10502,
        30644,
        680,
        11994,
        10263,
        447,
        847,
        1353
      ],
      "temperature": 0,
      "avg_logprob": -0.31618162643077763,
      "compression_ratio": 1.5307692307692307,
      "no_speech_prob": 1.4225729216832406e-7
    },
    {
      "id": 170,
      "seek": 112140,
      "start": 1131.8400000000001,
      "end": 1135.52,
      "text": " in KPLM quando tutta l'azienda usava Angular.",
      "tokens": [
        294,
        591,
        21593,
        44,
        7770,
        3672,
        1328,
        287,
        6,
        921,
        30498,
        505,
        4061,
        34107,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.31618162643077763,
      "compression_ratio": 1.5307692307692307,
      "no_speech_prob": 1.4225729216832406e-7
    },
    {
      "id": 171,
      "seek": 112140,
      "start": 1135.52,
      "end": 1142.44,
      "text": " Quindi noi proprio, Valespa, Valdi... ed è stata una scelta giusta perché alla fine",
      "tokens": [
        32534,
        22447,
        28203,
        11,
        691,
        4229,
        4306,
        11,
        691,
        3976,
        72,
        485,
        1257,
        4873,
        49554,
        2002,
        795,
        338,
        1328,
        1735,
        28652,
        14303,
        11591,
        2489
      ],
      "temperature": 0,
      "avg_logprob": -0.31618162643077763,
      "compression_ratio": 1.5307692307692307,
      "no_speech_prob": 1.4225729216832406e-7
    },
    {
      "id": 172,
      "seek": 112140,
      "start": 1142.44,
      "end": 1144.24,
      "text": " abbiamo visto dove era giusto.",
      "tokens": [
        22815,
        17558,
        23287,
        4249,
        1735,
        48260,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.31618162643077763,
      "compression_ratio": 1.5307692307692307,
      "no_speech_prob": 1.4225729216832406e-7
    },
    {
      "id": 173,
      "seek": 112140,
      "start": 1144.24,
      "end": 1149.68,
      "text": " C'è forse anche da dire questo, nel senso che con un po' di esperienza ce l'abbiamo",
      "tokens": [
        383,
        6,
        1462,
        337,
        405,
        11585,
        1120,
        1264,
        10263,
        11,
        15373,
        3151,
        539,
        947,
        416,
        517,
        714,
        6,
        1026,
        10045,
        42331,
        1769,
        287,
        6,
        10797,
        7415
      ],
      "temperature": 0,
      "avg_logprob": -0.31618162643077763,
      "compression_ratio": 1.5307692307692307,
      "no_speech_prob": 1.4225729216832406e-7
    },
    {
      "id": 174,
      "seek": 114968,
      "start": 1149.68,
      "end": 1155.0800000000002,
      "text": " e sappiamo quando una cosa può funzionare o quando non può funzionare.",
      "tokens": [
        308,
        46938,
        7415,
        7770,
        2002,
        10163,
        26526,
        49345,
        313,
        543,
        277,
        7770,
        2107,
        26526,
        49345,
        313,
        543,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.3010238052962662,
      "compression_ratio": 1.5974842767295598,
      "no_speech_prob": 0.000001414473558725149
    },
    {
      "id": 175,
      "seek": 114968,
      "start": 1155.0800000000002,
      "end": 1157.48,
      "text": " A puzza di merda la sentiamo da lontano.",
      "tokens": [
        316,
        2362,
        26786,
        1026,
        3551,
        2675,
        635,
        2279,
        7415,
        1120,
        287,
        896,
        3730,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.3010238052962662,
      "compression_ratio": 1.5974842767295598,
      "no_speech_prob": 0.000001414473558725149
    },
    {
      "id": 176,
      "seek": 114968,
      "start": 1157.48,
      "end": 1162.92,
      "text": " O almeno abbiamo la presunzione di sentirla da lontano.",
      "tokens": [
        422,
        419,
        43232,
        22815,
        635,
        1183,
        409,
        19706,
        1026,
        23963,
        875,
        1120,
        287,
        896,
        3730,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.3010238052962662,
      "compression_ratio": 1.5974842767295598,
      "no_speech_prob": 0.000001414473558725149
    },
    {
      "id": 177,
      "seek": 114968,
      "start": 1162.92,
      "end": 1171.16,
      "text": " E proprio in funzione di questo ti chiedo, dalla tua esperienza, quando ha veramente",
      "tokens": [
        462,
        28203,
        294,
        1019,
        19706,
        1026,
        10263,
        8757,
        417,
        36035,
        11,
        35566,
        33578,
        10045,
        42331,
        11,
        7770,
        324,
        50079
      ],
      "temperature": 0,
      "avg_logprob": -0.3010238052962662,
      "compression_ratio": 1.5974842767295598,
      "no_speech_prob": 0.000001414473558725149
    },
    {
      "id": 178,
      "seek": 117116,
      "start": 1171.16,
      "end": 1181.24,
      "text": " senso usare GraphQL e quando invece il tutto è un po' spinto dall'abbrivio dato dalla",
      "tokens": [
        3151,
        539,
        505,
        543,
        21884,
        13695,
        308,
        7770,
        36344,
        1930,
        23048,
        4873,
        517,
        714,
        6,
        637,
        17246,
        43351,
        6,
        455,
        1443,
        592,
        1004,
        46971,
        35566
      ],
      "temperature": 0,
      "avg_logprob": -0.33377478394327287,
      "compression_ratio": 1.4082840236686391,
      "no_speech_prob": 2.2862858273242637e-8
    },
    {
      "id": 179,
      "seek": 117116,
      "start": 1181.24,
      "end": 1184.24,
      "text": " moda, dal trend?",
      "tokens": [
        1072,
        64,
        11,
        11702,
        6028,
        30
      ],
      "temperature": 0,
      "avg_logprob": -0.33377478394327287,
      "compression_ratio": 1.4082840236686391,
      "no_speech_prob": 2.2862858273242637e-8
    },
    {
      "id": 180,
      "seek": 117116,
      "start": 1184.24,
      "end": 1192.72,
      "text": " Allora, ormai direi che trend non è più trend, perché ormai è una tecnologia stabile",
      "tokens": [
        1057,
        3252,
        11,
        420,
        76,
        1301,
        1264,
        72,
        947,
        6028,
        2107,
        4873,
        10589,
        6028,
        11,
        14303,
        420,
        76,
        1301,
        4873,
        2002,
        44905,
        16343,
        794
      ],
      "temperature": 0,
      "avg_logprob": -0.33377478394327287,
      "compression_ratio": 1.4082840236686391,
      "no_speech_prob": 2.2862858273242637e-8
    },
    {
      "id": 181,
      "seek": 117116,
      "start": 1192.72,
      "end": 1197.8400000000001,
      "text": " e funzionante e che tante aziende richiedono.",
      "tokens": [
        308,
        49345,
        313,
        2879,
        308,
        947,
        256,
        2879,
        7883,
        45816,
        4593,
        1091,
        8957,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.33377478394327287,
      "compression_ratio": 1.4082840236686391,
      "no_speech_prob": 2.2862858273242637e-8
    },
    {
      "id": 182,
      "seek": 119784,
      "start": 1197.84,
      "end": 1207.4399999999998,
      "text": " Una cosa che sicuramente mette un punto a favore di GraphQL è la tipologia di applicazione",
      "tokens": [
        15491,
        10163,
        947,
        33579,
        374,
        3439,
        1131,
        975,
        517,
        14326,
        257,
        33801,
        418,
        1026,
        21884,
        13695,
        4873,
        635,
        4125,
        24103,
        1026,
        2580,
        12928
      ],
      "temperature": 0,
      "avg_logprob": -0.3801988348903426,
      "compression_ratio": 1.5634517766497462,
      "no_speech_prob": 1.0087543245163033e-7
    },
    {
      "id": 183,
      "seek": 119784,
      "start": 1207.4399999999998,
      "end": 1209.76,
      "text": " che uno va a fare.",
      "tokens": [
        947,
        8526,
        2773,
        257,
        11994,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.3801988348903426,
      "compression_ratio": 1.5634517766497462,
      "no_speech_prob": 1.0087543245163033e-7
    },
    {
      "id": 184,
      "seek": 119784,
      "start": 1209.76,
      "end": 1217.24,
      "text": " Noi lavoriamo, lo dice anche nella sigletta, siamo dei full stack developer che lavorano",
      "tokens": [
        883,
        72,
        29241,
        7415,
        11,
        450,
        10313,
        11585,
        23878,
        4556,
        2631,
        1328,
        11,
        33459,
        13874,
        1577,
        8630,
        10754,
        947,
        29241,
        3730
      ],
      "temperature": 0,
      "avg_logprob": -0.3801988348903426,
      "compression_ratio": 1.5634517766497462,
      "no_speech_prob": 1.0087543245163033e-7
    },
    {
      "id": 185,
      "seek": 119784,
      "start": 1217.24,
      "end": 1221.3999999999999,
      "text": " nel mondo del web, perché noi non siamo più dei full stack developer che fanno delle desktop",
      "tokens": [
        15373,
        40499,
        1103,
        3670,
        11,
        14303,
        22447,
        2107,
        33459,
        10589,
        13874,
        1577,
        8630,
        10754,
        947,
        283,
        13484,
        16485,
        14502
      ],
      "temperature": 0,
      "avg_logprob": -0.3801988348903426,
      "compression_ratio": 1.5634517766497462,
      "no_speech_prob": 1.0087543245163033e-7
    },
    {
      "id": 186,
      "seek": 119784,
      "start": 1221.3999999999999,
      "end": 1222.3999999999999,
      "text": " applications.",
      "tokens": [
        5821,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.3801988348903426,
      "compression_ratio": 1.5634517766497462,
      "no_speech_prob": 1.0087543245163033e-7
    },
    {
      "id": 187,
      "seek": 122240,
      "start": 1222.4,
      "end": 1230.0800000000002,
      "text": " Noi lavoriamo in un mondo in cui imbarba, ricito una cosa che abbiamo discusso di recente",
      "tokens": [
        883,
        72,
        29241,
        7415,
        294,
        517,
        40499,
        294,
        22929,
        566,
        5356,
        4231,
        11,
        21040,
        3528,
        2002,
        10163,
        947,
        22815,
        717,
        1149,
        539,
        1026,
        850,
        1576
      ],
      "temperature": 0,
      "avg_logprob": -0.3236217952909924,
      "compression_ratio": 1.5783132530120483,
      "no_speech_prob": 5.203548312238127e-7
    },
    {
      "id": 188,
      "seek": 122240,
      "start": 1230.0800000000002,
      "end": 1238.88,
      "text": " in un altro accolo in inglese che citavamo prima, imbarba a tutti i progetti, questa",
      "tokens": [
        294,
        517,
        40924,
        1317,
        401,
        78,
        294,
        3957,
        904,
        68,
        947,
        4814,
        706,
        10502,
        19507,
        11,
        566,
        5356,
        4231,
        257,
        19822,
        741,
        447,
        847,
        7317,
        11,
        16540
      ],
      "temperature": 0,
      "avg_logprob": -0.3236217952909924,
      "compression_ratio": 1.5783132530120483,
      "no_speech_prob": 5.203548312238127e-7
    },
    {
      "id": 189,
      "seek": 122240,
      "start": 1238.88,
      "end": 1246.64,
      "text": " cosa la si fa così, tempo perso a fare dei grafici e a decidere come fare il database,",
      "tokens": [
        10163,
        635,
        1511,
        2050,
        23278,
        11,
        8972,
        868,
        78,
        257,
        11994,
        13874,
        1295,
        1786,
        72,
        308,
        257,
        21937,
        323,
        808,
        11994,
        1930,
        8149,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.3236217952909924,
      "compression_ratio": 1.5783132530120483,
      "no_speech_prob": 5.203548312238127e-7
    },
    {
      "id": 190,
      "seek": 124664,
      "start": 1246.64,
      "end": 1253.3200000000002,
      "text": " il software web moderno lo decide il designer che sia un produttore.",
      "tokens": [
        1930,
        4722,
        3670,
        4363,
        78,
        450,
        4536,
        1930,
        11795,
        947,
        25176,
        517,
        15792,
        13478,
        418,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.2893833198932686,
      "compression_ratio": 1.6473429951690821,
      "no_speech_prob": 1.7704233812310122e-7
    },
    {
      "id": 191,
      "seek": 124664,
      "start": 1253.3200000000002,
      "end": 1260.16,
      "text": " Punto, non giriamoci attorno, c'è un produttore, parlo ovviamente di una situazione semplice,",
      "tokens": [
        430,
        24052,
        11,
        2107,
        14703,
        7415,
        537,
        951,
        21998,
        11,
        269,
        6,
        1462,
        517,
        15792,
        13478,
        418,
        11,
        971,
        752,
        14187,
        23347,
        1026,
        2002,
        2054,
        12928,
        4361,
        564,
        573,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.2893833198932686,
      "compression_ratio": 1.6473429951690821,
      "no_speech_prob": 1.7704233812310122e-7
    },
    {
      "id": 192,
      "seek": 124664,
      "start": 1260.16,
      "end": 1265,
      "text": " poi quando si parla di grandissime applicazioni ci sarà un più produttore, un più designer",
      "tokens": [
        19260,
        7770,
        1511,
        971,
        875,
        1026,
        2697,
        891,
        1312,
        2580,
        27569,
        6983,
        41338,
        517,
        10589,
        15792,
        13478,
        418,
        11,
        517,
        10589,
        11795
      ],
      "temperature": 0,
      "avg_logprob": -0.2893833198932686,
      "compression_ratio": 1.6473429951690821,
      "no_speech_prob": 1.7704233812310122e-7
    },
    {
      "id": 193,
      "seek": 124664,
      "start": 1265,
      "end": 1272.24,
      "text": " e cose diverse, però un'applicazione che uno commissiona e che uno decide di fare,",
      "tokens": [
        308,
        30261,
        9521,
        11,
        12673,
        517,
        6,
        1746,
        1050,
        12928,
        947,
        8526,
        9221,
        64,
        308,
        947,
        8526,
        4536,
        1026,
        11994,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.2893833198932686,
      "compression_ratio": 1.6473429951690821,
      "no_speech_prob": 1.7704233812310122e-7
    },
    {
      "id": 194,
      "seek": 127224,
      "start": 1272.24,
      "end": 1277.96,
      "text": " che fa una certa cosa, ha un designer e una serie di designer che fanno le cose e un produttore",
      "tokens": [
        947,
        2050,
        2002,
        44438,
        10163,
        11,
        324,
        517,
        11795,
        308,
        2002,
        23030,
        1026,
        11795,
        947,
        283,
        13484,
        476,
        30261,
        308,
        517,
        15792,
        13478,
        418
      ],
      "temperature": 0,
      "avg_logprob": -0.41141162109375,
      "compression_ratio": 1.7,
      "no_speech_prob": 9.276348578168836e-7
    },
    {
      "id": 195,
      "seek": 127224,
      "start": 1277.96,
      "end": 1282.92,
      "text": " che li guida, che ha in mente una feature e semplicemente loro mettono su una feature.",
      "tokens": [
        947,
        375,
        695,
        2887,
        11,
        947,
        324,
        294,
        26577,
        2002,
        4111,
        308,
        4361,
        4770,
        16288,
        28810,
        1131,
        1756,
        78,
        459,
        2002,
        4111,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.41141162109375,
      "compression_ratio": 1.7,
      "no_speech_prob": 9.276348578168836e-7
    },
    {
      "id": 196,
      "seek": 127224,
      "start": 1282.92,
      "end": 1288.8,
      "text": " Partendo dai design si fa il resto, perché così alla fine si fanno, almeno nelle mie",
      "tokens": [
        4100,
        3999,
        38586,
        1715,
        1511,
        2050,
        1930,
        28247,
        11,
        14303,
        23278,
        11591,
        2489,
        1511,
        283,
        13484,
        11,
        419,
        43232,
        46350,
        12597
      ],
      "temperature": 0,
      "avg_logprob": -0.41141162109375,
      "compression_ratio": 1.7,
      "no_speech_prob": 9.276348578168836e-7
    },
    {
      "id": 197,
      "seek": 127224,
      "start": 1288.8,
      "end": 1289.8,
      "text": " esperienze penso che…",
      "tokens": [
        10045,
        1053,
        1381,
        48005,
        947,
        1260
      ],
      "temperature": 0,
      "avg_logprob": -0.41141162109375,
      "compression_ratio": 1.7,
      "no_speech_prob": 9.276348578168836e-7
    },
    {
      "id": 198,
      "seek": 127224,
      "start": 1289.8,
      "end": 1290.8,
      "text": " No, si fanno le cose.",
      "tokens": [
        883,
        11,
        1511,
        283,
        13484,
        476,
        30261,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.41141162109375,
      "compression_ratio": 1.7,
      "no_speech_prob": 9.276348578168836e-7
    },
    {
      "id": 199,
      "seek": 127224,
      "start": 1290.8,
      "end": 1297.16,
      "text": " Sì, cioè io penso di aver avuto la fortuna, lavorando in Myrford, di aver avuto la fortuna",
      "tokens": [
        318,
        4749,
        11,
        41827,
        19785,
        48005,
        1026,
        18247,
        1305,
        8262,
        635,
        5009,
        5051,
        11,
        29241,
        1806,
        294,
        1222,
        81,
        2994,
        67,
        11,
        1026,
        18247,
        1305,
        8262,
        635,
        5009,
        5051
      ],
      "temperature": 0,
      "avg_logprob": -0.41141162109375,
      "compression_ratio": 1.7,
      "no_speech_prob": 9.276348578168836e-7
    },
    {
      "id": 200,
      "seek": 129716,
      "start": 1297.16,
      "end": 1305.52,
      "text": " di lavorare in tanti progetti molto interessanti, clienti, clienti e clienti interessanti e",
      "tokens": [
        1026,
        29241,
        543,
        294,
        256,
        11520,
        447,
        847,
        7317,
        16394,
        12478,
        11520,
        11,
        6423,
        72,
        11,
        6423,
        72,
        308,
        6423,
        72,
        12478,
        11520,
        308
      ],
      "temperature": 0,
      "avg_logprob": -0.2677930849734868,
      "compression_ratio": 1.6589861751152073,
      "no_speech_prob": 1.6893504550807847e-7
    },
    {
      "id": 201,
      "seek": 129716,
      "start": 1305.52,
      "end": 1311.4,
      "text": " questo è stato il sistema sempre, ovvio che devi avere un'idea di quello che c'è sotto,",
      "tokens": [
        10263,
        4873,
        29657,
        1930,
        13245,
        9553,
        11,
        14187,
        28226,
        947,
        31219,
        37914,
        517,
        6,
        482,
        64,
        1026,
        22813,
        947,
        269,
        6,
        1462,
        43754,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.2677930849734868,
      "compression_ratio": 1.6589861751152073,
      "no_speech_prob": 1.6893504550807847e-7
    },
    {
      "id": 202,
      "seek": 129716,
      "start": 1311.4,
      "end": 1316.44,
      "text": " devi sapere più o meno dove si andrà, di come gestire i dati, certo, ma alla fine il",
      "tokens": [
        31219,
        18985,
        323,
        10589,
        277,
        40236,
        23287,
        1511,
        293,
        39212,
        11,
        1026,
        808,
        7219,
        621,
        741,
        1137,
        72,
        11,
        22261,
        11,
        463,
        11591,
        2489,
        1930
      ],
      "temperature": 0,
      "avg_logprob": -0.2677930849734868,
      "compression_ratio": 1.6589861751152073,
      "no_speech_prob": 1.6893504550807847e-7
    },
    {
      "id": 203,
      "seek": 129716,
      "start": 1316.44,
      "end": 1322.52,
      "text": " fatto che a un certo punto appare una label con un valore, dal giorno alla notte ti arriva.",
      "tokens": [
        23228,
        947,
        257,
        517,
        22261,
        14326,
        724,
        543,
        2002,
        7645,
        416,
        517,
        1323,
        418,
        11,
        11702,
        42202,
        11591,
        406,
        975,
        8757,
        3399,
        2757,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.2677930849734868,
      "compression_ratio": 1.6589861751152073,
      "no_speech_prob": 1.6893504550807847e-7
    },
    {
      "id": 204,
      "seek": 132252,
      "start": 1322.52,
      "end": 1327.44,
      "text": " Quindi GraphQL da questo punto di vista è fenomenale, perché tu soprattutto in fase",
      "tokens": [
        32534,
        21884,
        13695,
        1120,
        10263,
        14326,
        1026,
        22553,
        4873,
        26830,
        4726,
        1220,
        11,
        14303,
        2604,
        50002,
        294,
        33931
      ],
      "temperature": 0,
      "avg_logprob": -0.36443990342160487,
      "compression_ratio": 1.681159420289855,
      "no_speech_prob": 5.804976126455585e-7
    },
    {
      "id": 205,
      "seek": 132252,
      "start": 1327.44,
      "end": 1334.2,
      "text": " di prima creazione, mantenimento, sul mantenimento potrei parlare male di GraphQL però…",
      "tokens": [
        1026,
        19507,
        1197,
        12928,
        11,
        38417,
        10030,
        11,
        17603,
        38417,
        10030,
        1847,
        10271,
        13734,
        543,
        7133,
        1026,
        21884,
        13695,
        12673,
        1260
      ],
      "temperature": 0,
      "avg_logprob": -0.36443990342160487,
      "compression_ratio": 1.681159420289855,
      "no_speech_prob": 5.804976126455585e-7
    },
    {
      "id": 206,
      "seek": 132252,
      "start": 1334.2,
      "end": 1339,
      "text": " No, ma poi ci arriviamo perché mi sono appuntato un punto proprio su quello.",
      "tokens": [
        883,
        11,
        463,
        19260,
        6983,
        30697,
        7415,
        14303,
        2752,
        9259,
        724,
        2760,
        2513,
        517,
        14326,
        28203,
        459,
        22813,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.36443990342160487,
      "compression_ratio": 1.681159420289855,
      "no_speech_prob": 5.804976126455585e-7
    },
    {
      "id": 207,
      "seek": 132252,
      "start": 1339,
      "end": 1345.92,
      "text": " Bravo, però sulla fase di progettazione, sulla fase di progettazione, GraphQL è fenomenale,",
      "tokens": [
        28861,
        11,
        12673,
        33625,
        33931,
        1026,
        447,
        847,
        83,
        12928,
        11,
        33625,
        33931,
        1026,
        447,
        847,
        83,
        12928,
        11,
        21884,
        13695,
        4873,
        26830,
        4726,
        1220,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.36443990342160487,
      "compression_ratio": 1.681159420289855,
      "no_speech_prob": 5.804976126455585e-7
    },
    {
      "id": 208,
      "seek": 134592,
      "start": 1345.92,
      "end": 1354.76,
      "text": " ogni tool che ci sono in giro, che velocemente da un database o da una struttura dati ti",
      "tokens": [
        33189,
        2290,
        947,
        6983,
        9259,
        294,
        1735,
        340,
        11,
        947,
        1241,
        752,
        384,
        4082,
        1120,
        517,
        8149,
        277,
        1120,
        2002,
        1056,
        13478,
        2991,
        1137,
        72,
        8757
      ],
      "temperature": 0,
      "avg_logprob": -0.36266653878348215,
      "compression_ratio": 1.535,
      "no_speech_prob": 2.785264143767563e-7
    },
    {
      "id": 209,
      "seek": 134592,
      "start": 1354.76,
      "end": 1363.76,
      "text": " creano, parto dal platformatic di Matteo Collina, cito anche Asura, quindi sono due tipologie",
      "tokens": [
        1197,
        3730,
        11,
        644,
        78,
        11702,
        3663,
        2399,
        1026,
        47544,
        78,
        4586,
        1426,
        11,
        269,
        3528,
        11585,
        1018,
        2991,
        11,
        15727,
        9259,
        3462,
        4125,
        20121
      ],
      "temperature": 0,
      "avg_logprob": -0.36266653878348215,
      "compression_ratio": 1.535,
      "no_speech_prob": 2.785264143767563e-7
    },
    {
      "id": 210,
      "seek": 134592,
      "start": 1363.76,
      "end": 1366.64,
      "text": " di tool che più o meno…",
      "tokens": [
        1026,
        2290,
        947,
        10589,
        277,
        40236,
        1260
      ],
      "temperature": 0,
      "avg_logprob": -0.36266653878348215,
      "compression_ratio": 1.535,
      "no_speech_prob": 2.785264143767563e-7
    },
    {
      "id": 211,
      "seek": 134592,
      "start": 1366.64,
      "end": 1367.64,
      "text": " Strapi…",
      "tokens": [
        745,
        4007,
        72,
        1260
      ],
      "temperature": 0,
      "avg_logprob": -0.36266653878348215,
      "compression_ratio": 1.535,
      "no_speech_prob": 2.785264143767563e-7
    },
    {
      "id": 212,
      "seek": 134592,
      "start": 1367.64,
      "end": 1371.24,
      "text": " Strapi per esempio non lo conosco, che però più o meno, però sì, immagino che anche",
      "tokens": [
        745,
        4007,
        72,
        680,
        33627,
        2107,
        450,
        49892,
        1291,
        11,
        947,
        12673,
        10589,
        277,
        40236,
        11,
        12673,
        49267,
        11,
        3397,
        559,
        2982,
        947,
        11585
      ],
      "temperature": 0,
      "avg_logprob": -0.36266653878348215,
      "compression_ratio": 1.535,
      "no_speech_prob": 2.785264143767563e-7
    },
    {
      "id": 213,
      "seek": 137124,
      "start": 1371.24,
      "end": 1376,
      "text": " Strapi faccia la stessa cosa, perché lui funziona comunque forniscendo l'API, tu hai",
      "tokens": [
        745,
        4007,
        72,
        1915,
        2755,
        635,
        342,
        8391,
        10163,
        11,
        14303,
        8783,
        49345,
        21758,
        45736,
        337,
        77,
        5606,
        3999,
        287,
        6,
        4715,
        40,
        11,
        2604,
        21822
      ],
      "temperature": 0,
      "avg_logprob": -0.3202549586786288,
      "compression_ratio": 1.5411255411255411,
      "no_speech_prob": 1.6893493182124075e-7
    },
    {
      "id": 214,
      "seek": 137124,
      "start": 1376,
      "end": 1383.52,
      "text": " la possibilità veramente in maniera veloce di tirar su un servizio che va e con le varie",
      "tokens": [
        635,
        24145,
        12445,
        50079,
        294,
        587,
        10609,
        1241,
        752,
        384,
        1026,
        29239,
        459,
        517,
        1658,
        590,
        1004,
        947,
        2773,
        308,
        416,
        476,
        1374,
        414
      ],
      "temperature": 0,
      "avg_logprob": -0.3202549586786288,
      "compression_ratio": 1.5411255411255411,
      "no_speech_prob": 1.6893493182124075e-7
    },
    {
      "id": 215,
      "seek": 137124,
      "start": 1383.52,
      "end": 1389.8,
      "text": " librerie che ti si attaccano a React rendono veramente velocissima la prototipazione, è",
      "tokens": [
        4939,
        17487,
        947,
        8757,
        1511,
        951,
        8476,
        3730,
        257,
        30644,
        6125,
        8957,
        50079,
        7806,
        891,
        4775,
        635,
        1742,
        310,
        647,
        12928,
        11,
        4873
      ],
      "temperature": 0,
      "avg_logprob": -0.3202549586786288,
      "compression_ratio": 1.5411255411255411,
      "no_speech_prob": 1.6893493182124075e-7
    },
    {
      "id": 216,
      "seek": 137124,
      "start": 1389.8,
      "end": 1397.56,
      "text": " una roba, è impressionante fare il prototyping in GraphQL, è molto veloce, perché tu non",
      "tokens": [
        2002,
        3870,
        64,
        11,
        4873,
        9995,
        2879,
        11994,
        1930,
        46219,
        3381,
        294,
        21884,
        13695,
        11,
        4873,
        16394,
        1241,
        752,
        384,
        11,
        14303,
        2604,
        2107
      ],
      "temperature": 0,
      "avg_logprob": -0.3202549586786288,
      "compression_ratio": 1.5411255411255411,
      "no_speech_prob": 1.6893493182124075e-7
    },
    {
      "id": 217,
      "seek": 139756,
      "start": 1397.56,
      "end": 1403.04,
      "text": " ti devi curare di niente, tu devi semplicemente buttare dentro dei dati, organizzarli, strutturarli,",
      "tokens": [
        8757,
        31219,
        1262,
        543,
        1026,
        297,
        8413,
        11,
        2604,
        31219,
        4361,
        4770,
        16288,
        6660,
        543,
        10856,
        13874,
        1137,
        72,
        11,
        4645,
        26236,
        2081,
        11,
        1056,
        13478,
        28586,
        2081,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.28567465816635684,
      "compression_ratio": 1.8169934640522876,
      "no_speech_prob": 1.25541959050679e-7
    },
    {
      "id": 218,
      "seek": 139756,
      "start": 1403.04,
      "end": 1406.8,
      "text": " perché bisogna fare le cose per bene, non bisogna prendere una spadellata di attributi",
      "tokens": [
        14303,
        40505,
        629,
        11994,
        476,
        30261,
        680,
        2537,
        11,
        2107,
        40505,
        629,
        9866,
        323,
        2002,
        637,
        762,
        285,
        3274,
        1026,
        9080,
        29161
      ],
      "temperature": 0,
      "avg_logprob": -0.28567465816635684,
      "compression_ratio": 1.8169934640522876,
      "no_speech_prob": 1.25541959050679e-7
    },
    {
      "id": 219,
      "seek": 139756,
      "start": 1406.8,
      "end": 1411.84,
      "text": " e buttarla dentro, cioè, c'hanno le cose per bene, però ti permette intanto di definire",
      "tokens": [
        308,
        6660,
        34148,
        10856,
        11,
        41827,
        11,
        269,
        6,
        71,
        13484,
        476,
        30261,
        680,
        2537,
        11,
        12673,
        8757,
        4784,
        3007,
        560,
        5857,
        1026,
        1561,
        621
      ],
      "temperature": 0,
      "avg_logprob": -0.28567465816635684,
      "compression_ratio": 1.8169934640522876,
      "no_speech_prob": 1.25541959050679e-7
    },
    {
      "id": 220,
      "seek": 139756,
      "start": 1411.84,
      "end": 1415.96,
      "text": " già, cioè il bello di GraphQL è che ti permette già di definire la struttura dati,",
      "tokens": [
        30469,
        11,
        41827,
        1930,
        312,
        1913,
        1026,
        21884,
        13695,
        4873,
        947,
        8757,
        4784,
        3007,
        30469,
        1026,
        1561,
        621,
        635,
        1056,
        13478,
        2991,
        1137,
        72,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.28567465816635684,
      "compression_ratio": 1.8169934640522876,
      "no_speech_prob": 1.25541959050679e-7
    },
    {
      "id": 221,
      "seek": 139756,
      "start": 1415.96,
      "end": 1421.8799999999999,
      "text": " e quando l'hai definita sei a metà dell'opera, perché col sistema di resolver e tutte le",
      "tokens": [
        308,
        7770,
        287,
        6,
        18230,
        1561,
        2786,
        10842,
        257,
        1131,
        1467,
        19781,
        6,
        404,
        1663,
        11,
        14303,
        1173,
        13245,
        1026,
        34480,
        308,
        38632,
        476
      ],
      "temperature": 0,
      "avg_logprob": -0.28567465816635684,
      "compression_ratio": 1.8169934640522876,
      "no_speech_prob": 1.25541959050679e-7
    },
    {
      "id": 222,
      "seek": 139756,
      "start": 1421.8799999999999,
      "end": 1427.1599999999999,
      "text": " cose che hanno, tutto funziona meravigliosamente, poi problematiche di performance, avremo l'intera",
      "tokens": [
        30261,
        947,
        26595,
        11,
        23048,
        49345,
        21758,
        3551,
        706,
        328,
        2081,
        329,
        3439,
        11,
        19260,
        1154,
        267,
        9304,
        1026,
        3389,
        11,
        1305,
        44172,
        287,
        6,
        5106,
        64
      ],
      "temperature": 0,
      "avg_logprob": -0.28567465816635684,
      "compression_ratio": 1.8169934640522876,
      "no_speech_prob": 1.25541959050679e-7
    },
    {
      "id": 223,
      "seek": 142716,
      "start": 1427.16,
      "end": 1433.68,
      "text": " puntata per descrivere queste cose, ma per partire da zero, sicuramente, che sia una",
      "tokens": [
        18212,
        3274,
        680,
        2189,
        5887,
        35455,
        30261,
        11,
        463,
        680,
        644,
        621,
        1120,
        4018,
        11,
        33579,
        374,
        3439,
        11,
        947,
        25176,
        2002
      ],
      "temperature": 0,
      "avg_logprob": -0.3694788176437904,
      "compression_ratio": 1.7017543859649122,
      "no_speech_prob": 5.368722781895485e-7
    },
    {
      "id": 224,
      "seek": 142716,
      "start": 1433.68,
      "end": 1438.52,
      "text": " piccola applicazione, che sia una grande applicazione, poi adesso con la federazione, con quello",
      "tokens": [
        13363,
        66,
        4711,
        2580,
        12928,
        11,
        947,
        25176,
        2002,
        8883,
        2580,
        12928,
        11,
        19260,
        39552,
        416,
        635,
        38024,
        12928,
        11,
        416,
        22813
      ],
      "temperature": 0,
      "avg_logprob": -0.3694788176437904,
      "compression_ratio": 1.7017543859649122,
      "no_speech_prob": 5.368722781895485e-7
    },
    {
      "id": 225,
      "seek": 142716,
      "start": 1438.52,
      "end": 1445.3600000000001,
      "text": " che ti permette di fare la federazione, è tanta roba, cioè si fanno grandi cose.",
      "tokens": [
        947,
        8757,
        4784,
        3007,
        1026,
        11994,
        635,
        38024,
        12928,
        11,
        4873,
        40864,
        3870,
        64,
        11,
        41827,
        1511,
        283,
        13484,
        45155,
        30261,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.3694788176437904,
      "compression_ratio": 1.7017543859649122,
      "no_speech_prob": 5.368722781895485e-7
    },
    {
      "id": 226,
      "seek": 142716,
      "start": 1445.3600000000001,
      "end": 1449.72,
      "text": " Si, hai detto, ah scusa, vai, vai.",
      "tokens": [
        4909,
        11,
        21822,
        41031,
        11,
        3716,
        795,
        20318,
        11,
        4405,
        11,
        4405,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.3694788176437904,
      "compression_ratio": 1.7017543859649122,
      "no_speech_prob": 5.368722781895485e-7
    },
    {
      "id": 227,
      "seek": 142716,
      "start": 1449.72,
      "end": 1455.96,
      "text": " No, volevo dire, semmai la riprendiamo dopo, perché questo apre una voragine. Quando io",
      "tokens": [
        883,
        11,
        49877,
        3080,
        1264,
        11,
        369,
        2174,
        1301,
        635,
        12782,
        4542,
        7415,
        35196,
        11,
        14303,
        10263,
        1882,
        265,
        2002,
        4245,
        10260,
        13,
        18725,
        19785
      ],
      "temperature": 0,
      "avg_logprob": -0.3694788176437904,
      "compression_ratio": 1.7017543859649122,
      "no_speech_prob": 5.368722781895485e-7
    },
    {
      "id": 228,
      "seek": 145596,
      "start": 1455.96,
      "end": 1461.96,
      "text": " ho parlato con persone e mi dicono, ah ma GraphQL, così, poi quando ci parli per bene,",
      "tokens": [
        1106,
        13734,
        2513,
        416,
        29944,
        308,
        2752,
        14285,
        8957,
        11,
        3716,
        463,
        21884,
        13695,
        11,
        23278,
        11,
        19260,
        7770,
        6983,
        971,
        2081,
        680,
        2537,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.2587216581617083,
      "compression_ratio": 1.5236051502145922,
      "no_speech_prob": 7.002138886491593e-7
    },
    {
      "id": 229,
      "seek": 145596,
      "start": 1461.96,
      "end": 1465.24,
      "text": " quando ti siedi con loro, quando gli fai vedere la semplicità con cui si fanno certe",
      "tokens": [
        7770,
        8757,
        262,
        1091,
        72,
        416,
        28810,
        11,
        7770,
        17161,
        283,
        1301,
        35373,
        635,
        4361,
        4770,
        12445,
        416,
        22929,
        1511,
        283,
        13484,
        5351,
        68
      ],
      "temperature": 0,
      "avg_logprob": -0.2587216581617083,
      "compression_ratio": 1.5236051502145922,
      "no_speech_prob": 7.002138886491593e-7
    },
    {
      "id": 230,
      "seek": 145596,
      "start": 1465.24,
      "end": 1473.56,
      "text": " cose, l'affermazione da parte loro spesso è stata, ah, non avevo capito. Poi non sto",
      "tokens": [
        30261,
        11,
        287,
        6,
        2518,
        966,
        12928,
        1120,
        6975,
        28810,
        637,
        5557,
        4873,
        49554,
        11,
        3716,
        11,
        2107,
        3472,
        3080,
        1410,
        3528,
        13,
        430,
        4869,
        2107,
        22784
      ],
      "temperature": 0,
      "avg_logprob": -0.2587216581617083,
      "compression_ratio": 1.5236051502145922,
      "no_speech_prob": 7.002138886491593e-7
    },
    {
      "id": 231,
      "seek": 145596,
      "start": 1473.56,
      "end": 1480.72,
      "text": " dicendo è la migliore tecnologia del mondo, rest funziona da Dio e io le uso indipendentemente",
      "tokens": [
        14285,
        3999,
        4873,
        635,
        6186,
        2081,
        418,
        44905,
        1103,
        40499,
        11,
        1472,
        49345,
        21758,
        1120,
        413,
        1004,
        308,
        19785,
        476,
        22728,
        1016,
        647,
        521,
        317,
        16288
      ],
      "temperature": 0,
      "avg_logprob": -0.2587216581617083,
      "compression_ratio": 1.5236051502145922,
      "no_speech_prob": 7.002138886491593e-7
    },
    {
      "id": 232,
      "seek": 148072,
      "start": 1480.72,
      "end": 1486.2,
      "text": " tutte e due, perché poi non sono sempre io a scegliere, a volte c'è chi dice no, preferisco",
      "tokens": [
        38632,
        308,
        3462,
        11,
        14303,
        19260,
        2107,
        9259,
        9553,
        19785,
        257,
        262,
        384,
        41443,
        323,
        11,
        257,
        37801,
        269,
        6,
        1462,
        13228,
        10313,
        572,
        11,
        4382,
        8610
      ],
      "temperature": 0,
      "avg_logprob": -0.3566608240108679,
      "compression_ratio": 1.560747663551402,
      "no_speech_prob": 7.307423111058142e-9
    },
    {
      "id": 233,
      "seek": 148072,
      "start": 1486.2,
      "end": 1490.52,
      "text": " stare su quella, beh, allora restiamo su quelle tecnologie.",
      "tokens": [
        22432,
        459,
        32234,
        11,
        1540,
        11,
        44141,
        1472,
        7415,
        459,
        29237,
        20105,
        20121,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.3566608240108679,
      "compression_ratio": 1.560747663551402,
      "no_speech_prob": 7.307423111058142e-9
    },
    {
      "id": 234,
      "seek": 148072,
      "start": 1490.52,
      "end": 1496,
      "text": " Vero, prima hai detto una cosa interessante sulla quale voglio costruire una piccola analogia.",
      "tokens": [
        691,
        2032,
        11,
        19507,
        21822,
        41031,
        2002,
        10163,
        24372,
        33625,
        421,
        1220,
        31273,
        19987,
        2063,
        894,
        621,
        2002,
        13363,
        66,
        4711,
        16660,
        654,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.3566608240108679,
      "compression_ratio": 1.560747663551402,
      "no_speech_prob": 7.307423111058142e-9
    },
    {
      "id": 235,
      "seek": 148072,
      "start": 1496,
      "end": 1501.96,
      "text": " Tu hai detto, le UI, comunque i prodotti, anzi mi correggo, i prodotti sono fatti dal",
      "tokens": [
        7836,
        21822,
        41031,
        11,
        476,
        15682,
        11,
        45736,
        741,
        15792,
        37514,
        11,
        364,
        3992,
        2752,
        29731,
        1615,
        78,
        11,
        741,
        15792,
        37514,
        9259,
        283,
        21515,
        11702
      ],
      "temperature": 0,
      "avg_logprob": -0.3566608240108679,
      "compression_ratio": 1.560747663551402,
      "no_speech_prob": 7.307423111058142e-9
    },
    {
      "id": 236,
      "seek": 150196,
      "start": 1501.96,
      "end": 1514.3600000000001,
      "text": " produttore, ok, e dal UX, il designer. Io ho detto designer, ma con designer intendevo",
      "tokens": [
        15792,
        13478,
        418,
        11,
        3133,
        11,
        308,
        11702,
        40176,
        11,
        1930,
        11795,
        13,
        19239,
        1106,
        41031,
        11795,
        11,
        463,
        416,
        11795,
        560,
        5445,
        3080
      ],
      "temperature": 0,
      "avg_logprob": -0.2927978892385224,
      "compression_ratio": 1.4891304347826086,
      "no_speech_prob": 3.432126405300551e-8
    },
    {
      "id": 237,
      "seek": 150196,
      "start": 1514.3600000000001,
      "end": 1521.32,
      "text": " tutto quel messione via ruoli che fanno questo. Esatto, e spesso questo non succede nei prodotti",
      "tokens": [
        23048,
        7178,
        2082,
        72,
        546,
        5766,
        5420,
        9384,
        947,
        283,
        13484,
        10263,
        13,
        2313,
        37491,
        11,
        308,
        637,
        5557,
        10263,
        2107,
        1965,
        29815,
        34517,
        15792,
        37514
      ],
      "temperature": 0,
      "avg_logprob": -0.2927978892385224,
      "compression_ratio": 1.4891304347826086,
      "no_speech_prob": 3.432126405300551e-8
    },
    {
      "id": 238,
      "seek": 150196,
      "start": 1521.32,
      "end": 1526.44,
      "text": " digitali, forse perché l'industria non è ancora abbastanza matura, forse perché abbiamo",
      "tokens": [
        4562,
        72,
        11,
        337,
        405,
        14303,
        287,
        6,
        29850,
        4668,
        2107,
        4873,
        30656,
        16903,
        525,
        20030,
        3803,
        2991,
        11,
        337,
        405,
        14303,
        22815
      ],
      "temperature": 0,
      "avg_logprob": -0.2927978892385224,
      "compression_ratio": 1.4891304347826086,
      "no_speech_prob": 3.432126405300551e-8
    },
    {
      "id": 239,
      "seek": 152644,
      "start": 1526.44,
      "end": 1532.44,
      "text": " un po' un approccio cinofallico, come diceva qualcuno, spesso perché i tempi sono quelli",
      "tokens": [
        517,
        714,
        6,
        517,
        2075,
        66,
        8529,
        269,
        2982,
        6691,
        2789,
        11,
        808,
        10313,
        2757,
        32101,
        12638,
        11,
        637,
        5557,
        14303,
        741,
        18274,
        72,
        9259,
        631,
        16320
      ],
      "temperature": 0,
      "avg_logprob": -0.26929541878078295,
      "compression_ratio": 1.6682242990654206,
      "no_speech_prob": 6.118426654211362e-8
    },
    {
      "id": 240,
      "seek": 152644,
      "start": 1532.44,
      "end": 1537.68,
      "text": " che, e spesso perché da sviluppatore abbiamo la presunzione di dover dettare una riga,",
      "tokens": [
        947,
        11,
        308,
        637,
        5557,
        14303,
        1120,
        17342,
        388,
        10504,
        43148,
        22815,
        635,
        1183,
        409,
        19706,
        1026,
        360,
        331,
        1141,
        83,
        543,
        2002,
        8329,
        64,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.26929541878078295,
      "compression_ratio": 1.6682242990654206,
      "no_speech_prob": 6.118426654211362e-8
    },
    {
      "id": 241,
      "seek": 152644,
      "start": 1537.68,
      "end": 1544.3200000000002,
      "text": " no, di dire no ma questo non si può fare, posto che quello che mi disse il mio produttore",
      "tokens": [
        572,
        11,
        1026,
        1264,
        572,
        463,
        10263,
        2107,
        1511,
        26526,
        11994,
        11,
        2183,
        78,
        947,
        22813,
        947,
        2752,
        17581,
        1930,
        29908,
        15792,
        13478,
        284,
        68
      ],
      "temperature": 0,
      "avg_logprob": -0.26929541878078295,
      "compression_ratio": 1.6682242990654206,
      "no_speech_prob": 6.118426654211362e-8
    },
    {
      "id": 242,
      "seek": 152644,
      "start": 1544.3200000000002,
      "end": 1548.44,
      "text": " è la prossima volta che mi dici non si può fare ti prendo a calcino il culo, nel senso",
      "tokens": [
        4873,
        635,
        48794,
        4775,
        18765,
        947,
        2752,
        274,
        8787,
        2107,
        1511,
        26526,
        11994,
        8757,
        9866,
        78,
        257,
        2104,
        66,
        2982,
        1930,
        11021,
        78,
        11,
        15373,
        3151,
        539
      ],
      "temperature": 0,
      "avg_logprob": -0.26929541878078295,
      "compression_ratio": 1.6682242990654206,
      "no_speech_prob": 6.118426654211362e-8
    },
    {
      "id": 243,
      "seek": 154844,
      "start": 1548.44,
      "end": 1559.52,
      "text": " che tutto si può fare, dipende sempre da budget, tempo e risorse, però al di là di",
      "tokens": [
        947,
        23048,
        1511,
        26526,
        11994,
        11,
        10460,
        5445,
        9553,
        1120,
        4706,
        11,
        8972,
        308,
        2253,
        18699,
        11,
        12673,
        419,
        1026,
        3684,
        1026
      ],
      "temperature": 0,
      "avg_logprob": -0.23035042285919188,
      "compression_ratio": 1.5028901734104045,
      "no_speech_prob": 1.0799669603045459e-8
    },
    {
      "id": 244,
      "seek": 154844,
      "start": 1559.52,
      "end": 1564.3600000000001,
      "text": " quello un po' è quello che succede nel mondo, cioè nel senso che il mio fratello è un",
      "tokens": [
        22813,
        517,
        714,
        6,
        4873,
        22813,
        947,
        1965,
        29815,
        15373,
        40499,
        11,
        41827,
        15373,
        3151,
        539,
        947,
        1930,
        29908,
        431,
        473,
        1913,
        4873,
        517
      ],
      "temperature": 0,
      "avg_logprob": -0.23035042285919188,
      "compression_ratio": 1.5028901734104045,
      "no_speech_prob": 1.0799669603045459e-8
    },
    {
      "id": 245,
      "seek": 154844,
      "start": 1564.3600000000001,
      "end": 1572,
      "text": " architetto, il 90% del mio fratello è da fare i progetti, avere più o meno idea dei",
      "tokens": [
        3912,
        16341,
        1353,
        11,
        1930,
        4289,
        4,
        1103,
        29908,
        431,
        473,
        1913,
        4873,
        1120,
        11994,
        741,
        447,
        847,
        7317,
        11,
        37914,
        10589,
        277,
        40236,
        1558,
        13874
      ],
      "temperature": 0,
      "avg_logprob": -0.23035042285919188,
      "compression_ratio": 1.5028901734104045,
      "no_speech_prob": 1.0799669603045459e-8
    },
    {
      "id": 246,
      "seek": 157200,
      "start": 1572,
      "end": 1578.84,
      "text": " calcoli e della parte strutturale, poi lo dà in mano a un ingegnere e gli dice, come,",
      "tokens": [
        2104,
        8768,
        72,
        308,
        11618,
        6975,
        1056,
        13478,
        374,
        1220,
        11,
        19260,
        450,
        274,
        1467,
        294,
        18384,
        257,
        517,
        3957,
        1146,
        77,
        323,
        308,
        17161,
        10313,
        11,
        808,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.24093668715758892,
      "compression_ratio": 1.8553719008264462,
      "no_speech_prob": 9.833213354681902e-9
    },
    {
      "id": 247,
      "seek": 157200,
      "start": 1578.84,
      "end": 1583.28,
      "text": " vi faccio questi, prova a costruirvi queste immagini, prende un pezzo di carta, lo appallottola",
      "tokens": [
        1932,
        1915,
        8529,
        29729,
        11,
        28959,
        257,
        2063,
        894,
        347,
        4917,
        35455,
        3397,
        559,
        3812,
        11,
        9866,
        68,
        517,
        520,
        35130,
        1026,
        41815,
        11,
        450,
        724,
        336,
        1521,
        4711
      ],
      "temperature": 0,
      "avg_logprob": -0.24093668715758892,
      "compression_ratio": 1.8553719008264462,
      "no_speech_prob": 9.833213354681902e-9
    },
    {
      "id": 248,
      "seek": 157200,
      "start": 1583.28,
      "end": 1589.84,
      "text": " e dice voglio questa struttura del cazzo tutta strana, un po' esceriana, ok, lo dà a un",
      "tokens": [
        308,
        10313,
        31273,
        19987,
        16540,
        1056,
        13478,
        2991,
        1103,
        269,
        921,
        4765,
        3672,
        1328,
        1056,
        2095,
        11,
        517,
        714,
        6,
        785,
        1776,
        8497,
        11,
        3133,
        11,
        450,
        274,
        1467,
        257,
        517
      ],
      "temperature": 0,
      "avg_logprob": -0.24093668715758892,
      "compression_ratio": 1.8553719008264462,
      "no_speech_prob": 9.833213354681902e-9
    },
    {
      "id": 249,
      "seek": 157200,
      "start": 1589.84,
      "end": 1594.88,
      "text": " ingegnere e gli dice bene fammela stare in piedi, ecco, dobbiamo un po' ricalibrare il",
      "tokens": [
        3957,
        1146,
        77,
        323,
        308,
        17161,
        10313,
        2537,
        1087,
        76,
        4053,
        22432,
        294,
        24186,
        72,
        11,
        11437,
        1291,
        11,
        360,
        6692,
        7415,
        517,
        714,
        6,
        367,
        804,
        897,
        35559,
        1930
      ],
      "temperature": 0,
      "avg_logprob": -0.24093668715758892,
      "compression_ratio": 1.8553719008264462,
      "no_speech_prob": 9.833213354681902e-9
    },
    {
      "id": 250,
      "seek": 157200,
      "start": 1594.88,
      "end": 1601.68,
      "text": " nostro modo, noi dovremmo essere quelli che al ecco fammela stare in piedi, dobbiamo saper",
      "tokens": [
        35779,
        16664,
        11,
        22447,
        30870,
        265,
        2174,
        78,
        19799,
        631,
        16320,
        947,
        419,
        11437,
        1291,
        1087,
        76,
        4053,
        22432,
        294,
        24186,
        72,
        11,
        360,
        6692,
        7415,
        262,
        2332
      ],
      "temperature": 0,
      "avg_logprob": -0.24093668715758892,
      "compression_ratio": 1.8553719008264462,
      "no_speech_prob": 9.833213354681902e-9
    },
    {
      "id": 251,
      "seek": 160168,
      "start": 1601.68,
      "end": 1606.52,
      "text": " dire sì, io te la faccio stare in piedi, questo è quello di cui ho bisogno per fartela stare",
      "tokens": [
        1264,
        49267,
        11,
        19785,
        535,
        635,
        1915,
        8529,
        22432,
        294,
        24186,
        72,
        11,
        10263,
        4873,
        22813,
        1026,
        22929,
        1106,
        40505,
        1771,
        680,
        24575,
        4053,
        22432
      ],
      "temperature": 0,
      "avg_logprob": -0.24439833097368757,
      "compression_ratio": 1.5879828326180256,
      "no_speech_prob": 7.856218076085497e-8
    },
    {
      "id": 252,
      "seek": 160168,
      "start": 1606.52,
      "end": 1612.68,
      "text": " in piedi, non, non si può fare. Detto questo, aggiungo un'altra parte ricollegandomi al",
      "tokens": [
        294,
        24186,
        72,
        11,
        2107,
        11,
        2107,
        1511,
        26526,
        11994,
        13,
        4237,
        1353,
        10263,
        11,
        42254,
        1063,
        78,
        517,
        6,
        38865,
        6975,
        21040,
        1833,
        1146,
        4606,
        72,
        419
      ],
      "temperature": 0,
      "avg_logprob": -0.24439833097368757,
      "compression_ratio": 1.5879828326180256,
      "no_speech_prob": 7.856218076085497e-8
    },
    {
      "id": 253,
      "seek": 160168,
      "start": 1612.68,
      "end": 1621.4,
      "text": " fatto dello schema che hai evidenziato, sto per lanciare una bold opinion sul mondo JavaScript,",
      "tokens": [
        23228,
        368,
        1913,
        34078,
        947,
        21822,
        43699,
        3992,
        2513,
        11,
        22784,
        680,
        9326,
        537,
        543,
        2002,
        11928,
        4800,
        17603,
        40499,
        15778,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.24439833097368757,
      "compression_ratio": 1.5879828326180256,
      "no_speech_prob": 7.856218076085497e-8
    },
    {
      "id": 254,
      "seek": 160168,
      "start": 1621.4,
      "end": 1627.52,
      "text": " quindi Davide, so che tu sei un amante di JavaScript, però secondo me la vera innovazione",
      "tokens": [
        15727,
        3724,
        482,
        11,
        370,
        947,
        2604,
        10842,
        517,
        669,
        2879,
        1026,
        15778,
        11,
        12673,
        41601,
        385,
        635,
        1306,
        64,
        5083,
        12928
      ],
      "temperature": 0,
      "avg_logprob": -0.24439833097368757,
      "compression_ratio": 1.5879828326180256,
      "no_speech_prob": 7.856218076085497e-8
    },
    {
      "id": 255,
      "seek": 162752,
      "start": 1627.52,
      "end": 1636.8799999999999,
      "text": " di GraphQL è stata quella di dire, ehi cari amanti di JavaScript, guardate che i tipi",
      "tokens": [
        1026,
        21884,
        13695,
        4873,
        49554,
        32234,
        1026,
        1264,
        11,
        308,
        4954,
        1032,
        72,
        669,
        11520,
        1026,
        15778,
        11,
        6290,
        473,
        947,
        741,
        4125,
        72
      ],
      "temperature": 0,
      "avg_logprob": -0.21487020517324473,
      "compression_ratio": 1.483695652173913,
      "no_speech_prob": 2.1477708500583503e-8
    },
    {
      "id": 256,
      "seek": 162752,
      "start": 1636.8799999999999,
      "end": 1645.08,
      "text": " e la struttura dei dati che gira all'interno della vostra applicazione non è poco importante,",
      "tokens": [
        308,
        635,
        1056,
        13478,
        2991,
        13874,
        1137,
        72,
        947,
        290,
        4271,
        439,
        6,
        5106,
        1771,
        11618,
        28944,
        424,
        2580,
        12928,
        2107,
        4873,
        10639,
        9416,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.21487020517324473,
      "compression_ratio": 1.483695652173913,
      "no_speech_prob": 2.1477708500583503e-8
    },
    {
      "id": 257,
      "seek": 162752,
      "start": 1645.08,
      "end": 1652.8,
      "text": " è molto importante, probabilmente è centrale nell'applicazione, quindi secondo me la cosa",
      "tokens": [
        4873,
        16394,
        9416,
        11,
        31959,
        4082,
        4873,
        32199,
        1220,
        44666,
        6,
        1746,
        1050,
        12928,
        11,
        15727,
        41601,
        385,
        635,
        10163
      ],
      "temperature": 0,
      "avg_logprob": -0.21487020517324473,
      "compression_ratio": 1.483695652173913,
      "no_speech_prob": 2.1477708500583503e-8
    },
    {
      "id": 258,
      "seek": 165280,
      "start": 1652.8,
      "end": 1659.24,
      "text": " che ha portato GraphQL è anche una certa disciplina sulla struttura dei dati, in un",
      "tokens": [
        947,
        324,
        2436,
        2513,
        21884,
        13695,
        4873,
        11585,
        2002,
        44438,
        8644,
        1426,
        33625,
        1056,
        13478,
        2991,
        13874,
        1137,
        72,
        11,
        294,
        517
      ],
      "temperature": 0,
      "avg_logprob": -0.26751911492995273,
      "compression_ratio": 1.3958333333333333,
      "no_speech_prob": 2.8012212993644425e-8
    },
    {
      "id": 259,
      "seek": 165280,
      "start": 1659.24,
      "end": 1666.96,
      "text": " mondo e in un'industria dove i dati venivano strutturati ad katsum perché bisognava release",
      "tokens": [
        40499,
        308,
        294,
        517,
        6,
        29850,
        4668,
        23287,
        741,
        1137,
        72,
        6138,
        592,
        3730,
        1056,
        13478,
        374,
        6908,
        614,
        350,
        1720,
        449,
        14303,
        7393,
        2912,
        4061,
        4374
      ],
      "temperature": 0,
      "avg_logprob": -0.26751911492995273,
      "compression_ratio": 1.3958333333333333,
      "no_speech_prob": 2.8012212993644425e-8
    },
    {
      "id": 260,
      "seek": 165280,
      "start": 1666.96,
      "end": 1675.08,
      "text": " fast e break things. Cosa ne pensi in merito a questa affermazione? Hai detto che facciamo",
      "tokens": [
        2370,
        308,
        1821,
        721,
        13,
        383,
        6447,
        408,
        6099,
        72,
        294,
        3551,
        3528,
        257,
        16540,
        2096,
        966,
        12928,
        30,
        24055,
        41031,
        947,
        1915,
        42052
      ],
      "temperature": 0,
      "avg_logprob": -0.26751911492995273,
      "compression_ratio": 1.3958333333333333,
      "no_speech_prob": 2.8012212993644425e-8
    },
    {
      "id": 261,
      "seek": 167508,
      "start": 1675.08,
      "end": 1684.4399999999998,
      "text": " un'altra puntata JavaScript contro TypeScript? Invitiamo anche il nostro comune amico Matteo",
      "tokens": [
        517,
        6,
        38865,
        18212,
        3274,
        15778,
        1583,
        15576,
        14237,
        30,
        682,
        10398,
        7415,
        11585,
        1930,
        35779,
        395,
        2613,
        669,
        2789,
        47544,
        78
      ],
      "temperature": 0,
      "avg_logprob": -0.2591538102659461,
      "compression_ratio": 1.5337078651685394,
      "no_speech_prob": 2.7852564699060167e-7
    },
    {
      "id": 262,
      "seek": 167508,
      "start": 1684.4399999999998,
      "end": 1692.04,
      "text": " in quel caso, ti ricordo una puntata dove Matteo disse che TypeScript era una proiezione",
      "tokens": [
        294,
        7178,
        9666,
        11,
        8757,
        21040,
        23872,
        2002,
        18212,
        3274,
        23287,
        47544,
        78,
        17581,
        947,
        15576,
        14237,
        4249,
        2002,
        447,
        18812,
        5328
      ],
      "temperature": 0,
      "avg_logprob": -0.2591538102659461,
      "compression_ratio": 1.5337078651685394,
      "no_speech_prob": 2.7852564699060167e-7
    },
    {
      "id": 263,
      "seek": 167508,
      "start": 1692.04,
      "end": 1697.8799999999999,
      "text": " mentale del tuo io digital, qualcosa del genere, cioè tipo TypeScript non esiste, sta solo",
      "tokens": [
        3074,
        1220,
        1103,
        45352,
        19785,
        4562,
        11,
        42400,
        1103,
        41553,
        11,
        41827,
        9746,
        15576,
        14237,
        2107,
        785,
        8375,
        11,
        11135,
        6944
      ],
      "temperature": 0,
      "avg_logprob": -0.2591538102659461,
      "compression_ratio": 1.5337078651685394,
      "no_speech_prob": 2.7852564699060167e-7
    },
    {
      "id": 264,
      "seek": 169788,
      "start": 1697.88,
      "end": 1712.8400000000001,
      "text": " sulla tua mente e dimenticalo! Hai assolutamente ragione, scherzi a parte, secondo me è stato",
      "tokens": [
        33625,
        33578,
        26577,
        308,
        274,
        2328,
        804,
        78,
        0,
        24055,
        1256,
        2308,
        3439,
        17539,
        5328,
        11,
        956,
        260,
        3992,
        257,
        6975,
        11,
        41601,
        385,
        4873,
        29657
      ],
      "temperature": 0,
      "avg_logprob": -0.3256737880217723,
      "compression_ratio": 1.4545454545454546,
      "no_speech_prob": 9.625565411397474e-8
    },
    {
      "id": 265,
      "seek": 169788,
      "start": 1712.8400000000001,
      "end": 1717.8000000000002,
      "text": " un passo molto importante, poi un passo che generalmente gli sviluppatori un po' attenti",
      "tokens": [
        517,
        38159,
        16394,
        9416,
        11,
        19260,
        517,
        38159,
        947,
        2674,
        4082,
        17161,
        17342,
        388,
        10504,
        39842,
        517,
        714,
        6,
        30980,
        72
      ],
      "temperature": 0,
      "avg_logprob": -0.3256737880217723,
      "compression_ratio": 1.4545454545454546,
      "no_speech_prob": 9.625565411397474e-8
    },
    {
      "id": 266,
      "seek": 169788,
      "start": 1717.8000000000002,
      "end": 1725.2,
      "text": " già facevano prima, perché io quando lavoro con Fastify non c'è Empoint che non abbia",
      "tokens": [
        30469,
        1851,
        85,
        3730,
        19507,
        11,
        14303,
        19785,
        7770,
        42060,
        416,
        15968,
        2505,
        2107,
        269,
        6,
        1462,
        8599,
        3600,
        947,
        2107,
        16903,
        654
      ],
      "temperature": 0,
      "avg_logprob": -0.3256737880217723,
      "compression_ratio": 1.4545454545454546,
      "no_speech_prob": 9.625565411397474e-8
    },
    {
      "id": 267,
      "seek": 172520,
      "start": 1725.2,
      "end": 1734,
      "text": " uno schema davanti che mi fa la validazione, diciamo che la validazione e la sanitizzazione",
      "tokens": [
        8526,
        34078,
        11753,
        11520,
        947,
        2752,
        2050,
        635,
        7363,
        12928,
        11,
        14285,
        7415,
        947,
        635,
        7363,
        12928,
        308,
        635,
        24533,
        8072,
        12928
      ],
      "temperature": 0,
      "avg_logprob": -0.29888980388641356,
      "compression_ratio": 1.5614035087719298,
      "no_speech_prob": 6.412054176507809e-8
    },
    {
      "id": 268,
      "seek": 172520,
      "start": 1734,
      "end": 1742.0800000000002,
      "text": " dei dati in input è un good practice che va fatta appunto, senza tanto ne quanto sia",
      "tokens": [
        13874,
        1137,
        72,
        294,
        4846,
        4873,
        517,
        665,
        3124,
        947,
        2773,
        283,
        1591,
        64,
        724,
        24052,
        11,
        36208,
        10331,
        408,
        17820,
        25176
      ],
      "temperature": 0,
      "avg_logprob": -0.29888980388641356,
      "compression_ratio": 1.5614035087719298,
      "no_speech_prob": 6.412054176507809e-8
    },
    {
      "id": 269,
      "seek": 172520,
      "start": 1742.0800000000002,
      "end": 1747.96,
      "text": " in entrata che in uscita e l'aver messo nero su bianco il contratto è un passaggio molto",
      "tokens": [
        294,
        8041,
        3274,
        947,
        294,
        505,
        66,
        2786,
        308,
        287,
        6,
        20655,
        2082,
        78,
        297,
        2032,
        459,
        272,
        952,
        1291,
        1930,
        40944,
        1353,
        4873,
        517,
        1320,
        30763,
        16394
      ],
      "temperature": 0,
      "avg_logprob": -0.29888980388641356,
      "compression_ratio": 1.5614035087719298,
      "no_speech_prob": 6.412054176507809e-8
    },
    {
      "id": 270,
      "seek": 174796,
      "start": 1747.96,
      "end": 1756.16,
      "text": " importante, nell'unico punto da sviluppatore JavaScript in cui questa cosa è essenziale,",
      "tokens": [
        9416,
        11,
        44666,
        6,
        409,
        2789,
        14326,
        1120,
        17342,
        388,
        10504,
        43148,
        15778,
        294,
        22929,
        16540,
        10163,
        4873,
        2097,
        11368,
        25051,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.2630914527679158,
      "compression_ratio": 1.59009009009009,
      "no_speech_prob": 1.2751834788105043e-7
    },
    {
      "id": 271,
      "seek": 174796,
      "start": 1756.16,
      "end": 1760.2,
      "text": " la comunicazione con l'esterno, dove tu i dati non li controlli, tu non sai che cosa",
      "tokens": [
        635,
        31710,
        12928,
        416,
        287,
        6,
        377,
        1248,
        78,
        11,
        23287,
        2604,
        741,
        1137,
        72,
        2107,
        375,
        1583,
        16320,
        11,
        2604,
        2107,
        32417,
        947,
        10163
      ],
      "temperature": 0,
      "avg_logprob": -0.2630914527679158,
      "compression_ratio": 1.59009009009009,
      "no_speech_prob": 1.2751834788105043e-7
    },
    {
      "id": 272,
      "seek": 174796,
      "start": 1760.2,
      "end": 1765.76,
      "text": " ti entra, poi in realtà li puoi sanitizzare, puoi validarli, puoi fare un sacco di cose,",
      "tokens": [
        8757,
        22284,
        11,
        19260,
        294,
        47512,
        375,
        2362,
        4869,
        24533,
        8072,
        543,
        11,
        2362,
        4869,
        7363,
        289,
        2081,
        11,
        2362,
        4869,
        11994,
        517,
        4899,
        1291,
        1026,
        30261,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.2630914527679158,
      "compression_ratio": 1.59009009009009,
      "no_speech_prob": 1.2751834788105043e-7
    },
    {
      "id": 273,
      "seek": 174796,
      "start": 1765.76,
      "end": 1772.76,
      "text": " tra l'altro schema non è che GraphQL permette di fare anche mille validazioni, permette",
      "tokens": [
        944,
        287,
        6,
        47484,
        34078,
        2107,
        4873,
        947,
        21884,
        13695,
        4784,
        3007,
        1026,
        11994,
        11585,
        1728,
        68,
        7363,
        27569,
        11,
        4784,
        3007
      ],
      "temperature": 0,
      "avg_logprob": -0.2630914527679158,
      "compression_ratio": 1.59009009009009,
      "no_speech_prob": 1.2751834788105043e-7
    },
    {
      "id": 274,
      "seek": 177276,
      "start": 1772.76,
      "end": 1778.92,
      "text": " di fare tante cose, su quelle directing fondamentalmente fai tutto quello che fai anche sempre, però",
      "tokens": [
        1026,
        11994,
        256,
        2879,
        30261,
        11,
        459,
        29237,
        1264,
        349,
        278,
        9557,
        44538,
        4082,
        283,
        1301,
        23048,
        22813,
        947,
        283,
        1301,
        11585,
        9553,
        11,
        12673
      ],
      "temperature": 0,
      "avg_logprob": -0.38947898585621904,
      "compression_ratio": 1.5666666666666667,
      "no_speech_prob": 4.0125598843587795e-8
    },
    {
      "id": 275,
      "seek": 177276,
      "start": 1778.92,
      "end": 1790.8799999999999,
      "text": " è di base, è naturale, ha una dipendazione molto basilaria, che però fa, come dici tu,",
      "tokens": [
        4873,
        1026,
        3096,
        11,
        4873,
        3303,
        68,
        11,
        324,
        2002,
        10460,
        521,
        12928,
        16394,
        987,
        2202,
        654,
        11,
        947,
        12673,
        2050,
        11,
        808,
        274,
        8787,
        2604,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.38947898585621904,
      "compression_ratio": 1.5666666666666667,
      "no_speech_prob": 4.0125598843587795e-8
    },
    {
      "id": 276,
      "seek": 177276,
      "start": 1790.8799999999999,
      "end": 1800.84,
      "text": " detta delle regole, detta delle regole che lo sviluppatore JavaScript, in cui motto spesso",
      "tokens": [
        48888,
        16485,
        1121,
        4812,
        11,
        48888,
        16485,
        1121,
        4812,
        947,
        450,
        17342,
        388,
        10504,
        43148,
        15778,
        11,
        294,
        22929,
        32680,
        637,
        5557
      ],
      "temperature": 0,
      "avg_logprob": -0.38947898585621904,
      "compression_ratio": 1.5666666666666667,
      "no_speech_prob": 4.0125598843587795e-8
    },
    {
      "id": 277,
      "seek": 180084,
      "start": 1800.84,
      "end": 1809.6799999999998,
      "text": " è quick and dirty, più dirti che quick tavola, ho delle robe nei miei software in giro che",
      "tokens": [
        4873,
        1702,
        293,
        9360,
        11,
        10589,
        11483,
        72,
        947,
        1702,
        1846,
        85,
        4711,
        11,
        1106,
        16485,
        37213,
        34517,
        12597,
        72,
        4722,
        294,
        1735,
        340,
        947
      ],
      "temperature": 0,
      "avg_logprob": -0.3449048475785689,
      "compression_ratio": 1.6306306306306306,
      "no_speech_prob": 5.122890343045583e-7
    },
    {
      "id": 278,
      "seek": 180084,
      "start": 1809.6799999999998,
      "end": 1817.9599999999998,
      "text": " veramente fanno i miei ribrivi, la validazione, la stabilizzazione della struttura delle cose",
      "tokens": [
        50079,
        283,
        13484,
        741,
        12597,
        72,
        9162,
        470,
        4917,
        11,
        635,
        7363,
        12928,
        11,
        635,
        11652,
        8072,
        12928,
        11618,
        1056,
        13478,
        2991,
        16485,
        30261
      ],
      "temperature": 0,
      "avg_logprob": -0.3449048475785689,
      "compression_ratio": 1.6306306306306306,
      "no_speech_prob": 5.122890343045583e-7
    },
    {
      "id": 279,
      "seek": 180084,
      "start": 1817.9599999999998,
      "end": 1822.72,
      "text": " si fa con i test, però anche li puoi bisogna saperli fare, fare delle porcate anche con",
      "tokens": [
        1511,
        2050,
        416,
        741,
        1500,
        11,
        12673,
        11585,
        375,
        2362,
        4869,
        40505,
        629,
        262,
        2332,
        2081,
        11994,
        11,
        11994,
        16485,
        1515,
        66,
        473,
        11585,
        416
      ],
      "temperature": 0,
      "avg_logprob": -0.3449048475785689,
      "compression_ratio": 1.6306306306306306,
      "no_speech_prob": 5.122890343045583e-7
    },
    {
      "id": 280,
      "seek": 180084,
      "start": 1822.72,
      "end": 1829.1999999999998,
      "text": " gli unit test è un'altra, e quindi questo è un po' l'approccio, concordo pienamente,",
      "tokens": [
        17161,
        4985,
        1500,
        4873,
        517,
        6,
        38865,
        11,
        308,
        15727,
        10263,
        4873,
        517,
        714,
        6,
        287,
        6,
        1746,
        24174,
        8529,
        11,
        1588,
        23872,
        26274,
        3439,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.3449048475785689,
      "compression_ratio": 1.6306306306306306,
      "no_speech_prob": 5.122890343045583e-7
    },
    {
      "id": 281,
      "seek": 182920,
      "start": 1829.2,
      "end": 1837.8400000000001,
      "text": " ha dato finalmente un linguaggio per comunicare correttamente fra front-end e back-end in",
      "tokens": [
        324,
        46971,
        35577,
        517,
        21766,
        30763,
        680,
        31710,
        543,
        1181,
        14313,
        3439,
        6600,
        1868,
        12,
        521,
        308,
        646,
        12,
        521,
        294
      ],
      "temperature": 0,
      "avg_logprob": -0.30079326033592224,
      "compression_ratio": 1.3773584905660377,
      "no_speech_prob": 2.4198845949285896e-7
    },
    {
      "id": 282,
      "seek": 182920,
      "start": 1837.8400000000001,
      "end": 1842.8,
      "text": " maniera buona, questo è essenziale.",
      "tokens": [
        587,
        10609,
        758,
        4037,
        11,
        10263,
        4873,
        2097,
        11368,
        25051,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.30079326033592224,
      "compression_ratio": 1.3773584905660377,
      "no_speech_prob": 2.4198845949285896e-7
    },
    {
      "id": 283,
      "seek": 182920,
      "start": 1842.8,
      "end": 1855.48,
      "text": " Facciamo una riflessione, magari mi sbaglio, però provavo a pensare al concetto di business",
      "tokens": [
        17667,
        42052,
        2002,
        13203,
        1832,
        5328,
        11,
        49932,
        2752,
        262,
        17282,
        19987,
        11,
        12673,
        1439,
        25713,
        257,
        6099,
        543,
        419,
        1588,
        23778,
        1026,
        1606
      ],
      "temperature": 0,
      "avg_logprob": -0.30079326033592224,
      "compression_ratio": 1.3773584905660377,
      "no_speech_prob": 2.4198845949285896e-7
    },
    {
      "id": 284,
      "seek": 185548,
      "start": 1855.48,
      "end": 1860.24,
      "text": " logic, non mi è chiara questa cosa nella mente, quindi te la dico come mi viene e poi",
      "tokens": [
        9952,
        11,
        2107,
        2752,
        4873,
        13228,
        2419,
        16540,
        10163,
        23878,
        26577,
        11,
        15727,
        535,
        635,
        274,
        2789,
        808,
        2752,
        19561,
        308,
        19260
      ],
      "temperature": 0,
      "avg_logprob": -0.32047860375766096,
      "compression_ratio": 1.63013698630137,
      "no_speech_prob": 3.689883669721894e-7
    },
    {
      "id": 285,
      "seek": 185548,
      "start": 1860.24,
      "end": 1861.64,
      "text": " ci ragioniamo insieme.",
      "tokens": [
        6983,
        17539,
        313,
        7415,
        1028,
        44940,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.32047860375766096,
      "compression_ratio": 1.63013698630137,
      "no_speech_prob": 3.689883669721894e-7
    },
    {
      "id": 286,
      "seek": 185548,
      "start": 1861.64,
      "end": 1868.52,
      "text": " Adesso buona parte delle nostre applicazioni sono, come dice Carmine, che a me oggi non",
      "tokens": [
        1999,
        5557,
        758,
        4037,
        6975,
        16485,
        10397,
        265,
        2580,
        27569,
        9259,
        11,
        808,
        10313,
        44530,
        533,
        11,
        947,
        257,
        385,
        34768,
        2107
      ],
      "temperature": 0,
      "avg_logprob": -0.32047860375766096,
      "compression_ratio": 1.63013698630137,
      "no_speech_prob": 3.689883669721894e-7
    },
    {
      "id": 287,
      "seek": 185548,
      "start": 1868.52,
      "end": 1873.4,
      "text": " c'è, sono il crudino della chiesa, o il crudino dell'azione cattolica, cioè quattro",
      "tokens": [
        269,
        6,
        1462,
        11,
        9259,
        1930,
        941,
        532,
        2982,
        11618,
        417,
        530,
        64,
        11,
        277,
        1930,
        941,
        532,
        2982,
        19781,
        6,
        12928,
        269,
        1591,
        401,
        2262,
        11,
        41827,
        421,
        1591,
        340
      ],
      "temperature": 0,
      "avg_logprob": -0.32047860375766096,
      "compression_ratio": 1.63013698630137,
      "no_speech_prob": 3.689883669721894e-7
    },
    {
      "id": 288,
      "seek": 185548,
      "start": 1873.4,
      "end": 1878.92,
      "text": " end point che ti fanno il get, il post e fanno il crud, fondamentalmente.",
      "tokens": [
        917,
        935,
        947,
        8757,
        283,
        13484,
        1930,
        483,
        11,
        1930,
        2183,
        308,
        283,
        13484,
        1930,
        941,
        532,
        11,
        9557,
        44538,
        4082,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.32047860375766096,
      "compression_ratio": 1.63013698630137,
      "no_speech_prob": 3.689883669721894e-7
    },
    {
      "id": 289,
      "seek": 187892,
      "start": 1878.92,
      "end": 1887.1200000000001,
      "text": " Le nostre applicazioni sono piene di crudini, grazie a Dio esistono tool come platformatic,",
      "tokens": [
        1456,
        10397,
        265,
        2580,
        27569,
        9259,
        280,
        10174,
        1026,
        941,
        532,
        3812,
        11,
        1295,
        3283,
        257,
        413,
        1004,
        785,
        468,
        8957,
        2290,
        808,
        3663,
        2399,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.3654027811686198,
      "compression_ratio": 1.409356725146199,
      "no_speech_prob": 4.546831533502882e-8
    },
    {
      "id": 290,
      "seek": 187892,
      "start": 1887.1200000000001,
      "end": 1891.96,
      "text": " come azura che un po' ci risolvono il problema con i limiti che hanno, però comunque è",
      "tokens": [
        808,
        7883,
        2991,
        947,
        517,
        714,
        6,
        6983,
        2253,
        401,
        85,
        8957,
        1930,
        12395,
        416,
        741,
        4948,
        72,
        947,
        26595,
        11,
        12673,
        45736,
        4873
      ],
      "temperature": 0,
      "avg_logprob": -0.3654027811686198,
      "compression_ratio": 1.409356725146199,
      "no_speech_prob": 4.546831533502882e-8
    },
    {
      "id": 291,
      "seek": 187892,
      "start": 1891.96,
      "end": 1901.0800000000002,
      "text": " una roba un po' da programmare, un po' da mechanical turkey.",
      "tokens": [
        2002,
        3870,
        64,
        517,
        714,
        6,
        1120,
        1461,
        15455,
        11,
        517,
        714,
        6,
        1120,
        12070,
        21551,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.3654027811686198,
      "compression_ratio": 1.409356725146199,
      "no_speech_prob": 4.546831533502882e-8
    },
    {
      "id": 292,
      "seek": 190108,
      "start": 1901.08,
      "end": 1910.28,
      "text": " D'altro canto però ci sono dei pezzi di business logic che invece hanno una certa complessità,",
      "tokens": [
        413,
        6,
        47484,
        393,
        1353,
        12673,
        6983,
        9259,
        13874,
        520,
        89,
        3992,
        1026,
        1606,
        9952,
        947,
        36344,
        26595,
        2002,
        44438,
        1209,
        442,
        12445,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.24953275331309144,
      "compression_ratio": 1.4519774011299436,
      "no_speech_prob": 4.163644096166763e-9
    },
    {
      "id": 293,
      "seek": 190108,
      "start": 1910.28,
      "end": 1914.84,
      "text": " dove abbiamo certi dati che entrano, abbiamo delle dipendenze esterne che è un database",
      "tokens": [
        23287,
        22815,
        5351,
        72,
        1137,
        72,
        947,
        8041,
        3730,
        11,
        22815,
        16485,
        10460,
        8896,
        1381,
        871,
        48135,
        947,
        4873,
        517,
        8149
      ],
      "temperature": 0,
      "avg_logprob": -0.24953275331309144,
      "compression_ratio": 1.4519774011299436,
      "no_speech_prob": 4.163644096166763e-9
    },
    {
      "id": 294,
      "seek": 190108,
      "start": 1914.84,
      "end": 1920.6399999999999,
      "text": " e dei dati risultanti che non necessariamente fittano 100% il database.",
      "tokens": [
        308,
        13874,
        1137,
        72,
        2253,
        723,
        11520,
        947,
        2107,
        2688,
        45149,
        48876,
        3730,
        2319,
        4,
        1930,
        8149,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.24953275331309144,
      "compression_ratio": 1.4519774011299436,
      "no_speech_prob": 4.163644096166763e-9
    },
    {
      "id": 295,
      "seek": 192064,
      "start": 1920.64,
      "end": 1931.1200000000001,
      "text": " È da un po' che mi chiedo, in casi dove la business logic è fortemente presente,",
      "tokens": [
        34495,
        1120,
        517,
        714,
        6,
        947,
        2752,
        417,
        36035,
        11,
        294,
        22567,
        23287,
        635,
        1606,
        9952,
        4873,
        5009,
        16288,
        28709,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.2747089958190918,
      "compression_ratio": 1.5,
      "no_speech_prob": 9.833213354681902e-9
    },
    {
      "id": 296,
      "seek": 192064,
      "start": 1931.1200000000001,
      "end": 1936.64,
      "text": " tu hai parlato di pachettizzazione, di booking, che sono dei casi dove in realtà il calcolo",
      "tokens": [
        2604,
        21822,
        13734,
        2513,
        1026,
        280,
        608,
        3093,
        8072,
        12928,
        11,
        1026,
        34424,
        11,
        947,
        9259,
        13874,
        22567,
        23287,
        294,
        47512,
        1930,
        2104,
        46086
      ],
      "temperature": 0,
      "avg_logprob": -0.2747089958190918,
      "compression_ratio": 1.5,
      "no_speech_prob": 9.833213354681902e-9
    },
    {
      "id": 297,
      "seek": 192064,
      "start": 1936.64,
      "end": 1943.68,
      "text": " della availability di una certa struttura presuppone un'importante business logic, ecco",
      "tokens": [
        11618,
        17945,
        1026,
        2002,
        44438,
        1056,
        13478,
        2991,
        1183,
        10504,
        546,
        517,
        6,
        20737,
        2879,
        1606,
        9952,
        11,
        11437,
        1291
      ],
      "temperature": 0,
      "avg_logprob": -0.2747089958190918,
      "compression_ratio": 1.5,
      "no_speech_prob": 9.833213354681902e-9
    },
    {
      "id": 298,
      "seek": 192064,
      "start": 1943.68,
      "end": 1947.88,
      "text": " GraphQL ci può venire d'aiuto, te lo dico perché adesso sto scrivendo un motore di",
      "tokens": [
        21884,
        13695,
        6983,
        26526,
        6138,
        621,
        274,
        6,
        1301,
        8262,
        11,
        535,
        450,
        274,
        2789,
        14303,
        39552,
        22784,
        5545,
        85,
        3999,
        517,
        2184,
        418,
        1026
      ],
      "temperature": 0,
      "avg_logprob": -0.2747089958190918,
      "compression_ratio": 1.5,
      "no_speech_prob": 9.833213354681902e-9
    },
    {
      "id": 299,
      "seek": 194788,
      "start": 1947.88,
      "end": 1953.88,
      "text": " booking e mi sto ponendo proprio questa domanda, mi sto chiedendo, gli end point sono tre end",
      "tokens": [
        34424,
        308,
        2752,
        22784,
        9224,
        3999,
        28203,
        16540,
        3285,
        5575,
        11,
        2752,
        22784,
        417,
        1091,
        3999,
        11,
        17161,
        917,
        935,
        9259,
        2192,
        917
      ],
      "temperature": 0,
      "avg_logprob": -0.3019525377373946,
      "compression_ratio": 1.483695652173913,
      "no_speech_prob": 1.4225767586140137e-7
    },
    {
      "id": 300,
      "seek": 194788,
      "start": 1953.88,
      "end": 1965.24,
      "text": " point cagati, ma GraphQL mi può aiutare in qualcosa dove la business logic è importante?",
      "tokens": [
        935,
        269,
        559,
        6908,
        11,
        463,
        21884,
        13695,
        2752,
        26526,
        9783,
        325,
        543,
        294,
        42400,
        23287,
        635,
        1606,
        9952,
        4873,
        9416,
        30
      ],
      "temperature": 0,
      "avg_logprob": -0.3019525377373946,
      "compression_ratio": 1.483695652173913,
      "no_speech_prob": 1.4225767586140137e-7
    },
    {
      "id": 301,
      "seek": 194788,
      "start": 1965.24,
      "end": 1973.1200000000001,
      "text": " Sì, assolutamente, non il linguaggio, cioè non GraphQL inteso come linguaggio, perché",
      "tokens": [
        318,
        4749,
        11,
        1256,
        2308,
        3439,
        11,
        2107,
        1930,
        21766,
        30763,
        11,
        41827,
        2107,
        21884,
        13695,
        560,
        41189,
        808,
        21766,
        30763,
        11,
        14303
      ],
      "temperature": 0,
      "avg_logprob": -0.3019525377373946,
      "compression_ratio": 1.483695652173913,
      "no_speech_prob": 1.4225767586140137e-7
    },
    {
      "id": 302,
      "seek": 197312,
      "start": 1973.12,
      "end": 1979.28,
      "text": " tu comunque lo interrovi sempre nella stessa maniera. Gli engine su cui GraphQL gira, con",
      "tokens": [
        2604,
        45736,
        450,
        728,
        340,
        4917,
        9553,
        23878,
        342,
        8391,
        587,
        10609,
        13,
        460,
        2081,
        2848,
        459,
        22929,
        21884,
        13695,
        290,
        4271,
        11,
        416
      ],
      "temperature": 0,
      "avg_logprob": -0.3108426626626547,
      "compression_ratio": 1.4943181818181819,
      "no_speech_prob": 2.631501239136469e-8
    },
    {
      "id": 303,
      "seek": 197312,
      "start": 1979.28,
      "end": 1989.84,
      "text": " il sistema dei resolver, il sistema dei tipi che vengono forniti dal sistema, permette",
      "tokens": [
        1930,
        13245,
        13874,
        34480,
        11,
        1930,
        13245,
        13874,
        4125,
        72,
        947,
        371,
        1501,
        8957,
        337,
        77,
        8707,
        11702,
        13245,
        11,
        4784,
        3007
      ],
      "temperature": 0,
      "avg_logprob": -0.3108426626626547,
      "compression_ratio": 1.4943181818181819,
      "no_speech_prob": 2.631501239136469e-8
    },
    {
      "id": 304,
      "seek": 197312,
      "start": 1989.84,
      "end": 2000.1599999999999,
      "text": " di fare delle cose molto interessanti, poi c'è un po' di tooling da usare insieme per",
      "tokens": [
        1026,
        11994,
        16485,
        30261,
        16394,
        12478,
        11520,
        11,
        19260,
        269,
        6,
        1462,
        517,
        714,
        6,
        1026,
        46593,
        1120,
        505,
        543,
        1028,
        44940,
        680
      ],
      "temperature": 0,
      "avg_logprob": -0.3108426626626547,
      "compression_ratio": 1.4943181818181819,
      "no_speech_prob": 2.631501239136469e-8
    },
    {
      "id": 305,
      "seek": 200016,
      "start": 2000.16,
      "end": 2008.88,
      "text": " gestire i problemati di reloading, di caching, tutta una serie di cose. La cosa veramente",
      "tokens": [
        7219,
        621,
        741,
        1154,
        6908,
        1026,
        25628,
        278,
        11,
        1026,
        269,
        2834,
        11,
        3672,
        1328,
        2002,
        23030,
        1026,
        30261,
        13,
        2369,
        10163,
        50079
      ],
      "temperature": 0,
      "avg_logprob": -0.3119789877055604,
      "compression_ratio": 1.5,
      "no_speech_prob": 7.614500674435476e-8
    },
    {
      "id": 306,
      "seek": 200016,
      "start": 2008.88,
      "end": 2015.4,
      "text": " secondo me figa di GraphQL, da questo punto di vista, è che tu il singolo dato puoi andare",
      "tokens": [
        41601,
        385,
        2147,
        64,
        1026,
        21884,
        13695,
        11,
        1120,
        10263,
        14326,
        1026,
        22553,
        11,
        4873,
        947,
        2604,
        1930,
        1522,
        7902,
        46971,
        2362,
        4869,
        42742
      ],
      "temperature": 0,
      "avg_logprob": -0.3119789877055604,
      "compression_ratio": 1.5,
      "no_speech_prob": 7.614500674435476e-8
    },
    {
      "id": 307,
      "seek": 200016,
      "start": 2015.4,
      "end": 2024.92,
      "text": " a mettere la logica nel singolo dato, ok? E il tutto ti viene tenuto in ordine dal sistema,",
      "tokens": [
        257,
        27812,
        323,
        635,
        3565,
        2262,
        15373,
        1522,
        7902,
        46971,
        11,
        3133,
        30,
        462,
        1930,
        23048,
        8757,
        19561,
        2064,
        8262,
        294,
        4792,
        533,
        11702,
        13245,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.3119789877055604,
      "compression_ratio": 1.5,
      "no_speech_prob": 7.614500674435476e-8
    },
    {
      "id": 308,
      "seek": 202492,
      "start": 2024.92,
      "end": 2031.3600000000001,
      "text": " cioè il sistema ti permette di dire che il prezzo del prodotto X è calcolato seguendo",
      "tokens": [
        41827,
        1930,
        13245,
        8757,
        4784,
        3007,
        1026,
        1264,
        947,
        1930,
        659,
        35130,
        1103,
        15792,
        18838,
        1783,
        4873,
        2104,
        8768,
        2513,
        8878,
        3999
      ],
      "temperature": 0,
      "avg_logprob": -0.22434903117059504,
      "compression_ratio": 1.577092511013216,
      "no_speech_prob": 7.453759280906525e-7
    },
    {
      "id": 309,
      "seek": 202492,
      "start": 2031.3600000000001,
      "end": 2037.68,
      "text": " delle logiche e tu puoi andare in quel punto a pensare esclusivamente a quel punto, non",
      "tokens": [
        16485,
        3565,
        9304,
        308,
        2604,
        2362,
        4869,
        42742,
        294,
        7178,
        14326,
        257,
        6099,
        543,
        4721,
        3063,
        23957,
        257,
        7178,
        14326,
        11,
        2107
      ],
      "temperature": 0,
      "avg_logprob": -0.22434903117059504,
      "compression_ratio": 1.577092511013216,
      "no_speech_prob": 7.453759280906525e-7
    },
    {
      "id": 310,
      "seek": 202492,
      "start": 2037.68,
      "end": 2047.8400000000001,
      "text": " devi fare dei salti mortali, non devi andare a organizzare tutta una funzione di 50 righe",
      "tokens": [
        31219,
        11994,
        13874,
        5139,
        72,
        6599,
        5103,
        11,
        2107,
        31219,
        42742,
        257,
        4645,
        89,
        543,
        3672,
        1328,
        2002,
        1019,
        19706,
        1026,
        2625,
        8329,
        675
      ],
      "temperature": 0,
      "avg_logprob": -0.22434903117059504,
      "compression_ratio": 1.577092511013216,
      "no_speech_prob": 7.453759280906525e-7
    },
    {
      "id": 311,
      "seek": 202492,
      "start": 2047.8400000000001,
      "end": 2054.32,
      "text": " perché devi ritornare a una struttura che poi verrà ritornata dalla resta. Tu ti preoccupi",
      "tokens": [
        14303,
        31219,
        11289,
        1865,
        543,
        257,
        2002,
        1056,
        13478,
        2991,
        947,
        19260,
        1306,
        39212,
        11289,
        1865,
        3274,
        35566,
        1472,
        64,
        13,
        7836,
        8757,
        44388,
        72
      ],
      "temperature": 0,
      "avg_logprob": -0.22434903117059504,
      "compression_ratio": 1.577092511013216,
      "no_speech_prob": 7.453759280906525e-7
    },
    {
      "id": 312,
      "seek": 205432,
      "start": 2054.32,
      "end": 2059.1600000000003,
      "text": " solo di quello. Ovviamente qualcuno potrebbe dire, eh, ma se tu devi andare a caricare dei",
      "tokens": [
        6944,
        1026,
        22813,
        13,
        50005,
        23347,
        32101,
        12638,
        1847,
        39487,
        1264,
        11,
        7670,
        11,
        463,
        369,
        2604,
        31219,
        42742,
        257,
        45732,
        543,
        13874
      ],
      "temperature": 0,
      "avg_logprob": -0.27820254817153467,
      "compression_ratio": 1.6323529411764706,
      "no_speech_prob": 3.466320492861996e-7
    },
    {
      "id": 313,
      "seek": 205432,
      "start": 2059.1600000000003,
      "end": 2063,
      "text": " dati lì che poi ti servono anche dall'altra parte, con il resto li carichi una volta e",
      "tokens": [
        1137,
        72,
        287,
        4749,
        947,
        19260,
        8757,
        1658,
        8957,
        11585,
        43351,
        6,
        38865,
        6975,
        11,
        416,
        1930,
        28247,
        375,
        1032,
        18543,
        2002,
        18765,
        308
      ],
      "temperature": 0,
      "avg_logprob": -0.27820254817153467,
      "compression_ratio": 1.6323529411764706,
      "no_speech_prob": 3.466320492861996e-7
    },
    {
      "id": 314,
      "seek": 205432,
      "start": 2063,
      "end": 2068.88,
      "text": " li usi dappertutto. Questa cosa con GraphQL si fa nella stessa identica maniera, ci sono",
      "tokens": [
        375,
        505,
        72,
        1120,
        427,
        911,
        28698,
        13,
        2326,
        7841,
        10163,
        416,
        21884,
        13695,
        1511,
        2050,
        23878,
        342,
        8391,
        2473,
        2262,
        587,
        10609,
        11,
        6983,
        9259
      ],
      "temperature": 0,
      "avg_logprob": -0.27820254817153467,
      "compression_ratio": 1.6323529411764706,
      "no_speech_prob": 3.466320492861996e-7
    },
    {
      "id": 315,
      "seek": 205432,
      "start": 2068.88,
      "end": 2074.48,
      "text": " dei sistemi, tu hai modo, hai modo tramite i data loader di avere un unico punto d'accesso",
      "tokens": [
        13874,
        10555,
        13372,
        11,
        2604,
        21822,
        16664,
        11,
        21822,
        16664,
        25749,
        642,
        741,
        1412,
        3677,
        260,
        1026,
        37914,
        517,
        517,
        2789,
        14326,
        274,
        6,
        326,
        43611
      ],
      "temperature": 0,
      "avg_logprob": -0.27820254817153467,
      "compression_ratio": 1.6323529411764706,
      "no_speech_prob": 3.466320492861996e-7
    },
    {
      "id": 316,
      "seek": 205432,
      "start": 2074.48,
      "end": 2079.36,
      "text": " per il caricamento dei dati, questa cosa ti permette fra l'altro utilizzando anche il",
      "tokens": [
        680,
        1930,
        45732,
        8824,
        13874,
        1137,
        72,
        11,
        16540,
        10163,
        8757,
        4784,
        3007,
        6600,
        287,
        6,
        47484,
        40355,
        1806,
        11585,
        1930
      ],
      "temperature": 0,
      "avg_logprob": -0.27820254817153467,
      "compression_ratio": 1.6323529411764706,
      "no_speech_prob": 3.466320492861996e-7
    },
    {
      "id": 317,
      "seek": 207936,
      "start": 2079.36,
      "end": 2087.7200000000003,
      "text": " sistema di caching un po' di fregartene dell'andare a pescare un dato. Quante volte tu vai a",
      "tokens": [
        13245,
        1026,
        269,
        2834,
        517,
        714,
        6,
        1026,
        2130,
        49330,
        1450,
        19781,
        6,
        474,
        543,
        257,
        9262,
        5685,
        517,
        46971,
        13,
        2326,
        2879,
        37801,
        2604,
        4405,
        257
      ],
      "temperature": 0,
      "avg_logprob": -0.31322489558039485,
      "compression_ratio": 1.639269406392694,
      "no_speech_prob": 4.664434811729734e-7
    },
    {
      "id": 318,
      "seek": 207936,
      "start": 2087.7200000000003,
      "end": 2097.1600000000003,
      "text": " pescare i dati oppure li metti nella query, scusa, li metti nel codice perché non vuoi",
      "tokens": [
        9262,
        5685,
        741,
        1137,
        72,
        1458,
        540,
        375,
        1131,
        7317,
        23878,
        14581,
        11,
        795,
        20318,
        11,
        375,
        1131,
        7317,
        15373,
        17656,
        573,
        14303,
        2107,
        9732,
        4869
      ],
      "temperature": 0,
      "avg_logprob": -0.31322489558039485,
      "compression_ratio": 1.639269406392694,
      "no_speech_prob": 4.664434811729734e-7
    },
    {
      "id": 319,
      "seek": 207936,
      "start": 2097.1600000000003,
      "end": 2101.6,
      "text": " andarli a richiedere tutte le volte, con un sistema di caching ben fatto. In realtà poi",
      "tokens": [
        50009,
        2081,
        257,
        4593,
        1091,
        323,
        38632,
        476,
        37801,
        11,
        416,
        517,
        13245,
        1026,
        269,
        2834,
        3271,
        23228,
        13,
        682,
        47512,
        19260
      ],
      "temperature": 0,
      "avg_logprob": -0.31322489558039485,
      "compression_ratio": 1.639269406392694,
      "no_speech_prob": 4.664434811729734e-7
    },
    {
      "id": 320,
      "seek": 207936,
      "start": 2101.6,
      "end": 2106.6800000000003,
      "text": " il sistema di caching non fa neanche il resto. Diciamo che GraphQL ti forza un po' a fare",
      "tokens": [
        1930,
        13245,
        1026,
        269,
        2834,
        2107,
        2050,
        408,
        22806,
        1930,
        28247,
        13,
        413,
        299,
        7415,
        947,
        21884,
        13695,
        8757,
        337,
        2394,
        517,
        714,
        6,
        257,
        11994
      ],
      "temperature": 0,
      "avg_logprob": -0.31322489558039485,
      "compression_ratio": 1.639269406392694,
      "no_speech_prob": 4.664434811729734e-7
    },
    {
      "id": 321,
      "seek": 210668,
      "start": 2106.68,
      "end": 2112.96,
      "text": " certi shape, questo non è un vantaggio perché alla fine è una forzatura, però ti abitui",
      "tokens": [
        5351,
        72,
        3909,
        11,
        10263,
        2107,
        4873,
        517,
        371,
        394,
        30763,
        14303,
        11591,
        2489,
        4873,
        2002,
        337,
        89,
        19660,
        11,
        12673,
        8757,
        410,
        270,
        3077
      ],
      "temperature": 0,
      "avg_logprob": -0.31644936970302034,
      "compression_ratio": 1.5913043478260869,
      "no_speech_prob": 2.699572974051989e-7
    },
    {
      "id": 322,
      "seek": 210668,
      "start": 2112.96,
      "end": 2123.04,
      "text": " un po' a lavorare con queste cose e quindi faccio un esempio, una tipologia prodotto,",
      "tokens": [
        517,
        714,
        6,
        257,
        29241,
        543,
        416,
        35455,
        30261,
        308,
        15727,
        1915,
        8529,
        517,
        33627,
        11,
        2002,
        4125,
        24103,
        15792,
        18838,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.31644936970302034,
      "compression_ratio": 1.5913043478260869,
      "no_speech_prob": 2.699572974051989e-7
    },
    {
      "id": 323,
      "seek": 210668,
      "start": 2123.04,
      "end": 2127.96,
      "text": " tu prendi il prodotto, il prodotto te lo restituisce solo lui di della tipologia e tu hai 20 tipologie",
      "tokens": [
        2604,
        9866,
        72,
        1930,
        15792,
        18838,
        11,
        1930,
        15792,
        18838,
        535,
        450,
        1472,
        6380,
        49596,
        6944,
        8783,
        1026,
        11618,
        4125,
        24103,
        308,
        2604,
        21822,
        945,
        4125,
        20121
      ],
      "temperature": 0,
      "avg_logprob": -0.31644936970302034,
      "compression_ratio": 1.5913043478260869,
      "no_speech_prob": 2.699572974051989e-7
    },
    {
      "id": 324,
      "seek": 210668,
      "start": 2127.96,
      "end": 2134.2,
      "text": " in tutto, non ne hai di più, parliamo di un caso abbastanza ristretto. Cosa fai? Vai",
      "tokens": [
        294,
        23048,
        11,
        2107,
        408,
        21822,
        1026,
        10589,
        11,
        971,
        49926,
        1026,
        517,
        9666,
        16903,
        525,
        20030,
        367,
        468,
        1505,
        1353,
        13,
        383,
        6447,
        283,
        1301,
        30,
        24206
      ],
      "temperature": 0,
      "avg_logprob": -0.31644936970302034,
      "compression_ratio": 1.5913043478260869,
      "no_speech_prob": 2.699572974051989e-7
    },
    {
      "id": 325,
      "seek": 213420,
      "start": 2134.2,
      "end": 2142.3999999999996,
      "text": " a chiedere la singola tipologia al sistema? Sì, gliela vai a chiedere, poi te la cachi,",
      "tokens": [
        257,
        417,
        1091,
        323,
        635,
        1522,
        4711,
        4125,
        24103,
        419,
        13245,
        30,
        318,
        4749,
        11,
        1563,
        1187,
        64,
        4405,
        257,
        417,
        1091,
        323,
        11,
        19260,
        535,
        635,
        269,
        326,
        4954,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.25035126426003196,
      "compression_ratio": 1.583710407239819,
      "no_speech_prob": 6.513025851972998e-8
    },
    {
      "id": 326,
      "seek": 213420,
      "start": 2142.3999999999996,
      "end": 2148.48,
      "text": " tanto la tipologia del prodotto è una roba che non cambia, quindi è una roba che può",
      "tokens": [
        10331,
        635,
        4125,
        24103,
        1103,
        15792,
        18838,
        4873,
        2002,
        3870,
        64,
        947,
        2107,
        18751,
        654,
        11,
        15727,
        4873,
        2002,
        3870,
        64,
        947,
        26526
      ],
      "temperature": 0,
      "avg_logprob": -0.25035126426003196,
      "compression_ratio": 1.583710407239819,
      "no_speech_prob": 6.513025851972998e-8
    },
    {
      "id": 327,
      "seek": 213420,
      "start": 2148.48,
      "end": 2153.2,
      "text": " stare tranquillamente, puoi caricarlo ogni minuto, ogni minuto puoi fare di nuovo la",
      "tokens": [
        22432,
        17640,
        373,
        3439,
        11,
        2362,
        4869,
        45732,
        19457,
        33189,
        923,
        8262,
        11,
        33189,
        923,
        8262,
        2362,
        4869,
        11994,
        1026,
        49348,
        635
      ],
      "temperature": 0,
      "avg_logprob": -0.25035126426003196,
      "compression_ratio": 1.583710407239819,
      "no_speech_prob": 6.513025851972998e-8
    },
    {
      "id": 328,
      "seek": 213420,
      "start": 2153.2,
      "end": 2158.9199999999996,
      "text": " richiesta. Quindi questa cosa ti fa benissimo il resto e la fai dappertutto. Con GraphQL",
      "tokens": [
        4593,
        38804,
        13,
        32534,
        16540,
        10163,
        8757,
        2050,
        3271,
        34966,
        1930,
        28247,
        308,
        635,
        283,
        1301,
        1120,
        427,
        911,
        28698,
        13,
        2656,
        21884,
        13695
      ],
      "temperature": 0,
      "avg_logprob": -0.25035126426003196,
      "compression_ratio": 1.583710407239819,
      "no_speech_prob": 6.513025851972998e-8
    },
    {
      "id": 329,
      "seek": 215892,
      "start": 2158.92,
      "end": 2166.36,
      "text": " tu puoi tranquillamente nel tuo resolverino fare questa cosa fregandotene delle performance",
      "tokens": [
        2604,
        2362,
        4869,
        17640,
        373,
        3439,
        15373,
        45352,
        34480,
        2982,
        11994,
        16540,
        10163,
        2130,
        70,
        474,
        310,
        1450,
        16485,
        3389
      ],
      "temperature": 0,
      "avg_logprob": -0.2509212684631348,
      "compression_ratio": 1.5714285714285714,
      "no_speech_prob": 2.286290268216362e-8
    },
    {
      "id": 330,
      "seek": 215892,
      "start": 2166.36,
      "end": 2172.7200000000003,
      "text": " perché tanto un sistema di caching o di preloading o di cose fatto per bene ti dà tutta questa",
      "tokens": [
        14303,
        10331,
        517,
        13245,
        1026,
        269,
        2834,
        277,
        1026,
        659,
        2907,
        278,
        277,
        1026,
        30261,
        23228,
        680,
        2537,
        8757,
        274,
        1467,
        3672,
        1328,
        16540
      ],
      "temperature": 0,
      "avg_logprob": -0.2509212684631348,
      "compression_ratio": 1.5714285714285714,
      "no_speech_prob": 2.286290268216362e-8
    },
    {
      "id": 331,
      "seek": 215892,
      "start": 2172.7200000000003,
      "end": 2177.6,
      "text": " automazione e ti permette di farlo. Poi se devi fare invece una roba complessa perché",
      "tokens": [
        3553,
        12928,
        308,
        8757,
        4784,
        3007,
        1026,
        1400,
        752,
        13,
        430,
        4869,
        369,
        31219,
        11994,
        36344,
        2002,
        3870,
        64,
        1209,
        8391,
        14303
      ],
      "temperature": 0,
      "avg_logprob": -0.2509212684631348,
      "compression_ratio": 1.5714285714285714,
      "no_speech_prob": 2.286290268216362e-8
    },
    {
      "id": 332,
      "seek": 215892,
      "start": 2177.6,
      "end": 2181.48,
      "text": " devi fare dei calcoli e cose così, alla fine è una funzione come un'altra, non è che",
      "tokens": [
        31219,
        11994,
        13874,
        2104,
        8768,
        72,
        308,
        30261,
        23278,
        11,
        11591,
        2489,
        4873,
        2002,
        1019,
        19706,
        808,
        517,
        6,
        38865,
        11,
        2107,
        4873,
        947
      ],
      "temperature": 0,
      "avg_logprob": -0.2509212684631348,
      "compression_ratio": 1.5714285714285714,
      "no_speech_prob": 2.286290268216362e-8
    },
    {
      "id": 333,
      "seek": 218148,
      "start": 2181.48,
      "end": 2190.68,
      "text": " ci siano delle cose speciali, cioè non ti facilita. Tu hai un ingresso e dei resolver,",
      "tokens": [
        6983,
        262,
        6254,
        16485,
        30261,
        2121,
        72,
        11,
        41827,
        2107,
        8757,
        10217,
        2786,
        13,
        7836,
        21822,
        517,
        3957,
        29652,
        308,
        13874,
        34480,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.28061912677906176,
      "compression_ratio": 1.704225352112676,
      "no_speech_prob": 3.486175259581614e-8
    },
    {
      "id": 334,
      "seek": 218148,
      "start": 2190.68,
      "end": 2197.68,
      "text": " punto, hai dei dati e ogni dato ha un resolver. Questo resolver ti restituisce uno scalare",
      "tokens": [
        2362,
        580,
        78,
        11,
        21822,
        13874,
        1137,
        72,
        308,
        33189,
        46971,
        324,
        517,
        34480,
        13,
        38167,
        34480,
        8757,
        1472,
        6380,
        49596,
        8526,
        15664,
        543
      ],
      "temperature": 0,
      "avg_logprob": -0.28061912677906176,
      "compression_ratio": 1.704225352112676,
      "no_speech_prob": 3.486175259581614e-8
    },
    {
      "id": 335,
      "seek": 218148,
      "start": 2197.68,
      "end": 2203.16,
      "text": " se uno scalare, ti restituisce una struttura o qualcosa del genere se all'interno di questo",
      "tokens": [
        369,
        8526,
        15664,
        543,
        11,
        8757,
        1472,
        6380,
        49596,
        2002,
        1056,
        13478,
        2991,
        277,
        42400,
        1103,
        41553,
        369,
        439,
        6,
        5106,
        1771,
        1026,
        10263
      ],
      "temperature": 0,
      "avg_logprob": -0.28061912677906176,
      "compression_ratio": 1.704225352112676,
      "no_speech_prob": 3.486175259581614e-8
    },
    {
      "id": 336,
      "seek": 218148,
      "start": 2203.16,
      "end": 2209.76,
      "text": " attributo è un altro oggetto e così è cascata, tu puoi fare le cose e avere dei risultati",
      "tokens": [
        9080,
        8262,
        4873,
        517,
        40924,
        5360,
        847,
        1353,
        308,
        23278,
        4873,
        3058,
        66,
        3274,
        11,
        2604,
        2362,
        4869,
        11994,
        476,
        30261,
        308,
        37914,
        13874,
        2253,
        723,
        6908
      ],
      "temperature": 0,
      "avg_logprob": -0.28061912677906176,
      "compression_ratio": 1.704225352112676,
      "no_speech_prob": 3.486175259581614e-8
    },
    {
      "id": 337,
      "seek": 220976,
      "start": 2209.76,
      "end": 2218.5600000000004,
      "text": " che nella mia esperienza funzionano. Poi se tu fai un node che restituisce un JSON statico",
      "tokens": [
        947,
        23878,
        21290,
        10045,
        42331,
        49345,
        313,
        3730,
        13,
        430,
        4869,
        369,
        2604,
        283,
        1301,
        517,
        9984,
        947,
        1472,
        6380,
        49596,
        517,
        31828,
        2219,
        2789
      ],
      "temperature": 0,
      "avg_logprob": -0.27814462449815536,
      "compression_ratio": 1.289855072463768,
      "no_speech_prob": 6.276700332819019e-7
    },
    {
      "id": 338,
      "seek": 220976,
      "start": 2218.5600000000004,
      "end": 2227.88,
      "text": " è più veloce. Tieni presente che devi anche stare attento a cosa usi per serializzare",
      "tokens": [
        4873,
        10589,
        1241,
        752,
        384,
        13,
        314,
        35462,
        28709,
        947,
        31219,
        11585,
        22432,
        951,
        15467,
        257,
        10163,
        505,
        72,
        680,
        17436,
        8072,
        543
      ],
      "temperature": 0,
      "avg_logprob": -0.27814462449815536,
      "compression_ratio": 1.289855072463768,
      "no_speech_prob": 6.276700332819019e-7
    },
    {
      "id": 339,
      "seek": 222788,
      "start": 2227.88,
      "end": 2249.52,
      "text": " il JSON. Ti apro una parentesi, in un progetto in passato avevamo un Postgres che aveva una",
      "tokens": [
        1930,
        31828,
        13,
        20456,
        14602,
        2002,
        2596,
        21181,
        11,
        294,
        517,
        447,
        847,
        1353,
        294,
        1320,
        2513,
        3472,
        85,
        10502,
        517,
        10223,
        45189,
        947,
        3472,
        2757,
        2002
      ],
      "temperature": 0,
      "avg_logprob": -0.35295348013600997,
      "compression_ratio": 1.0963855421686748,
      "no_speech_prob": 6.577907356586365e-7
    },
    {
      "id": 340,
      "seek": 224952,
      "start": 2249.52,
      "end": 2258.08,
      "text": " colonna JSON, un record che aveva una colonna JSON. Il problema è che una colonna JSON,",
      "tokens": [
        1173,
        784,
        31828,
        11,
        517,
        2136,
        947,
        3472,
        2757,
        2002,
        1173,
        784,
        31828,
        13,
        4416,
        12395,
        4873,
        947,
        2002,
        1173,
        784,
        31828,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.2588788596066562,
      "compression_ratio": 1.5227272727272727,
      "no_speech_prob": 3.0288415331369833e-8
    },
    {
      "id": 341,
      "seek": 224952,
      "start": 2258.08,
      "end": 2269.48,
      "text": " quando è di 300k, 400k, è un JSON da 300k da deserializzare. Quindi un robo che noi",
      "tokens": [
        7770,
        4873,
        1026,
        6641,
        74,
        11,
        8423,
        74,
        11,
        4873,
        517,
        31828,
        1120,
        6641,
        74,
        1120,
        730,
        260,
        831,
        8072,
        543,
        13,
        32534,
        517,
        744,
        1763,
        947,
        22447
      ],
      "temperature": 0,
      "avg_logprob": -0.2588788596066562,
      "compression_ratio": 1.5227272727272727,
      "no_speech_prob": 3.0288415331369833e-8
    },
    {
      "id": 342,
      "seek": 224952,
      "start": 2269.48,
      "end": 2275.72,
      "text": " prendevamo e così com'era, lo deserializzavamo e lo mandavamo al frontend senza farci niente",
      "tokens": [
        9866,
        13379,
        10502,
        308,
        23278,
        395,
        6,
        1663,
        11,
        450,
        730,
        260,
        831,
        8072,
        706,
        10502,
        308,
        450,
        7411,
        706,
        10502,
        419,
        1868,
        521,
        36208,
        1400,
        537,
        297,
        8413
      ],
      "temperature": 0,
      "avg_logprob": -0.2588788596066562,
      "compression_ratio": 1.5227272727272727,
      "no_speech_prob": 3.0288415331369833e-8
    },
    {
      "id": 343,
      "seek": 227572,
      "start": 2275.72,
      "end": 2280.12,
      "text": " di quella deserializzazione, ci creava un sacco di problemi, perché con la deserializzazione",
      "tokens": [
        1026,
        32234,
        730,
        260,
        831,
        8072,
        12928,
        11,
        6983,
        1197,
        4061,
        517,
        4899,
        1291,
        1026,
        1154,
        72,
        11,
        14303,
        416,
        635,
        730,
        260,
        831,
        8072,
        12928
      ],
      "temperature": 0,
      "avg_logprob": -0.30768281867705194,
      "compression_ratio": 1.6646341463414633,
      "no_speech_prob": 0.0000018738730886980193
    },
    {
      "id": 344,
      "seek": 227572,
      "start": 2280.12,
      "end": 2291.7599999999998,
      "text": " costava 50ms. Per uno non è un problema, ma quando tu questo JSON lo carichi, sono",
      "tokens": [
        2063,
        4061,
        2625,
        2592,
        13,
        3026,
        8526,
        2107,
        4873,
        517,
        12395,
        11,
        463,
        7770,
        2604,
        10263,
        31828,
        450,
        1032,
        18543,
        11,
        9259
      ],
      "temperature": 0,
      "avg_logprob": -0.30768281867705194,
      "compression_ratio": 1.6646341463414633,
      "no_speech_prob": 0.0000018738730886980193
    },
    {
      "id": 345,
      "seek": 227572,
      "start": 2291.7599999999998,
      "end": 2304.7599999999998,
      "text": " 50ms di processore, non sono i 50ms di await. La deserializzazione di JSON e la serializzazione",
      "tokens": [
        2625,
        2592,
        1026,
        1399,
        418,
        11,
        2107,
        9259,
        741,
        2625,
        2592,
        1026,
        19670,
        13,
        2369,
        730,
        260,
        831,
        8072,
        12928,
        1026,
        31828,
        308,
        635,
        17436,
        8072,
        12928
      ],
      "temperature": 0,
      "avg_logprob": -0.30768281867705194,
      "compression_ratio": 1.6646341463414633,
      "no_speech_prob": 0.0000018738730886980193
    },
    {
      "id": 346,
      "seek": 230476,
      "start": 2304.76,
      "end": 2312.7200000000003,
      "text": " di JSON è sempre facile e molto attento. Noi ci andiamo molto rapidi là sopra, però",
      "tokens": [
        1026,
        31828,
        4873,
        9553,
        23670,
        308,
        16394,
        951,
        15467,
        13,
        883,
        72,
        6983,
        293,
        7415,
        16394,
        7558,
        72,
        3684,
        370,
        43255,
        11,
        12673
      ],
      "temperature": 0,
      "avg_logprob": -0.3733512249189554,
      "compression_ratio": 1.5020576131687242,
      "no_speech_prob": 0.0000019947265172959305
    },
    {
      "id": 347,
      "seek": 230476,
      "start": 2312.7200000000003,
      "end": 2320.2400000000002,
      "text": " in azienda ne abbiamo viste di specie quando le performance contano. Sono questi poi i",
      "tokens": [
        294,
        7883,
        30498,
        408,
        22815,
        371,
        8375,
        1026,
        1608,
        414,
        7770,
        476,
        3389,
        660,
        3730,
        13,
        48344,
        29729,
        19260,
        741
      ],
      "temperature": 0,
      "avg_logprob": -0.3733512249189554,
      "compression_ratio": 1.5020576131687242,
      "no_speech_prob": 0.0000019947265172959305
    },
    {
      "id": 348,
      "seek": 230476,
      "start": 2320.2400000000002,
      "end": 2328.0400000000004,
      "text": " casi in cui veramente si va a fare le pulci su tutto. Però voglio ritornare sulla complessità,",
      "tokens": [
        22567,
        294,
        22929,
        50079,
        1511,
        2773,
        257,
        11994,
        476,
        8331,
        537,
        459,
        23048,
        13,
        20533,
        31273,
        19987,
        11289,
        1865,
        543,
        33625,
        1209,
        442,
        12445,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.3733512249189554,
      "compression_ratio": 1.5020576131687242,
      "no_speech_prob": 0.0000019947265172959305
    },
    {
      "id": 349,
      "seek": 230476,
      "start": 2328.0400000000004,
      "end": 2334,
      "text": " perché quello che hai detto è molto interessante, cioè il fatto di concentrarmi nel caso in",
      "tokens": [
        14303,
        22813,
        947,
        21822,
        41031,
        4873,
        16394,
        24372,
        11,
        41827,
        1930,
        23228,
        1026,
        5512,
        5352,
        3057,
        15373,
        9666,
        294
      ],
      "temperature": 0,
      "avg_logprob": -0.3733512249189554,
      "compression_ratio": 1.5020576131687242,
      "no_speech_prob": 0.0000019947265172959305
    },
    {
      "id": 350,
      "seek": 233400,
      "start": 2334,
      "end": 2340.36,
      "text": " cui la business logic è importante, sul micro dato all'interno di un albero di dati un",
      "tokens": [
        22929,
        635,
        1606,
        9952,
        4873,
        9416,
        11,
        17603,
        4532,
        46971,
        439,
        6,
        5106,
        1771,
        1026,
        517,
        419,
        46659,
        1026,
        1137,
        72,
        517
      ],
      "temperature": 0,
      "avg_logprob": -0.24911558628082275,
      "compression_ratio": 1.5227272727272727,
      "no_speech_prob": 2.9649046950908087e-7
    },
    {
      "id": 351,
      "seek": 233400,
      "start": 2340.36,
      "end": 2351.84,
      "text": " po' più complesso, è importantissimo perché ci permette, volendo, di ignorare il contesto.",
      "tokens": [
        714,
        6,
        10589,
        1209,
        5557,
        11,
        4873,
        1021,
        34966,
        14303,
        6983,
        4784,
        3007,
        11,
        1996,
        3999,
        11,
        1026,
        14698,
        543,
        1930,
        10287,
        78,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.24911558628082275,
      "compression_ratio": 1.5227272727272727,
      "no_speech_prob": 2.9649046950908087e-7
    },
    {
      "id": 352,
      "seek": 233400,
      "start": 2351.84,
      "end": 2357.32,
      "text": " Il contesto, come dici tu, al quale possiamo risalire facilmente, che ne so, risalendo",
      "tokens": [
        4416,
        10287,
        78,
        11,
        808,
        274,
        8787,
        2604,
        11,
        419,
        421,
        1220,
        44758,
        2253,
        304,
        621,
        10217,
        4082,
        11,
        947,
        408,
        370,
        11,
        2253,
        304,
        3999
      ],
      "temperature": 0,
      "avg_logprob": -0.24911558628082275,
      "compression_ratio": 1.5227272727272727,
      "no_speech_prob": 2.9649046950908087e-7
    },
    {
      "id": 353,
      "seek": 235732,
      "start": 2357.32,
      "end": 2364,
      "text": " il nodo superiore o andando ad accedere a un context della situazione, però nel contempo",
      "tokens": [
        1930,
        15224,
        78,
        1687,
        72,
        418,
        277,
        293,
        1806,
        614,
        696,
        1232,
        323,
        257,
        517,
        4319,
        11618,
        2054,
        12928,
        11,
        12673,
        15373,
        660,
        443,
        2259
      ],
      "temperature": 0,
      "avg_logprob": -0.23525737580798922,
      "compression_ratio": 1.654708520179372,
      "no_speech_prob": 7.614508490405569e-8
    },
    {
      "id": 354,
      "seek": 235732,
      "start": 2364,
      "end": 2371.8,
      "text": " io mi concentro su quell'elemento. E una delle rivoluzioni poi in realtà di GraphQL è stata",
      "tokens": [
        19785,
        2752,
        5512,
        340,
        459,
        631,
        285,
        6,
        68,
        3054,
        78,
        13,
        462,
        2002,
        16485,
        367,
        21356,
        3334,
        15273,
        19260,
        294,
        47512,
        1026,
        21884,
        13695,
        4873,
        49554
      ],
      "temperature": 0,
      "avg_logprob": -0.23525737580798922,
      "compression_ratio": 1.654708520179372,
      "no_speech_prob": 7.614508490405569e-8
    },
    {
      "id": 355,
      "seek": 235732,
      "start": 2371.8,
      "end": 2378.6400000000003,
      "text": " quella di spostare parte di questa complessità di business anche sul frontend, perché il",
      "tokens": [
        32234,
        1026,
        637,
        555,
        543,
        6975,
        1026,
        16540,
        1209,
        442,
        12445,
        1026,
        1606,
        11585,
        17603,
        1868,
        521,
        11,
        14303,
        1930
      ],
      "temperature": 0,
      "avg_logprob": -0.23525737580798922,
      "compression_ratio": 1.654708520179372,
      "no_speech_prob": 7.614508490405569e-8
    },
    {
      "id": 356,
      "seek": 235732,
      "start": 2378.6400000000003,
      "end": 2384.82,
      "text": " fatto di poter chiedere, quindi parte di questa business logic sul frontend, il fatto di poter",
      "tokens": [
        23228,
        1026,
        1847,
        260,
        417,
        1091,
        323,
        11,
        15727,
        6975,
        1026,
        16540,
        1606,
        9952,
        17603,
        1868,
        521,
        11,
        1930,
        23228,
        1026,
        1847,
        260
      ],
      "temperature": 0,
      "avg_logprob": -0.23525737580798922,
      "compression_ratio": 1.654708520179372,
      "no_speech_prob": 7.614508490405569e-8
    },
    {
      "id": 357,
      "seek": 238482,
      "start": 2384.82,
      "end": 2394.84,
      "text": " chiedere al database, al backend, a me servono questi dati, poi ci penso io, tu non ti preoccupare",
      "tokens": [
        417,
        1091,
        323,
        419,
        8149,
        11,
        419,
        38087,
        11,
        257,
        385,
        1658,
        8957,
        29729,
        1137,
        72,
        11,
        19260,
        6983,
        48005,
        19785,
        11,
        2604,
        2107,
        8757,
        44388,
        543
      ],
      "temperature": 0,
      "avg_logprob": -0.28184902100335985,
      "compression_ratio": 1.590643274853801,
      "no_speech_prob": 2.5907064937769064e-8
    },
    {
      "id": 358,
      "seek": 238482,
      "start": 2394.84,
      "end": 2399.44,
      "text": " che ci penso io client side che non ti costa niente a te, vada bravo, dammi i dati come",
      "tokens": [
        947,
        6983,
        48005,
        19785,
        6423,
        1252,
        947,
        2107,
        8757,
        2063,
        64,
        297,
        8413,
        257,
        535,
        11,
        371,
        1538,
        1548,
        3080,
        11,
        2422,
        3057,
        741,
        1137,
        72,
        808
      ],
      "temperature": 0,
      "avg_logprob": -0.28184902100335985,
      "compression_ratio": 1.590643274853801,
      "no_speech_prob": 2.5907064937769064e-8
    },
    {
      "id": 359,
      "seek": 238482,
      "start": 2399.44,
      "end": 2408.04,
      "text": " mi servono. In questo approccio quali sono secondo te i vantaggi più grandi e quanto",
      "tokens": [
        2752,
        1658,
        8957,
        13,
        682,
        10263,
        2075,
        66,
        8529,
        4101,
        72,
        9259,
        41601,
        535,
        741,
        371,
        394,
        46893,
        10589,
        45155,
        308,
        17820
      ],
      "temperature": 0,
      "avg_logprob": -0.28184902100335985,
      "compression_ratio": 1.590643274853801,
      "no_speech_prob": 2.5907064937769064e-8
    },
    {
      "id": 360,
      "seek": 240804,
      "start": 2408.04,
      "end": 2415.48,
      "text": " invece i calci nel culo che ne derivano da questa approccia? Parto dagli svantaggi e lo",
      "tokens": [
        36344,
        741,
        2104,
        537,
        15373,
        11021,
        78,
        947,
        408,
        10151,
        3730,
        1120,
        16540,
        2075,
        66,
        2755,
        30,
        4100,
        78,
        15460,
        2081,
        262,
        5219,
        46893,
        308,
        450
      ],
      "temperature": 0,
      "avg_logprob": -0.25338473362205305,
      "compression_ratio": 1.5367965367965368,
      "no_speech_prob": 0.0000017603198330107261
    },
    {
      "id": 361,
      "seek": 240804,
      "start": 2415.48,
      "end": 2421.8,
      "text": " svantaggio l'hai detto tu nella frase dalli a me che a te non costa niente, che è la",
      "tokens": [
        262,
        5219,
        30763,
        287,
        6,
        18230,
        41031,
        2604,
        23878,
        38406,
        274,
        336,
        72,
        257,
        385,
        947,
        257,
        535,
        2107,
        2063,
        64,
        297,
        8413,
        11,
        947,
        4873,
        635
      ],
      "temperature": 0,
      "avg_logprob": -0.25338473362205305,
      "compression_ratio": 1.5367965367965368,
      "no_speech_prob": 0.0000017603198330107261
    },
    {
      "id": 362,
      "seek": 240804,
      "start": 2421.8,
      "end": 2427.7599999999998,
      "text": " cosa peggiore in assoluto che uno possa pensare con un server GraphQL. Si tirano giù i server",
      "tokens": [
        10163,
        520,
        22771,
        418,
        294,
        1256,
        2308,
        78,
        947,
        8526,
        41564,
        6099,
        543,
        416,
        517,
        7154,
        21884,
        13695,
        13,
        4909,
        13807,
        3730,
        1735,
        5035,
        741,
        7154
      ],
      "temperature": 0,
      "avg_logprob": -0.25338473362205305,
      "compression_ratio": 1.5367965367965368,
      "no_speech_prob": 0.0000017603198330107261
    },
    {
      "id": 363,
      "seek": 240804,
      "start": 2427.7599999999998,
      "end": 2434.64,
      "text": " in questa maniera e non sto scherzando. E volevo portarti a parlare proprio di quello.",
      "tokens": [
        294,
        16540,
        587,
        10609,
        308,
        2107,
        22784,
        956,
        260,
        89,
        1806,
        13,
        462,
        49877,
        3080,
        2436,
        40155,
        257,
        13734,
        543,
        28203,
        1026,
        22813,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.25338473362205305,
      "compression_ratio": 1.5367965367965368,
      "no_speech_prob": 0.0000017603198330107261
    },
    {
      "id": 364,
      "seek": 243464,
      "start": 2434.64,
      "end": 2446.3199999999997,
      "text": " Torno al mitico talk presentato all'epoca da questa donna, non mi ricordo come si chiama,",
      "tokens": [
        7160,
        1771,
        419,
        2194,
        2789,
        751,
        1974,
        2513,
        439,
        6,
        595,
        24035,
        1120,
        16540,
        500,
        629,
        11,
        2107,
        2752,
        21040,
        23872,
        808,
        1511,
        13228,
        2404,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.2841513098739996,
      "compression_ratio": 1.5595238095238095,
      "no_speech_prob": 0.0000010029953045886941
    },
    {
      "id": 365,
      "seek": 243464,
      "start": 2446.3199999999997,
      "end": 2454.3199999999997,
      "text": " ma è uno degli altri ingegneri di Facebook dell'epoca, non so se lavori ancora per loro,",
      "tokens": [
        463,
        4873,
        8526,
        32079,
        33707,
        3957,
        1146,
        1193,
        72,
        1026,
        4384,
        19781,
        6,
        595,
        24035,
        11,
        2107,
        370,
        369,
        20923,
        7386,
        30656,
        680,
        28810,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.2841513098739996,
      "compression_ratio": 1.5595238095238095,
      "no_speech_prob": 0.0000010029953045886941
    },
    {
      "id": 366,
      "seek": 243464,
      "start": 2454.3199999999997,
      "end": 2459.96,
      "text": " comunque, e ricordo che presentava questa cosa meravigliosa in cui ognuno decideva",
      "tokens": [
        45736,
        11,
        308,
        21040,
        23872,
        947,
        1974,
        4061,
        16540,
        10163,
        3551,
        706,
        328,
        2081,
        6447,
        294,
        22929,
        277,
        4568,
        12638,
        979,
        482,
        2757
      ],
      "temperature": 0,
      "avg_logprob": -0.2841513098739996,
      "compression_ratio": 1.5595238095238095,
      "no_speech_prob": 0.0000010029953045886941
    },
    {
      "id": 367,
      "seek": 245996,
      "start": 2459.96,
      "end": 2467.6,
      "text": " che dati prendere e quindi tu in quella richiesta hai bisogno di nome, cognome e numero di telefono,",
      "tokens": [
        947,
        1137,
        72,
        9866,
        323,
        308,
        15727,
        2604,
        294,
        32234,
        4593,
        38804,
        21822,
        40505,
        1771,
        1026,
        19003,
        11,
        11786,
        423,
        308,
        46839,
        1026,
        40616,
        8957,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.25697812877717563,
      "compression_ratio": 1.792626728110599,
      "no_speech_prob": 8.186355557882052e-7
    },
    {
      "id": 368,
      "seek": 245996,
      "start": 2467.6,
      "end": 2472.28,
      "text": " nell'altra richiesta hai bisogno anche dell'indirizzo e quindi chiedi l'indirizzo e ti restituisce",
      "tokens": [
        44666,
        6,
        38865,
        4593,
        38804,
        21822,
        40505,
        1771,
        11585,
        19781,
        6,
        471,
        347,
        590,
        4765,
        308,
        15727,
        417,
        1091,
        72,
        287,
        6,
        471,
        347,
        590,
        4765,
        308,
        8757,
        1472,
        6380,
        49596
      ],
      "temperature": 0,
      "avg_logprob": -0.25697812877717563,
      "compression_ratio": 1.792626728110599,
      "no_speech_prob": 8.186355557882052e-7
    },
    {
      "id": 369,
      "seek": 245996,
      "start": 2472.28,
      "end": 2478.28,
      "text": " l'indirizzo. Questo sarebbe il modo perfetto, indipendentemente da cosa costa andare a prendere",
      "tokens": [
        287,
        6,
        471,
        347,
        590,
        4765,
        13,
        38167,
        38706,
        39042,
        1930,
        16664,
        13826,
        23778,
        11,
        1016,
        647,
        521,
        317,
        16288,
        1120,
        10163,
        2063,
        64,
        42742,
        257,
        9866,
        323
      ],
      "temperature": 0,
      "avg_logprob": -0.25697812877717563,
      "compression_ratio": 1.792626728110599,
      "no_speech_prob": 8.186355557882052e-7
    },
    {
      "id": 370,
      "seek": 245996,
      "start": 2478.28,
      "end": 2484.32,
      "text": " questi dati, che ne parliamo dopo. Nella pratica le cose non funzionano così. Nella pratica,",
      "tokens": [
        29729,
        1137,
        72,
        11,
        947,
        408,
        971,
        49926,
        35196,
        13,
        426,
        9885,
        28844,
        2262,
        476,
        30261,
        2107,
        49345,
        313,
        3730,
        23278,
        13,
        426,
        9885,
        28844,
        2262,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.25697812877717563,
      "compression_ratio": 1.792626728110599,
      "no_speech_prob": 8.186355557882052e-7
    },
    {
      "id": 371,
      "seek": 248432,
      "start": 2484.32,
      "end": 2495.6400000000003,
      "text": " ho visto abbastanza codice front-end per dirtelo, funziona che io ho la mia entità prodotto,",
      "tokens": [
        1106,
        17558,
        16903,
        525,
        20030,
        17656,
        573,
        1868,
        12,
        521,
        680,
        11483,
        10590,
        11,
        49345,
        21758,
        947,
        19785,
        1106,
        635,
        21290,
        948,
        12445,
        15792,
        18838,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.2938891819545201,
      "compression_ratio": 1.592920353982301,
      "no_speech_prob": 0.000003041583113372326
    },
    {
      "id": 372,
      "seek": 248432,
      "start": 2495.6400000000003,
      "end": 2500.88,
      "text": " di questa entità prodotto mi faccio un bel fragment che mi restituisce tutti gli attributi",
      "tokens": [
        1026,
        16540,
        948,
        12445,
        15792,
        18838,
        2752,
        1915,
        8529,
        517,
        989,
        26424,
        947,
        2752,
        1472,
        6380,
        49596,
        19822,
        17161,
        9080,
        29161
      ],
      "temperature": 0,
      "avg_logprob": -0.2938891819545201,
      "compression_ratio": 1.592920353982301,
      "no_speech_prob": 0.000003041583113372326
    },
    {
      "id": 373,
      "seek": 248432,
      "start": 2500.88,
      "end": 2507.88,
      "text": " del prodotto. Il fragment, per quelli che non lo sapessero, è una sorta di... è il",
      "tokens": [
        1103,
        15792,
        18838,
        13,
        4416,
        26424,
        11,
        680,
        631,
        16320,
        947,
        2107,
        450,
        18985,
        442,
        2032,
        11,
        4873,
        2002,
        33425,
        1026,
        485,
        4873,
        1930
      ],
      "temperature": 0,
      "avg_logprob": -0.2938891819545201,
      "compression_ratio": 1.592920353982301,
      "no_speech_prob": 0.000003041583113372326
    },
    {
      "id": 374,
      "seek": 248432,
      "start": 2507.88,
      "end": 2513.96,
      "text": " modo che ha GraphQL per definire un seme di attributi e utilizzarli in vari punti in modo",
      "tokens": [
        16664,
        947,
        324,
        21884,
        13695,
        680,
        1561,
        621,
        517,
        369,
        1398,
        1026,
        9080,
        29161,
        308,
        40355,
        289,
        2081,
        294,
        3034,
        18212,
        72,
        294,
        16664
      ],
      "temperature": 0,
      "avg_logprob": -0.2938891819545201,
      "compression_ratio": 1.592920353982301,
      "no_speech_prob": 0.000003041583113372326
    },
    {
      "id": 375,
      "seek": 251396,
      "start": 2513.96,
      "end": 2519.96,
      "text": " da non doverli riscrivere tutti. Possiamo dire un template per porzioni di query GraphQL",
      "tokens": [
        1120,
        2107,
        360,
        331,
        2081,
        2253,
        1142,
        5887,
        19822,
        13,
        33112,
        7415,
        1264,
        517,
        12379,
        680,
        1515,
        89,
        15273,
        1026,
        14581,
        21884,
        13695
      ],
      "temperature": 0,
      "avg_logprob": -0.3086390551398782,
      "compression_ratio": 1.46524064171123,
      "no_speech_prob": 1.3788094577193988e-7
    },
    {
      "id": 376,
      "seek": 251396,
      "start": 2519.96,
      "end": 2528.8,
      "text": " giusto per semplificarlo. Esatto, c'è una cosa di questo tipo e quindi lo sviluppatore",
      "tokens": [
        1735,
        48260,
        680,
        4361,
        564,
        1089,
        19457,
        13,
        2313,
        37491,
        11,
        269,
        6,
        1462,
        2002,
        10163,
        1026,
        10263,
        9746,
        308,
        15727,
        450,
        17342,
        388,
        10504,
        43148
      ],
      "temperature": 0,
      "avg_logprob": -0.3086390551398782,
      "compression_ratio": 1.46524064171123,
      "no_speech_prob": 1.3788094577193988e-7
    },
    {
      "id": 377,
      "seek": 251396,
      "start": 2528.8,
      "end": 2535.96,
      "text": " di turno una volta che si è fatto quel fragment, fregandosene di tutto, lo usa ovunque. E quindi",
      "tokens": [
        1026,
        1261,
        78,
        2002,
        18765,
        947,
        1511,
        4873,
        23228,
        7178,
        26424,
        11,
        2130,
        70,
        474,
        329,
        1450,
        1026,
        23048,
        11,
        450,
        29909,
        14187,
        409,
        1077,
        13,
        462,
        15727
      ],
      "temperature": 0,
      "avg_logprob": -0.3086390551398782,
      "compression_ratio": 1.46524064171123,
      "no_speech_prob": 1.3788094577193988e-7
    },
    {
      "id": 378,
      "seek": 253596,
      "start": 2535.96,
      "end": 2545.32,
      "text": " hai un fragment che in un punto ha dieci campi di cui ne servono sei, nell'altro ha sempre",
      "tokens": [
        21822,
        517,
        26424,
        947,
        294,
        517,
        14326,
        324,
        978,
        537,
        2255,
        72,
        1026,
        22929,
        408,
        1658,
        8957,
        10842,
        11,
        44666,
        6,
        47484,
        324,
        9553
      ],
      "temperature": 0,
      "avg_logprob": -0.30124805591724535,
      "compression_ratio": 1.6772727272727272,
      "no_speech_prob": 0.000002994414671775303
    },
    {
      "id": 379,
      "seek": 253596,
      "start": 2545.32,
      "end": 2550.88,
      "text": " dieci campi ma di cui ne servono cinque e solo uno di questi è comune perché gli servono",
      "tokens": [
        978,
        537,
        2255,
        72,
        463,
        1026,
        22929,
        408,
        1658,
        8957,
        6539,
        1077,
        308,
        6944,
        8526,
        1026,
        29729,
        4873,
        11040,
        68,
        14303,
        17161,
        1658,
        8957
      ],
      "temperature": 0,
      "avg_logprob": -0.30124805591724535,
      "compression_ratio": 1.6772727272727272,
      "no_speech_prob": 0.000002994414671775303
    },
    {
      "id": 380,
      "seek": 253596,
      "start": 2550.88,
      "end": 2557.16,
      "text": " quegli altri. E questa è una roba comunissima, le codebase sono piene di casi di questo tipo,",
      "tokens": [
        631,
        41443,
        33707,
        13,
        462,
        16540,
        4873,
        2002,
        3870,
        64,
        11040,
        891,
        4775,
        11,
        476,
        3089,
        17429,
        9259,
        280,
        10174,
        1026,
        22567,
        1026,
        10263,
        9746,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.30124805591724535,
      "compression_ratio": 1.6772727272727272,
      "no_speech_prob": 0.000002994414671775303
    },
    {
      "id": 381,
      "seek": 253596,
      "start": 2557.16,
      "end": 2563.28,
      "text": " che il 50% delle volte non danno problemi, a parte il traffico, non sto neanche considerando",
      "tokens": [
        947,
        1930,
        2625,
        4,
        16485,
        37801,
        2107,
        3594,
        78,
        1154,
        72,
        11,
        257,
        6975,
        1930,
        21073,
        2789,
        11,
        2107,
        22784,
        408,
        22806,
        1949,
        1806
      ],
      "temperature": 0,
      "avg_logprob": -0.30124805591724535,
      "compression_ratio": 1.6772727272727272,
      "no_speech_prob": 0.000002994414671775303
    },
    {
      "id": 382,
      "seek": 256328,
      "start": 2563.28,
      "end": 2570.0800000000004,
      "text": " il traffico, però il 50% delle volte non danno problemi. Però tu non lo sai perché",
      "tokens": [
        1930,
        21073,
        2789,
        11,
        12673,
        1930,
        2625,
        4,
        16485,
        37801,
        2107,
        3594,
        78,
        1154,
        72,
        13,
        20533,
        2604,
        2107,
        450,
        32417,
        14303
      ],
      "temperature": 0,
      "avg_logprob": -0.2523794907789964,
      "compression_ratio": 1.7131474103585658,
      "no_speech_prob": 3.307588372081227e-7
    },
    {
      "id": 383,
      "seek": 256328,
      "start": 2570.0800000000004,
      "end": 2575.6800000000003,
      "text": " quello che è un nome, cognome, indirizzo e l'indirizzo dici ma che cosa vuoi che costi",
      "tokens": [
        22813,
        947,
        4873,
        517,
        19003,
        11,
        11786,
        423,
        11,
        1016,
        347,
        590,
        4765,
        308,
        287,
        6,
        471,
        347,
        590,
        4765,
        274,
        8787,
        463,
        947,
        10163,
        9732,
        4869,
        947,
        2063,
        72
      ],
      "temperature": 0,
      "avg_logprob": -0.2523794907789964,
      "compression_ratio": 1.7131474103585658,
      "no_speech_prob": 3.307588372081227e-7
    },
    {
      "id": 384,
      "seek": 256328,
      "start": 2575.6800000000003,
      "end": 2581.2400000000002,
      "text": " fare l'indirizzo, tu non lo sai che cosa sta succedendo là dietro. Potrebbe essere",
      "tokens": [
        11994,
        287,
        6,
        471,
        347,
        590,
        4765,
        11,
        2604,
        2107,
        450,
        32417,
        947,
        10163,
        11135,
        1965,
        1232,
        3999,
        3684,
        6339,
        340,
        13,
        9145,
        39487,
        19799
      ],
      "temperature": 0,
      "avg_logprob": -0.2523794907789964,
      "compression_ratio": 1.7131474103585658,
      "no_speech_prob": 3.307588372081227e-7
    },
    {
      "id": 385,
      "seek": 256328,
      "start": 2581.2400000000002,
      "end": 2585.32,
      "text": " che là dietro c'è qualcuno che per prendere questo indirizzo deve chiedere un servizio",
      "tokens": [
        947,
        3684,
        6339,
        340,
        269,
        6,
        1462,
        32101,
        12638,
        947,
        680,
        9866,
        323,
        10263,
        1016,
        347,
        590,
        4765,
        17761,
        417,
        1091,
        323,
        517,
        1658,
        590,
        1004
      ],
      "temperature": 0,
      "avg_logprob": -0.2523794907789964,
      "compression_ratio": 1.7131474103585658,
      "no_speech_prob": 3.307588372081227e-7
    },
    {
      "id": 386,
      "seek": 256328,
      "start": 2585.32,
      "end": 2589.1600000000003,
      "text": " esterno perché semmai ha un ID di questo indirizzo. Adesso l'indirizzo ho fatto un",
      "tokens": [
        871,
        1248,
        78,
        14303,
        369,
        2174,
        1301,
        324,
        517,
        7348,
        1026,
        10263,
        1016,
        347,
        590,
        4765,
        13,
        1999,
        5557,
        287,
        6,
        471,
        347,
        590,
        4765,
        1106,
        23228,
        517
      ],
      "temperature": 0,
      "avg_logprob": -0.2523794907789964,
      "compression_ratio": 1.7131474103585658,
      "no_speech_prob": 3.307588372081227e-7
    },
    {
      "id": 387,
      "seek": 258916,
      "start": 2589.16,
      "end": 2594.64,
      "text": " caso un po' che è raro, di solito sta dove sta il nome e il cognome.",
      "tokens": [
        9666,
        517,
        714,
        6,
        947,
        4873,
        367,
        9708,
        11,
        1026,
        1404,
        3528,
        11135,
        23287,
        11135,
        1930,
        19003,
        308,
        1930,
        11786,
        423,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.45149495051457333,
      "compression_ratio": 1.4294117647058824,
      "no_speech_prob": 1.9142871110489068e-7
    },
    {
      "id": 388,
      "seek": 258916,
      "start": 2594.64,
      "end": 2603,
      "text": " Io ho visto i pi grafici che chiamavano Google per fare reverse geocoding di robe, quindi",
      "tokens": [
        19239,
        1106,
        17558,
        741,
        3895,
        1295,
        1786,
        72,
        947,
        417,
        2918,
        706,
        3730,
        3329,
        680,
        11994,
        9943,
        1519,
        905,
        8616,
        1026,
        37213,
        11,
        15727
      ],
      "temperature": 0,
      "avg_logprob": -0.45149495051457333,
      "compression_ratio": 1.4294117647058824,
      "no_speech_prob": 1.9142871110489068e-7
    },
    {
      "id": 389,
      "seek": 258916,
      "start": 2603,
      "end": 2613.16,
      "text": " ho visto fatture di Google esplodere. Sì, perché poi quando dai il potere in mano",
      "tokens": [
        1106,
        17558,
        283,
        1591,
        540,
        1026,
        3329,
        785,
        564,
        378,
        323,
        13,
        318,
        4749,
        11,
        14303,
        19260,
        7770,
        38586,
        1930,
        1847,
        323,
        294,
        18384
      ],
      "temperature": 0,
      "avg_logprob": -0.45149495051457333,
      "compression_ratio": 1.4294117647058824,
      "no_speech_prob": 1.9142871110489068e-7
    },
    {
      "id": 390,
      "seek": 261316,
      "start": 2613.16,
      "end": 2624.2799999999997,
      "text": " agli sviluppatori questi lo usano e spesso io, già la parola full stack che hai nella",
      "tokens": [
        623,
        2081,
        17342,
        388,
        10504,
        39842,
        29729,
        450,
        505,
        3730,
        308,
        637,
        5557,
        19785,
        11,
        30469,
        635,
        971,
        4711,
        1577,
        8630,
        947,
        21822,
        23878
      ],
      "temperature": 0,
      "avg_logprob": -0.32560193407666554,
      "compression_ratio": 1.6071428571428572,
      "no_speech_prob": 4.691161237246888e-8
    },
    {
      "id": 391,
      "seek": 261316,
      "start": 2624.2799999999997,
      "end": 2628.16,
      "text": " presentazione potrebbe scatenare giorni e giorni di discussione.",
      "tokens": [
        1974,
        12928,
        1847,
        39487,
        795,
        7186,
        543,
        36937,
        72,
        308,
        36937,
        72,
        1026,
        5017,
        68,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.32560193407666554,
      "compression_ratio": 1.6071428571428572,
      "no_speech_prob": 4.691161237246888e-8
    },
    {
      "id": 392,
      "seek": 261316,
      "start": 2628.16,
      "end": 2633.68,
      "text": " Ma sta là per quello amico mio. Secondo qualcuno i full stack non esistono,",
      "tokens": [
        4042,
        11135,
        3684,
        680,
        22813,
        669,
        2789,
        29908,
        13,
        5736,
        78,
        32101,
        12638,
        741,
        1577,
        8630,
        2107,
        785,
        468,
        8957,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.32560193407666554,
      "compression_ratio": 1.6071428571428572,
      "no_speech_prob": 4.691161237246888e-8
    },
    {
      "id": 393,
      "seek": 261316,
      "start": 2633.68,
      "end": 2636.8799999999997,
      "text": " secondo me è perché non li avete mai visti i full stack, perché io di full stack ne",
      "tokens": [
        41601,
        385,
        4873,
        14303,
        2107,
        375,
        48201,
        12698,
        40247,
        72,
        741,
        1577,
        8630,
        11,
        14303,
        19785,
        1026,
        1577,
        8630,
        408
      ],
      "temperature": 0,
      "avg_logprob": -0.32560193407666554,
      "compression_ratio": 1.6071428571428572,
      "no_speech_prob": 4.691161237246888e-8
    },
    {
      "id": 394,
      "seek": 263688,
      "start": 2636.88,
      "end": 2645.56,
      "text": " ho visti tanti e ho lavorato con tanti full stack. Non vuol dire che sei un fenomeno ovunque,",
      "tokens": [
        1106,
        40247,
        72,
        256,
        11520,
        308,
        1106,
        29241,
        2513,
        416,
        256,
        11520,
        1577,
        8630,
        13,
        8774,
        9732,
        401,
        1264,
        947,
        10842,
        517,
        26830,
        4726,
        78,
        14187,
        409,
        1077,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.32701114186069424,
      "compression_ratio": 1.697674418604651,
      "no_speech_prob": 2.1477697842442467e-8
    },
    {
      "id": 395,
      "seek": 263688,
      "start": 2645.56,
      "end": 2650.28,
      "text": " ma non sei un fenomeno neanche se sei un frontender, perché ci sarà sempre quella parte del frontend",
      "tokens": [
        463,
        2107,
        10842,
        517,
        26830,
        4726,
        78,
        408,
        22806,
        369,
        10842,
        517,
        1868,
        3216,
        11,
        14303,
        6983,
        41338,
        9553,
        32234,
        6975,
        1103,
        1868,
        521
      ],
      "temperature": 0,
      "avg_logprob": -0.32701114186069424,
      "compression_ratio": 1.697674418604651,
      "no_speech_prob": 2.1477697842442467e-8
    },
    {
      "id": 396,
      "seek": 263688,
      "start": 2650.28,
      "end": 2655.1600000000003,
      "text": " che conosci di meno e così vale anche per i full stack e vale per i back end. Io che",
      "tokens": [
        947,
        49892,
        537,
        1026,
        40236,
        308,
        23278,
        15474,
        11585,
        680,
        741,
        1577,
        8630,
        308,
        15474,
        680,
        741,
        646,
        917,
        13,
        19239,
        947
      ],
      "temperature": 0,
      "avg_logprob": -0.32701114186069424,
      "compression_ratio": 1.697674418604651,
      "no_speech_prob": 2.1477697842442467e-8
    },
    {
      "id": 397,
      "seek": 263688,
      "start": 2655.1600000000003,
      "end": 2658.96,
      "text": " ho avuto la fortuna negli anni di lavorare un po' a destra, un po' a sinistra, o a",
      "tokens": [
        1106,
        1305,
        8262,
        635,
        5009,
        5051,
        2485,
        2081,
        31164,
        1026,
        29241,
        543,
        517,
        714,
        6,
        257,
        2677,
        424,
        11,
        517,
        714,
        6,
        257,
        3343,
        468,
        424,
        11,
        277,
        257
      ],
      "temperature": 0,
      "avg_logprob": -0.32701114186069424,
      "compression_ratio": 1.697674418604651,
      "no_speech_prob": 2.1477697842442467e-8
    },
    {
      "id": 398,
      "seek": 265896,
      "start": 2658.96,
      "end": 2667.04,
      "text": " sopra e sotto, prendendo i front e i back, quando io lavoro a front end faccio molta",
      "tokens": [
        370,
        43255,
        308,
        43754,
        11,
        9866,
        3999,
        741,
        1868,
        308,
        741,
        646,
        11,
        7770,
        19785,
        42060,
        257,
        1868,
        917,
        1915,
        8529,
        48564
      ],
      "temperature": 0,
      "avg_logprob": -0.26943449704152234,
      "compression_ratio": 1.6729857819905214,
      "no_speech_prob": 2.6577100697977585e-7
    },
    {
      "id": 399,
      "seek": 265896,
      "start": 2667.04,
      "end": 2671.36,
      "text": " attenzione a che cosa vuol dire il dato dietro. Non li vado mai a prendere con leggerezza",
      "tokens": [
        951,
        11368,
        5328,
        257,
        947,
        10163,
        9732,
        401,
        1264,
        1930,
        46971,
        6339,
        340,
        13,
        8774,
        375,
        371,
        1573,
        12698,
        257,
        9866,
        323,
        416,
        30991,
        323,
        26786
      ],
      "temperature": 0,
      "avg_logprob": -0.26943449704152234,
      "compression_ratio": 1.6729857819905214,
      "no_speech_prob": 2.6577100697977585e-7
    },
    {
      "id": 400,
      "seek": 265896,
      "start": 2671.36,
      "end": 2679.92,
      "text": " questi dati. Purtroppo chi non ha esperienza dietro e poca esperienza in generale, perché",
      "tokens": [
        29729,
        1137,
        72,
        13,
        430,
        6224,
        340,
        27000,
        13228,
        2107,
        324,
        10045,
        42331,
        6339,
        340,
        308,
        714,
        496,
        10045,
        42331,
        294,
        1337,
        1220,
        11,
        14303
      ],
      "temperature": 0,
      "avg_logprob": -0.26943449704152234,
      "compression_ratio": 1.6729857819905214,
      "no_speech_prob": 2.6577100697977585e-7
    },
    {
      "id": 401,
      "seek": 265896,
      "start": 2679.92,
      "end": 2684.96,
      "text": " poi quando i front end di esperienza sanno benissimo il costo dei dati, anche se non si",
      "tokens": [
        19260,
        7770,
        741,
        1868,
        917,
        1026,
        10045,
        42331,
        262,
        13484,
        3271,
        34966,
        1930,
        2063,
        78,
        13874,
        1137,
        72,
        11,
        11585,
        369,
        2107,
        1511
      ],
      "temperature": 0,
      "avg_logprob": -0.26943449704152234,
      "compression_ratio": 1.6729857819905214,
      "no_speech_prob": 2.6577100697977585e-7
    },
    {
      "id": 402,
      "seek": 268496,
      "start": 2684.96,
      "end": 2691.12,
      "text": " occupano loro di andarli a prendere, spesso dicono ma si vado a prendere poi ci penserò",
      "tokens": [
        8073,
        3730,
        28810,
        1026,
        50009,
        2081,
        257,
        9866,
        323,
        11,
        637,
        5557,
        14285,
        8957,
        463,
        1511,
        371,
        1573,
        257,
        9866,
        323,
        19260,
        6983,
        38940,
        4293
      ],
      "temperature": 0,
      "avg_logprob": -0.3990086735905828,
      "compression_ratio": 1.6338028169014085,
      "no_speech_prob": 3.059017217310611e-7
    },
    {
      "id": 403,
      "seek": 268496,
      "start": 2691.12,
      "end": 2697.2,
      "text": " e queste ha dei costi trementi, serializzazione e serializzazione, richiesta di database.",
      "tokens": [
        308,
        35455,
        324,
        13874,
        2063,
        72,
        2192,
        518,
        72,
        11,
        17436,
        8072,
        12928,
        308,
        17436,
        8072,
        12928,
        11,
        4593,
        38804,
        1026,
        8149,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.3990086735905828,
      "compression_ratio": 1.6338028169014085,
      "no_speech_prob": 3.059017217310611e-7
    },
    {
      "id": 404,
      "seek": 268496,
      "start": 2697.2,
      "end": 2703.76,
      "text": " Se tu hai un DynamoDB dietro, che costa a richiesta, è ovvio, se tu hai un DynamoDB",
      "tokens": [
        1100,
        2604,
        21822,
        517,
        22947,
        78,
        27735,
        6339,
        340,
        11,
        947,
        2063,
        64,
        257,
        4593,
        38804,
        11,
        4873,
        14187,
        28226,
        11,
        369,
        2604,
        21822,
        517,
        22947,
        78,
        27735
      ],
      "temperature": 0,
      "avg_logprob": -0.3990086735905828,
      "compression_ratio": 1.6338028169014085,
      "no_speech_prob": 3.059017217310611e-7
    },
    {
      "id": 405,
      "seek": 268496,
      "start": 2703.76,
      "end": 2711.36,
      "text": " è molto cheap se tu fai poche richieste, ma se la tua query invece ne fa 5 e di qui",
      "tokens": [
        4873,
        16394,
        7084,
        369,
        2604,
        283,
        1301,
        714,
        1876,
        4593,
        6495,
        68,
        11,
        463,
        369,
        635,
        33578,
        14581,
        36344,
        408,
        2050,
        1025,
        308,
        1026,
        1956
      ],
      "temperature": 0,
      "avg_logprob": -0.3990086735905828,
      "compression_ratio": 1.6338028169014085,
      "no_speech_prob": 3.059017217310611e-7
    },
    {
      "id": 406,
      "seek": 271136,
      "start": 2711.36,
      "end": 2721.6800000000003,
      "text": " ne hai 500.000 all'ora, iniziano ad essere dei soldi. Avere 5 e avere 10 fa la differenza.",
      "tokens": [
        408,
        21822,
        5923,
        13,
        1360,
        439,
        6,
        3252,
        11,
        294,
        590,
        6254,
        614,
        19799,
        13874,
        3718,
        72,
        13,
        23650,
        265,
        1025,
        308,
        37914,
        1266,
        2050,
        635,
        743,
        23691,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.3924687589917864,
      "compression_ratio": 1.5150214592274678,
      "no_speech_prob": 4.691162303060992e-8
    },
    {
      "id": 407,
      "seek": 271136,
      "start": 2721.6800000000003,
      "end": 2728.56,
      "text": " Quindi ci sono delle attenzioni a cui bisogna stare attenti. Si sbaglia tutti, io per primo",
      "tokens": [
        32534,
        6983,
        9259,
        16485,
        951,
        11368,
        15273,
        257,
        22929,
        40505,
        629,
        22432,
        951,
        23012,
        13,
        4909,
        262,
        17282,
        14218,
        19822,
        11,
        19785,
        680,
        38671
      ],
      "temperature": 0,
      "avg_logprob": -0.3924687589917864,
      "compression_ratio": 1.5150214592274678,
      "no_speech_prob": 4.691162303060992e-8
    },
    {
      "id": 408,
      "seek": 271136,
      "start": 2728.56,
      "end": 2733.44,
      "text": " faccio del castronato, ogni tanto, che è veramente sbattere la testa contro il muro.",
      "tokens": [
        1915,
        8529,
        1103,
        4193,
        2044,
        2513,
        11,
        33189,
        10331,
        11,
        947,
        4873,
        50079,
        262,
        65,
        1591,
        323,
        635,
        1500,
        64,
        1583,
        1930,
        2992,
        340,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.3924687589917864,
      "compression_ratio": 1.5150214592274678,
      "no_speech_prob": 4.691162303060992e-8
    },
    {
      "id": 409,
      "seek": 271136,
      "start": 2733.44,
      "end": 2738.92,
      "text": " Però sul costo dei dati, il costo inteso come che cosa vuol dire andare a prendere,",
      "tokens": [
        20533,
        17603,
        2063,
        78,
        13874,
        1137,
        72,
        11,
        1930,
        2063,
        78,
        560,
        41189,
        808,
        947,
        10163,
        9732,
        401,
        1264,
        42742,
        257,
        9866,
        323,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.3924687589917864,
      "compression_ratio": 1.5150214592274678,
      "no_speech_prob": 4.691162303060992e-8
    },
    {
      "id": 410,
      "seek": 273892,
      "start": 2738.92,
      "end": 2749.12,
      "text": " GraphQL da questo punto di vista non lo nasconde. Dandoti la possibilità di fare picking ti",
      "tokens": [
        21884,
        13695,
        1120,
        10263,
        14326,
        1026,
        22553,
        2107,
        450,
        297,
        4806,
        7259,
        13,
        413,
        474,
        8206,
        635,
        24145,
        12445,
        1026,
        11994,
        8867,
        8757
      ],
      "temperature": 0,
      "avg_logprob": -0.3577984497610447,
      "compression_ratio": 1.5938864628820961,
      "no_speech_prob": 2.2380265818355838e-7
    },
    {
      "id": 411,
      "seek": 273892,
      "start": 2749.12,
      "end": 2754.12,
      "text": " dà spesso il diritto di prendere. Se tu chiami una REST API, ti frega, tu chiami una REST",
      "tokens": [
        274,
        1467,
        637,
        5557,
        1930,
        4746,
        34924,
        1026,
        9866,
        323,
        13,
        1100,
        2604,
        417,
        15568,
        2002,
        497,
        14497,
        9362,
        11,
        8757,
        2130,
        3680,
        11,
        2604,
        417,
        15568,
        2002,
        497,
        14497
      ],
      "temperature": 0,
      "avg_logprob": -0.3577984497610447,
      "compression_ratio": 1.5938864628820961,
      "no_speech_prob": 2.2380265818355838e-7
    },
    {
      "id": 412,
      "seek": 273892,
      "start": 2754.12,
      "end": 2758.48,
      "text": " API, sai già che tanto i dati verranno ristituiti tutti. Questo è un altro dei grandi vantaggi",
      "tokens": [
        9362,
        11,
        32417,
        30469,
        947,
        10331,
        741,
        1137,
        72,
        45923,
        13484,
        2253,
        27689,
        1983,
        72,
        19822,
        13,
        38167,
        4873,
        517,
        40924,
        13874,
        45155,
        371,
        394,
        46893
      ],
      "temperature": 0,
      "avg_logprob": -0.3577984497610447,
      "compression_ratio": 1.5938864628820961,
      "no_speech_prob": 2.2380265818355838e-7
    },
    {
      "id": 413,
      "seek": 273892,
      "start": 2758.48,
      "end": 2764.36,
      "text": " di GraphQL, che spesso ci sono nelle REST API. Quante volte hai usato delle REST API",
      "tokens": [
        1026,
        21884,
        13695,
        11,
        947,
        637,
        5557,
        6983,
        9259,
        46350,
        497,
        14497,
        9362,
        13,
        2326,
        2879,
        37801,
        21822,
        505,
        2513,
        16485,
        497,
        14497,
        9362
      ],
      "temperature": 0,
      "avg_logprob": -0.3577984497610447,
      "compression_ratio": 1.5938864628820961,
      "no_speech_prob": 2.2380265818355838e-7
    },
    {
      "id": 414,
      "seek": 276436,
      "start": 2764.36,
      "end": 2773.08,
      "text": " per il parametro full response o qualcosa? Si, o complete. Allora usa GraphQL. Nel momento",
      "tokens": [
        680,
        1930,
        6220,
        302,
        340,
        1577,
        4134,
        277,
        42400,
        30,
        4909,
        11,
        277,
        3566,
        13,
        1057,
        3252,
        29909,
        21884,
        13695,
        13,
        426,
        338,
        9333
      ],
      "temperature": 0,
      "avg_logprob": -0.29157122126165425,
      "compression_ratio": 1.548936170212766,
      "no_speech_prob": 9.570825341143063e-7
    },
    {
      "id": 415,
      "seek": 276436,
      "start": 2773.08,
      "end": 2778.96,
      "text": " in cui devi dire che tipologia di dati vuoi dietro, usa uno strumento che è fatto per",
      "tokens": [
        294,
        22929,
        31219,
        1264,
        947,
        4125,
        24103,
        1026,
        1137,
        72,
        9732,
        4869,
        6339,
        340,
        11,
        29909,
        8526,
        1056,
        2206,
        78,
        947,
        4873,
        23228,
        680
      ],
      "temperature": 0,
      "avg_logprob": -0.29157122126165425,
      "compression_ratio": 1.548936170212766,
      "no_speech_prob": 9.570825341143063e-7
    },
    {
      "id": 416,
      "seek": 276436,
      "start": 2778.96,
      "end": 2785.84,
      "text": " queste cose. Si, assolutamente. Domanda invece, perché poi tra un po' voglio arrivare a un",
      "tokens": [
        35455,
        30261,
        13,
        4909,
        11,
        1256,
        2308,
        3439,
        13,
        16674,
        5575,
        36344,
        11,
        14303,
        19260,
        944,
        517,
        714,
        6,
        31273,
        19987,
        30697,
        543,
        257,
        517
      ],
      "temperature": 0,
      "avg_logprob": -0.29157122126165425,
      "compression_ratio": 1.548936170212766,
      "no_speech_prob": 9.570825341143063e-7
    },
    {
      "id": 417,
      "seek": 276436,
      "start": 2785.84,
      "end": 2791.4,
      "text": " punto saliente sul quale hai lavorato pesantemente e voglio trattare perché è una delle cose",
      "tokens": [
        14326,
        1845,
        8413,
        17603,
        421,
        1220,
        21822,
        29241,
        2513,
        9262,
        394,
        16288,
        308,
        31273,
        19987,
        504,
        1591,
        543,
        14303,
        4873,
        2002,
        16485,
        30261
      ],
      "temperature": 0,
      "avg_logprob": -0.29157122126165425,
      "compression_ratio": 1.548936170212766,
      "no_speech_prob": 9.570825341143063e-7
    },
    {
      "id": 418,
      "seek": 279140,
      "start": 2791.4,
      "end": 2799.32,
      "text": " che di GraphQL si conosce poco, è male e invece secondo me ha senso approfondire. Però",
      "tokens": [
        947,
        1026,
        21884,
        13695,
        1511,
        49892,
        384,
        10639,
        11,
        4873,
        7133,
        308,
        36344,
        41601,
        385,
        324,
        3151,
        539,
        2075,
        69,
        684,
        621,
        13,
        20533
      ],
      "temperature": 0,
      "avg_logprob": -0.2790960912351255,
      "compression_ratio": 1.2253521126760563,
      "no_speech_prob": 6.27669578534551e-7
    },
    {
      "id": 419,
      "seek": 279140,
      "start": 2799.32,
      "end": 2820.04,
      "text": " prima voglio farti una domanda. Apollo. Come vedi la posizione Apollo nell'ecosistema",
      "tokens": [
        19507,
        31273,
        19987,
        24575,
        72,
        2002,
        3285,
        5575,
        13,
        25187,
        13,
        2492,
        371,
        10323,
        635,
        1366,
        35740,
        25187,
        44666,
        6,
        3045,
        329,
        468,
        5619
      ],
      "temperature": 0,
      "avg_logprob": -0.2790960912351255,
      "compression_ratio": 1.2253521126760563,
      "no_speech_prob": 6.27669578534551e-7
    },
    {
      "id": 420,
      "seek": 282004,
      "start": 2820.04,
      "end": 2834.84,
      "text": " GraphQL e soprattutto a questo punto in che modo Mercurius ribalta i giochi? Allora, partiamo",
      "tokens": [
        21884,
        13695,
        308,
        50002,
        257,
        10263,
        14326,
        294,
        947,
        16664,
        18897,
        374,
        4872,
        9162,
        37896,
        741,
        48508,
        8036,
        30,
        1057,
        3252,
        11,
        644,
        7415
      ],
      "temperature": 0,
      "avg_logprob": -0.2522165620481813,
      "compression_ratio": 1.5113636363636365,
      "no_speech_prob": 2.5759533173186355e-7
    },
    {
      "id": 421,
      "seek": 282004,
      "start": 2834.84,
      "end": 2840.72,
      "text": " da Apollo. Partiamo dal fatto che è un po' che non uso Apollo, però leggendo un po'",
      "tokens": [
        1120,
        25187,
        13,
        4100,
        7415,
        11702,
        23228,
        947,
        4873,
        517,
        714,
        6,
        947,
        2107,
        22728,
        25187,
        11,
        12673,
        30991,
        3999,
        517,
        714,
        6
      ],
      "temperature": 0,
      "avg_logprob": -0.2522165620481813,
      "compression_ratio": 1.5113636363636365,
      "no_speech_prob": 2.5759533173186355e-7
    },
    {
      "id": 422,
      "seek": 282004,
      "start": 2840.72,
      "end": 2848.08,
      "text": " in giro e soprattutto essendo dalla parte di Mercurius sicuramente c'è una differenza",
      "tokens": [
        294,
        1735,
        340,
        308,
        50002,
        2097,
        3999,
        35566,
        6975,
        1026,
        18897,
        374,
        4872,
        33579,
        374,
        3439,
        269,
        6,
        1462,
        2002,
        743,
        23691
      ],
      "temperature": 0,
      "avg_logprob": -0.2522165620481813,
      "compression_ratio": 1.5113636363636365,
      "no_speech_prob": 2.5759533173186355e-7
    },
    {
      "id": 423,
      "seek": 284808,
      "start": 2848.08,
      "end": 2856.68,
      "text": " di performance. È un sistema più pesante per come è fatto, per come... non le so le",
      "tokens": [
        1026,
        3389,
        13,
        34495,
        517,
        13245,
        10589,
        9262,
        2879,
        680,
        808,
        4873,
        23228,
        11,
        680,
        808,
        485,
        2107,
        476,
        370,
        476
      ],
      "temperature": 0,
      "avg_logprob": -0.33282573803051096,
      "compression_ratio": 1.5224719101123596,
      "no_speech_prob": 0.0000014823408491793089
    },
    {
      "id": 424,
      "seek": 284808,
      "start": 2856.68,
      "end": 2862.04,
      "text": " motivazioni esatte. Conosco molto bene Mercurius, so che è fatto da un maniaco delle performance",
      "tokens": [
        5426,
        27569,
        785,
        30466,
        13,
        2656,
        329,
        1291,
        16394,
        2537,
        18897,
        374,
        4872,
        11,
        370,
        947,
        4873,
        23228,
        1120,
        517,
        587,
        72,
        11428,
        16485,
        3389
      ],
      "temperature": 0,
      "avg_logprob": -0.33282573803051096,
      "compression_ratio": 1.5224719101123596,
      "no_speech_prob": 0.0000014823408491793089
    },
    {
      "id": 425,
      "seek": 284808,
      "start": 2862.04,
      "end": 2870.48,
      "text": " e quindi a questo punto di vista funziona molto bene. Apollo è il leader del mercato.",
      "tokens": [
        308,
        15727,
        257,
        10263,
        14326,
        1026,
        22553,
        49345,
        21758,
        16394,
        2537,
        13,
        25187,
        4873,
        1930,
        5263,
        1103,
        10811,
        2513,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.33282573803051096,
      "compression_ratio": 1.5224719101123596,
      "no_speech_prob": 0.0000014823408491793089
    },
    {
      "id": 426,
      "seek": 287048,
      "start": 2870.48,
      "end": 2879.04,
      "text": " Non giriamoci attorno, è nel mondo JavaScript e GraphQL lo fa in JavaScript perché è fatto",
      "tokens": [
        8774,
        290,
        12988,
        10502,
        537,
        951,
        21998,
        11,
        4873,
        15373,
        40499,
        15778,
        308,
        21884,
        13695,
        450,
        2050,
        294,
        15778,
        14303,
        4873,
        23228
      ],
      "temperature": 0,
      "avg_logprob": -0.3385180904440684,
      "compression_ratio": 1.563953488372093,
      "no_speech_prob": 0.000007527872185164597
    },
    {
      "id": 427,
      "seek": 287048,
      "start": 2879.04,
      "end": 2890.32,
      "text": " per essere fatto in JavaScript. Io se devo pensare di fare un GraphQL con un C Sharp",
      "tokens": [
        680,
        19799,
        23228,
        294,
        15778,
        13,
        19239,
        369,
        49717,
        6099,
        543,
        1026,
        11994,
        517,
        21884,
        13695,
        416,
        517,
        383,
        31654
      ],
      "temperature": 0,
      "avg_logprob": -0.3385180904440684,
      "compression_ratio": 1.563953488372093,
      "no_speech_prob": 0.000007527872185164597
    },
    {
      "id": 428,
      "seek": 287048,
      "start": 2890.32,
      "end": 2896.44,
      "text": " o con un Java, con tutto quello che comporta la tipizzazione, non è tanto la tipizzazione",
      "tokens": [
        277,
        416,
        517,
        10745,
        11,
        416,
        23048,
        22813,
        947,
        25883,
        64,
        635,
        4125,
        8072,
        12928,
        11,
        2107,
        4873,
        10331,
        635,
        4125,
        8072,
        12928
      ],
      "temperature": 0,
      "avg_logprob": -0.3385180904440684,
      "compression_ratio": 1.563953488372093,
      "no_speech_prob": 0.000007527872185164597
    },
    {
      "id": 429,
      "seek": 289644,
      "start": 2896.44,
      "end": 2902.04,
      "text": " perché TypeScript è tipizzato, però ti dà delle libertà TypeScript che anche se",
      "tokens": [
        14303,
        15576,
        14237,
        4873,
        4125,
        8072,
        2513,
        11,
        12673,
        8757,
        274,
        1467,
        16485,
        18058,
        1467,
        15576,
        14237,
        947,
        11585,
        369
      ],
      "temperature": 0,
      "avg_logprob": -0.27732242682041264,
      "compression_ratio": 1.4488636363636365,
      "no_speech_prob": 4.1811486539700127e-7
    },
    {
      "id": 430,
      "seek": 289644,
      "start": 2902.04,
      "end": 2917.08,
      "text": " è tipizzato tu riesci a essere molto più veloce a fare code. C Sharp, dico C Sharp",
      "tokens": [
        4873,
        4125,
        8072,
        2513,
        2604,
        23932,
        537,
        257,
        19799,
        16394,
        10589,
        1241,
        752,
        384,
        257,
        11994,
        598,
        1479,
        13,
        383,
        31654,
        11,
        274,
        2789,
        383,
        31654
      ],
      "temperature": 0,
      "avg_logprob": -0.27732242682041264,
      "compression_ratio": 1.4488636363636365,
      "no_speech_prob": 4.1811486539700127e-7
    },
    {
      "id": 431,
      "seek": 289644,
      "start": 2917.08,
      "end": 2924.68,
      "text": " perché durante le vacanze di Natale, nelle mie due settimane di vacanze, ho lavorato",
      "tokens": [
        14303,
        14427,
        476,
        2842,
        282,
        1381,
        1026,
        6821,
        1220,
        11,
        46350,
        12597,
        3462,
        5584,
        332,
        1929,
        1026,
        2842,
        282,
        1381,
        11,
        1106,
        29241,
        2513
      ],
      "temperature": 0,
      "avg_logprob": -0.27732242682041264,
      "compression_ratio": 1.4488636363636365,
      "no_speech_prob": 4.1811486539700127e-7
    },
    {
      "id": 432,
      "seek": 292468,
      "start": 2924.68,
      "end": 2932,
      "text": " per Unity a full time. Sto facendo un giochino per i fatti miei perché mi rilassa e quindi",
      "tokens": [
        680,
        27913,
        257,
        1577,
        565,
        13,
        745,
        78,
        1915,
        3999,
        517,
        48508,
        339,
        2982,
        680,
        741,
        283,
        21515,
        12597,
        72,
        14303,
        2752,
        367,
        388,
        18948,
        308,
        15727
      ],
      "temperature": 0,
      "avg_logprob": -0.3350497191807009,
      "compression_ratio": 1.403225806451613,
      "no_speech_prob": 0.0000018738357994152466
    },
    {
      "id": 433,
      "seek": 292468,
      "start": 2932,
      "end": 2939.12,
      "text": " ho dovuto lavorare molto in C Sharp che non conosco. L'ho programmato all'inizio degli",
      "tokens": [
        1106,
        30870,
        8262,
        29241,
        543,
        16394,
        294,
        383,
        31654,
        947,
        2107,
        49892,
        1291,
        13,
        441,
        6,
        1289,
        1461,
        76,
        2513,
        439,
        6,
        9328,
        1004,
        32079
      ],
      "temperature": 0,
      "avg_logprob": -0.3350497191807009,
      "compression_ratio": 1.403225806451613,
      "no_speech_prob": 0.0000018738357994152466
    },
    {
      "id": 434,
      "seek": 292468,
      "start": 2939.12,
      "end": 2945.48,
      "text": " anni 2000, ma ovviamente adesso ho un altro linguaggio. Quindi queste complessità nel",
      "tokens": [
        31164,
        8132,
        11,
        463,
        14187,
        23347,
        39552,
        1106,
        517,
        40924,
        21766,
        30763,
        13,
        32534,
        35455,
        1209,
        442,
        12445,
        15373
      ],
      "temperature": 0,
      "avg_logprob": -0.3350497191807009,
      "compression_ratio": 1.403225806451613,
      "no_speech_prob": 0.0000018738357994152466
    },
    {
      "id": 435,
      "seek": 292468,
      "start": 2945.48,
      "end": 2953.3599999999997,
      "text": " fare le cose più banali, nel caricare un JSON, capisci? Io il JSON lo carico e lo",
      "tokens": [
        11994,
        476,
        30261,
        10589,
        5643,
        5103,
        11,
        15373,
        45732,
        543,
        517,
        31828,
        11,
        1410,
        271,
        537,
        30,
        19239,
        1930,
        31828,
        450,
        1032,
        2789,
        308,
        450
      ],
      "temperature": 0,
      "avg_logprob": -0.3350497191807009,
      "compression_ratio": 1.403225806451613,
      "no_speech_prob": 0.0000018738357994152466
    },
    {
      "id": 436,
      "seek": 295336,
      "start": 2953.36,
      "end": 2960.6400000000003,
      "text": " uso in JavaScript e in TypeScript. In C Sharp devo fare un serviziatore, una serie di robe",
      "tokens": [
        22728,
        294,
        15778,
        308,
        294,
        15576,
        14237,
        13,
        682,
        383,
        31654,
        49717,
        11994,
        517,
        1658,
        590,
        7676,
        418,
        11,
        2002,
        23030,
        1026,
        37213
      ],
      "temperature": 0,
      "avg_logprob": -0.3499507359095982,
      "compression_ratio": 1.521186440677966,
      "no_speech_prob": 7.811467526153137e-7
    },
    {
      "id": 437,
      "seek": 295336,
      "start": 2960.6400000000003,
      "end": 2965.32,
      "text": " che vanno fatte e poi verrò smentito da gente che C Sharp lo conosce, perché io c'è",
      "tokens": [
        947,
        371,
        13484,
        4046,
        975,
        308,
        19260,
        45923,
        4293,
        262,
        518,
        3528,
        1120,
        3788,
        947,
        383,
        31654,
        450,
        49892,
        384,
        11,
        14303,
        19785,
        269,
        6,
        1462
      ],
      "temperature": 0,
      "avg_logprob": -0.3499507359095982,
      "compression_ratio": 1.521186440677966,
      "no_speech_prob": 7.811467526153137e-7
    },
    {
      "id": 438,
      "seek": 295336,
      "start": 2965.32,
      "end": 2971.4,
      "text": " quel fatto, non lo conosco. Però così a naso mi viene da dire che GraphQL è nato",
      "tokens": [
        7178,
        23228,
        11,
        2107,
        450,
        49892,
        1291,
        13,
        20533,
        23278,
        257,
        5382,
        78,
        2752,
        19561,
        1120,
        1264,
        947,
        21884,
        13695,
        4873,
        2249,
        78
      ],
      "temperature": 0,
      "avg_logprob": -0.3499507359095982,
      "compression_ratio": 1.521186440677966,
      "no_speech_prob": 7.811467526153137e-7
    },
    {
      "id": 439,
      "seek": 295336,
      "start": 2971.4,
      "end": 2978.92,
      "text": " per essere fatto dal punto di vista di comodità in JavaScript. Anche perché il 90% è consumato",
      "tokens": [
        680,
        19799,
        23228,
        11702,
        14326,
        1026,
        22553,
        1026,
        395,
        378,
        12445,
        294,
        15778,
        13,
        1107,
        1876,
        14303,
        1930,
        4289,
        4,
        4873,
        3978,
        2513
      ],
      "temperature": 0,
      "avg_logprob": -0.3499507359095982,
      "compression_ratio": 1.521186440677966,
      "no_speech_prob": 7.811467526153137e-7
    },
    {
      "id": 440,
      "seek": 297892,
      "start": 2978.92,
      "end": 2986.64,
      "text": " da JavaScript. Esatto, in Rust è sicuramente più performante. Però detto questo, mi sono",
      "tokens": [
        1120,
        15778,
        13,
        2313,
        37491,
        11,
        294,
        34952,
        4873,
        33579,
        374,
        3439,
        10589,
        2042,
        2879,
        13,
        20533,
        41031,
        10263,
        11,
        2752,
        9259
      ],
      "temperature": 0,
      "avg_logprob": -0.3976873779296875,
      "compression_ratio": 1.4845814977973568,
      "no_speech_prob": 0.0000030894598239683546
    },
    {
      "id": 441,
      "seek": 297892,
      "start": 2986.64,
      "end": 2992.4,
      "text": " perso, dovrebbe che non mi ricordo più cosa stavo dicendo. No, la posizione di Apollo.",
      "tokens": [
        868,
        78,
        11,
        30870,
        39487,
        947,
        2107,
        2752,
        21040,
        23872,
        10589,
        10163,
        342,
        25713,
        14285,
        3999,
        13,
        883,
        11,
        635,
        1366,
        35740,
        1026,
        25187,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.3976873779296875,
      "compression_ratio": 1.4845814977973568,
      "no_speech_prob": 0.0000030894598239683546
    },
    {
      "id": 442,
      "seek": 297892,
      "start": 2992.4,
      "end": 2999.28,
      "text": " Nel mondo JavaScript Apollo è dominante, è utile che ci geriamo attorno. Se tu vai",
      "tokens": [
        426,
        338,
        40499,
        15778,
        25187,
        4873,
        8859,
        2879,
        11,
        4873,
        2839,
        794,
        947,
        6983,
        290,
        16310,
        10502,
        951,
        21998,
        13,
        1100,
        2604,
        4405
      ],
      "temperature": 0,
      "avg_logprob": -0.3976873779296875,
      "compression_ratio": 1.4845814977973568,
      "no_speech_prob": 0.0000030894598239683546
    },
    {
      "id": 443,
      "seek": 297892,
      "start": 2999.28,
      "end": 3008.76,
      "text": " sui trend di download, si vede la differenza. Sono due prodotti diversi.",
      "tokens": [
        459,
        72,
        6028,
        1026,
        5484,
        11,
        1511,
        371,
        4858,
        635,
        743,
        23691,
        13,
        48344,
        3462,
        15792,
        37514,
        6111,
        72,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.3976873779296875,
      "compression_ratio": 1.4845814977973568,
      "no_speech_prob": 0.0000030894598239683546
    },
    {
      "id": 444,
      "seek": 300876,
      "start": 3008.76,
      "end": 3015.44,
      "text": " Diversi dal punto di vista dell'utilizzo. Poi Apollo è amato ovviamente da chi entra",
      "tokens": [
        413,
        1762,
        72,
        11702,
        14326,
        1026,
        22553,
        19781,
        6,
        20835,
        590,
        4765,
        13,
        430,
        4869,
        25187,
        4873,
        669,
        2513,
        14187,
        23347,
        1120,
        13228,
        22284
      ],
      "temperature": 0,
      "avg_logprob": -0.3442913304979556,
      "compression_ratio": 1.4936170212765958,
      "no_speech_prob": 1.798302804445484e-7
    },
    {
      "id": 445,
      "seek": 300876,
      "start": 3015.44,
      "end": 3021.44,
      "text": " nel mondo Node, perché se fai una ricerca ti viene Apollo e il mondo Node. Anche perché",
      "tokens": [
        15373,
        40499,
        38640,
        11,
        14303,
        369,
        283,
        1301,
        2002,
        21040,
        36127,
        8757,
        19561,
        25187,
        308,
        1930,
        40499,
        38640,
        13,
        1107,
        1876,
        14303
      ],
      "temperature": 0,
      "avg_logprob": -0.3442913304979556,
      "compression_ratio": 1.4936170212765958,
      "no_speech_prob": 1.798302804445484e-7
    },
    {
      "id": 446,
      "seek": 300876,
      "start": 3021.44,
      "end": 3028.6000000000004,
      "text": " è un prodotto commerciale supportato da... Tutta una serie di cose. Però, adesso butto",
      "tokens": [
        4873,
        517,
        15792,
        18838,
        6841,
        68,
        1406,
        2513,
        1120,
        485,
        18392,
        1328,
        2002,
        23030,
        1026,
        30261,
        13,
        20533,
        11,
        39552,
        457,
        1353
      ],
      "temperature": 0,
      "avg_logprob": -0.3442913304979556,
      "compression_ratio": 1.4936170212765958,
      "no_speech_prob": 1.798302804445484e-7
    },
    {
      "id": 447,
      "seek": 300876,
      "start": 3028.6000000000004,
      "end": 3037.6800000000003,
      "text": " l'acqua un po' al mio di mulino. Se dovessimo usare un server, a questo punto useremmo",
      "tokens": [
        287,
        6,
        326,
        34787,
        517,
        714,
        6,
        419,
        29908,
        1026,
        14077,
        2982,
        13,
        1100,
        30870,
        442,
        6934,
        505,
        543,
        517,
        7154,
        11,
        257,
        10263,
        14326,
        505,
        323,
        2174,
        78
      ],
      "temperature": 0,
      "avg_logprob": -0.3442913304979556,
      "compression_ratio": 1.4936170212765958,
      "no_speech_prob": 1.798302804445484e-7
    },
    {
      "id": 448,
      "seek": 303768,
      "start": 3037.68,
      "end": 3044.8799999999997,
      "text": " Express se ragionassimo con questo sistema. La differenza è che Apollo ha un'azienda,",
      "tokens": [
        20212,
        369,
        17539,
        313,
        640,
        6934,
        416,
        10263,
        13245,
        13,
        2369,
        743,
        23691,
        4873,
        947,
        25187,
        324,
        517,
        6,
        921,
        30498,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.37284665788922994,
      "compression_ratio": 1.5560538116591929,
      "no_speech_prob": 0.000001051144749908417
    },
    {
      "id": 449,
      "seek": 303768,
      "start": 3044.8799999999997,
      "end": 3050.6,
      "text": " dietro Express no. Per quanto sia proprietario dell'IBM, fra una serie e tutta una serie,",
      "tokens": [
        6339,
        340,
        20212,
        572,
        13,
        3026,
        17820,
        25176,
        27881,
        4912,
        19781,
        6,
        40,
        18345,
        11,
        6600,
        2002,
        23030,
        308,
        3672,
        1328,
        2002,
        23030,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.37284665788922994,
      "compression_ratio": 1.5560538116591929,
      "no_speech_prob": 0.000001051144749908417
    },
    {
      "id": 450,
      "seek": 303768,
      "start": 3050.6,
      "end": 3057.8799999999997,
      "text": " che l'hanno vinto a carte giocando non mi ricordo qual è il motivo. Adesso Express",
      "tokens": [
        947,
        287,
        6,
        71,
        13484,
        371,
        17246,
        257,
        31483,
        1735,
        905,
        1806,
        2107,
        2752,
        21040,
        23872,
        4101,
        4873,
        1930,
        35804,
        13,
        1999,
        5557,
        20212
      ],
      "temperature": 0,
      "avg_logprob": -0.37284665788922994,
      "compression_ratio": 1.5560538116591929,
      "no_speech_prob": 0.000001051144749908417
    },
    {
      "id": 451,
      "seek": 303768,
      "start": 3057.8799999999997,
      "end": 3063.2799999999997,
      "text": " dovrebbe essere sotto l'ala dell'IBM. Non so proprietà, non proprietà, non so. Però",
      "tokens": [
        30870,
        39487,
        19799,
        43754,
        287,
        6,
        5159,
        19781,
        6,
        40,
        18345,
        13,
        8774,
        370,
        27881,
        1467,
        11,
        2107,
        27881,
        1467,
        11,
        2107,
        370,
        13,
        20533
      ],
      "temperature": 0,
      "avg_logprob": -0.37284665788922994,
      "compression_ratio": 1.5560538116591929,
      "no_speech_prob": 0.000001051144749908417
    },
    {
      "id": 452,
      "seek": 306328,
      "start": 3063.28,
      "end": 3068.0800000000004,
      "text": " a un certo punto mi sembra che fosse sotto un prodotto che l'IBM ha comprato. Comunque,",
      "tokens": [
        257,
        517,
        22261,
        14326,
        2752,
        20775,
        424,
        947,
        24528,
        43754,
        517,
        15792,
        18838,
        947,
        287,
        6,
        40,
        18345,
        324,
        16802,
        2513,
        13,
        2432,
        409,
        1077,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.364619272405451,
      "compression_ratio": 1.51931330472103,
      "no_speech_prob": 1.1793576959462371e-7
    },
    {
      "id": 453,
      "seek": 306328,
      "start": 3068.0800000000004,
      "end": 3074,
      "text": " dettagli. Però se noi dovessimo fare un server useremmo Express da questo punto di vista.",
      "tokens": [
        1141,
        25030,
        2081,
        13,
        20533,
        369,
        22447,
        30870,
        442,
        6934,
        11994,
        517,
        7154,
        505,
        323,
        2174,
        78,
        20212,
        1120,
        10263,
        14326,
        1026,
        22553,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.364619272405451,
      "compression_ratio": 1.51931330472103,
      "no_speech_prob": 1.1793576959462371e-7
    },
    {
      "id": 454,
      "seek": 306328,
      "start": 3074,
      "end": 3081.88,
      "text": " Quindi invece Fastify, che se non conoscete la community di Fastify, adesso a parte Matteo",
      "tokens": [
        32534,
        36344,
        15968,
        2505,
        11,
        947,
        369,
        2107,
        416,
        10466,
        3498,
        635,
        1768,
        1026,
        15968,
        2505,
        11,
        39552,
        257,
        6975,
        47544,
        78
      ],
      "temperature": 0,
      "avg_logprob": -0.364619272405451,
      "compression_ratio": 1.51931330472103,
      "no_speech_prob": 1.1793576959462371e-7
    },
    {
      "id": 455,
      "seek": 306328,
      "start": 3081.88,
      "end": 3089.52,
      "text": " e io... La conoscono, la conoscono. Rompo le balle io. No, è un prodotto fenomenale",
      "tokens": [
        308,
        19785,
        485,
        2369,
        49892,
        45846,
        11,
        635,
        49892,
        45846,
        13,
        497,
        8586,
        78,
        476,
        2594,
        68,
        19785,
        13,
        883,
        11,
        4873,
        517,
        15792,
        18838,
        26830,
        4726,
        1220
      ],
      "temperature": 0,
      "avg_logprob": -0.364619272405451,
      "compression_ratio": 1.51931330472103,
      "no_speech_prob": 1.1793576959462371e-7
    },
    {
      "id": 456,
      "seek": 308952,
      "start": 3089.52,
      "end": 3098.4,
      "text": " dal punto di vista della community. Cioè è una cosa mostruosa. Mercurius non è attivo",
      "tokens": [
        11702,
        14326,
        1026,
        22553,
        11618,
        1768,
        13,
        383,
        35983,
        4873,
        2002,
        10163,
        881,
        894,
        6447,
        13,
        18897,
        374,
        4872,
        2107,
        4873,
        951,
        6340
      ],
      "temperature": 0,
      "avg_logprob": -0.33346765252608285,
      "compression_ratio": 1.4475138121546962,
      "no_speech_prob": 9.132509717346693e-7
    },
    {
      "id": 457,
      "seek": 308952,
      "start": 3098.4,
      "end": 3104.04,
      "text": " come Fastify perché è più di nicchia in generale come prodotto, perché Grafico L",
      "tokens": [
        808,
        15968,
        2505,
        14303,
        4873,
        10589,
        1026,
        6201,
        339,
        654,
        294,
        1337,
        1220,
        808,
        15792,
        18838,
        11,
        14303,
        8985,
        1786,
        78,
        441
      ],
      "temperature": 0,
      "avg_logprob": -0.33346765252608285,
      "compression_ratio": 1.4475138121546962,
      "no_speech_prob": 9.132509717346693e-7
    },
    {
      "id": 458,
      "seek": 308952,
      "start": 3104.04,
      "end": 3110.72,
      "text": " è più di nicchia rispetto a un server rest. Poi per usare Apollo c'è Fastify sotto...",
      "tokens": [
        4873,
        10589,
        1026,
        6201,
        339,
        654,
        2253,
        42801,
        257,
        517,
        7154,
        1472,
        13,
        430,
        4869,
        680,
        505,
        543,
        25187,
        269,
        6,
        1462,
        15968,
        2505,
        43754,
        485
      ],
      "temperature": 0,
      "avg_logprob": -0.33346765252608285,
      "compression_ratio": 1.4475138121546962,
      "no_speech_prob": 9.132509717346693e-7
    },
    {
      "id": 459,
      "seek": 311072,
      "start": 3110.72,
      "end": 3119.64,
      "text": " Scusate, Mercurius c'è Fastify. Non è che non usate Fastify se usate Mercurius. Però",
      "tokens": [
        318,
        1149,
        473,
        11,
        18897,
        374,
        4872,
        269,
        6,
        1462,
        15968,
        2505,
        13,
        8774,
        4873,
        947,
        2107,
        505,
        473,
        15968,
        2505,
        369,
        505,
        473,
        18897,
        374,
        4872,
        13,
        20533
      ],
      "temperature": 0,
      "avg_logprob": -0.225701118337697,
      "compression_ratio": 1.6,
      "no_speech_prob": 0.0000018738684275376727
    },
    {
      "id": 460,
      "seek": 311072,
      "start": 3119.64,
      "end": 3126.6,
      "text": " è un prodotto ben supportato. Fatto bene, fatto che funzioni. Ha i suoi bug, ma ce li",
      "tokens": [
        4873,
        517,
        15792,
        18838,
        3271,
        1406,
        2513,
        13,
        16948,
        1353,
        2537,
        11,
        23228,
        947,
        49345,
        15273,
        13,
        4064,
        741,
        459,
        4869,
        7426,
        11,
        463,
        1769,
        375
      ],
      "temperature": 0,
      "avg_logprob": -0.225701118337697,
      "compression_ratio": 1.6,
      "no_speech_prob": 0.0000018738684275376727
    },
    {
      "id": 461,
      "seek": 311072,
      "start": 3126.6,
      "end": 3132.72,
      "text": " hanno tutti. Però una volta che si trova un bug poi si va a correggere. Io l'ho usato",
      "tokens": [
        26595,
        19822,
        13,
        20533,
        2002,
        18765,
        947,
        1511,
        4495,
        2757,
        517,
        7426,
        19260,
        1511,
        2773,
        257,
        29731,
        1615,
        323,
        13,
        19239,
        287,
        6,
        1289,
        505,
        2513
      ],
      "temperature": 0,
      "avg_logprob": -0.225701118337697,
      "compression_ratio": 1.6,
      "no_speech_prob": 0.0000018738684275376727
    },
    {
      "id": 462,
      "seek": 311072,
      "start": 3132.72,
      "end": 3137.68,
      "text": " in produzione in un progetto molto grosso, siamo contentissimi. Il cliente è stato contentissimo.",
      "tokens": [
        294,
        1082,
        19706,
        294,
        517,
        447,
        847,
        1353,
        16394,
        18638,
        539,
        11,
        33459,
        2701,
        891,
        10121,
        13,
        4416,
        6423,
        68,
        4873,
        29657,
        2701,
        34966,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.225701118337697,
      "compression_ratio": 1.6,
      "no_speech_prob": 0.0000018738684275376727
    },
    {
      "id": 463,
      "seek": 313768,
      "start": 3137.68,
      "end": 3146.48,
      "text": " Un progetto complesso, federazione, con dieci nodi. Non sto parlando di un serverino che",
      "tokens": [
        1156,
        447,
        847,
        1353,
        1209,
        5557,
        11,
        4636,
        1663,
        19706,
        11,
        416,
        978,
        537,
        15224,
        72,
        13,
        8774,
        22784,
        971,
        16201,
        1026,
        517,
        7154,
        2982,
        947
      ],
      "temperature": 0,
      "avg_logprob": -0.3142602610033612,
      "compression_ratio": 1.459016393442623,
      "no_speech_prob": 1.0087580903928028e-7
    },
    {
      "id": 464,
      "seek": 313768,
      "start": 3146.48,
      "end": 3155.68,
      "text": " serve tra entità. Parliamo di una roba... Abbiamo fatto una migrazione da Apollo a Mercurius",
      "tokens": [
        4596,
        944,
        948,
        12445,
        13,
        3457,
        49926,
        1026,
        2002,
        3870,
        64,
        485,
        32673,
        7415,
        23228,
        2002,
        6186,
        424,
        19706,
        1120,
        25187,
        257,
        18897,
        374,
        4872
      ],
      "temperature": 0,
      "avg_logprob": -0.3142602610033612,
      "compression_ratio": 1.459016393442623,
      "no_speech_prob": 1.0087580903928028e-7
    },
    {
      "id": 465,
      "seek": 313768,
      "start": 3155.68,
      "end": 3164.2799999999997,
      "text": " e il passaggio... Abbiamo tolto due cose, Sequelize, che era una versione vecchia, e",
      "tokens": [
        308,
        1930,
        1320,
        30763,
        485,
        32673,
        7415,
        281,
        75,
        1353,
        3462,
        30261,
        11,
        1100,
        20593,
        1125,
        11,
        947,
        4249,
        2002,
        3037,
        68,
        42021,
        339,
        654,
        11,
        308
      ],
      "temperature": 0,
      "avg_logprob": -0.3142602610033612,
      "compression_ratio": 1.459016393442623,
      "no_speech_prob": 1.0087580903928028e-7
    },
    {
      "id": 466,
      "seek": 316428,
      "start": 3164.28,
      "end": 3171.28,
      "text": " Mercurius. Sequelize e Apollo. La rimozione di Sequelize, che era una versione vecchia",
      "tokens": [
        18897,
        374,
        4872,
        13,
        1100,
        20593,
        1125,
        308,
        25187,
        13,
        2369,
        367,
        6934,
        19706,
        1026,
        1100,
        20593,
        1125,
        11,
        947,
        4249,
        2002,
        3037,
        68,
        42021,
        339,
        654
      ],
      "temperature": 0,
      "avg_logprob": -0.28927388921514285,
      "compression_ratio": 1.5219298245614035,
      "no_speech_prob": 9.777217968576224e-8
    },
    {
      "id": 467,
      "seek": 316428,
      "start": 3171.28,
      "end": 3181.36,
      "text": " che usava ancora Bluebird. Oh mio Dio. Esatto. Tappava non poco. Il passaggio Mercurius,",
      "tokens": [
        947,
        505,
        4061,
        30656,
        8510,
        18080,
        13,
        876,
        29908,
        413,
        1004,
        13,
        2313,
        37491,
        13,
        314,
        1746,
        4061,
        2107,
        10639,
        13,
        4416,
        1320,
        30763,
        18897,
        374,
        4872,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.28927388921514285,
      "compression_ratio": 1.5219298245614035,
      "no_speech_prob": 9.777217968576224e-8
    },
    {
      "id": 468,
      "seek": 316428,
      "start": 3181.36,
      "end": 3186.1200000000003,
      "text": " che ci ha permesso di andarci un po' più spinti con il sistema di cache, con queste",
      "tokens": [
        947,
        6983,
        324,
        4784,
        5557,
        1026,
        50009,
        537,
        517,
        714,
        6,
        10589,
        637,
        686,
        72,
        416,
        1930,
        13245,
        1026,
        19459,
        11,
        416,
        35455
      ],
      "temperature": 0,
      "avg_logprob": -0.28927388921514285,
      "compression_ratio": 1.5219298245614035,
      "no_speech_prob": 9.777217968576224e-8
    },
    {
      "id": 469,
      "seek": 316428,
      "start": 3186.1200000000003,
      "end": 3193.4,
      "text": " cose, ha fatto la differenza. Il sistema era oggettivamente più veloce, ma molto più",
      "tokens": [
        30261,
        11,
        324,
        23228,
        635,
        743,
        23691,
        13,
        4416,
        13245,
        4249,
        5360,
        847,
        83,
        23957,
        10589,
        1241,
        752,
        384,
        11,
        463,
        16394,
        10589
      ],
      "temperature": 0,
      "avg_logprob": -0.28927388921514285,
      "compression_ratio": 1.5219298245614035,
      "no_speech_prob": 9.777217968576224e-8
    },
    {
      "id": 470,
      "seek": 319340,
      "start": 3193.4,
      "end": 3204.6800000000003,
      "text": " veloce. Da questo punto di vista io voto Mercurius. Se si devono sentire più sicuri, avere un'azienda",
      "tokens": [
        1241,
        752,
        384,
        13,
        3933,
        10263,
        14326,
        1026,
        22553,
        19785,
        3478,
        78,
        18897,
        374,
        4872,
        13,
        1100,
        1511,
        1905,
        8957,
        2279,
        621,
        10589,
        33579,
        9744,
        11,
        37914,
        517,
        6,
        921,
        30498
      ],
      "temperature": 0,
      "avg_logprob": -0.3803725715156074,
      "compression_ratio": 1.5458333333333334,
      "no_speech_prob": 7.22443473932799e-7
    },
    {
      "id": 471,
      "seek": 319340,
      "start": 3204.6800000000003,
      "end": 3213.2000000000003,
      "text": " supportata che vuole un servizio di pagamento di Apollo, loro te lo danno. Quindi si può",
      "tokens": [
        1406,
        3274,
        947,
        9732,
        4812,
        517,
        1658,
        590,
        1004,
        1026,
        11812,
        8824,
        1026,
        25187,
        11,
        28810,
        535,
        450,
        3594,
        78,
        13,
        32534,
        1511,
        26526
      ],
      "temperature": 0,
      "avg_logprob": -0.3803725715156074,
      "compression_ratio": 1.5458333333333334,
      "no_speech_prob": 7.22443473932799e-7
    },
    {
      "id": 472,
      "seek": 319340,
      "start": 3213.2000000000003,
      "end": 3216.52,
      "text": " fare. È anche più supportata dal punto di vista del tooling di Apollo, c'è molte più",
      "tokens": [
        11994,
        13,
        34495,
        11585,
        10589,
        1406,
        3274,
        11702,
        14326,
        1026,
        22553,
        1103,
        46593,
        1026,
        25187,
        11,
        269,
        6,
        1462,
        8015,
        975,
        10589
      ],
      "temperature": 0,
      "avg_logprob": -0.3803725715156074,
      "compression_ratio": 1.5458333333333334,
      "no_speech_prob": 7.22443473932799e-7
    },
    {
      "id": 473,
      "seek": 319340,
      "start": 3216.52,
      "end": 3222.88,
      "text": " cose. Il tooling fra l'altro è uno dei grandi problemi di GraphQL. Se vuoi dopo parliamo",
      "tokens": [
        30261,
        13,
        4416,
        46593,
        6600,
        287,
        6,
        47484,
        4873,
        8526,
        13874,
        45155,
        1154,
        72,
        1026,
        21884,
        13695,
        13,
        1100,
        9732,
        4869,
        35196,
        971,
        49926
      ],
      "temperature": 0,
      "avg_logprob": -0.3803725715156074,
      "compression_ratio": 1.5458333333333334,
      "no_speech_prob": 7.22443473932799e-7
    },
    {
      "id": 474,
      "seek": 322288,
      "start": 3222.88,
      "end": 3228.32,
      "text": " di che cosa vuol dire secondo me lavorare in GraphQL. No, io so su cos'hai lavorato e",
      "tokens": [
        1026,
        947,
        10163,
        9732,
        401,
        1264,
        41601,
        385,
        29241,
        543,
        294,
        21884,
        13695,
        13,
        883,
        11,
        19785,
        370,
        459,
        3792,
        6,
        18230,
        29241,
        2513,
        308
      ],
      "temperature": 0,
      "avg_logprob": -0.3843455655234201,
      "compression_ratio": 1.5321888412017168,
      "no_speech_prob": 1.7159548804102087e-7
    },
    {
      "id": 475,
      "seek": 322288,
      "start": 3228.32,
      "end": 3235.44,
      "text": " quindi voglio arrivarci su quello. Perché tu hai ultimamente lavorato su Mercurius anche",
      "tokens": [
        15727,
        31273,
        19987,
        3399,
        8517,
        537,
        459,
        22813,
        13,
        47978,
        2604,
        21822,
        3725,
        332,
        3439,
        29241,
        2513,
        459,
        18897,
        374,
        4872,
        11585
      ],
      "temperature": 0,
      "avg_logprob": -0.3843455655234201,
      "compression_ratio": 1.5321888412017168,
      "no_speech_prob": 1.7159548804102087e-7
    },
    {
      "id": 476,
      "seek": 322288,
      "start": 3235.44,
      "end": 3241.76,
      "text": " in progetti abbastanza grossi. È uno dei topic... hai lavorato anche su il core di",
      "tokens": [
        294,
        447,
        847,
        7317,
        16903,
        525,
        20030,
        11367,
        72,
        13,
        34495,
        8526,
        13874,
        4829,
        485,
        21822,
        29241,
        2513,
        11585,
        459,
        1930,
        4965,
        1026
      ],
      "temperature": 0,
      "avg_logprob": -0.3843455655234201,
      "compression_ratio": 1.5321888412017168,
      "no_speech_prob": 1.7159548804102087e-7
    },
    {
      "id": 477,
      "seek": 322288,
      "start": 3241.76,
      "end": 3252.84,
      "text": " Mercurius se non mi sbaglio. Io ho fatto refactoring. Io ho estrapolato il Gateway e la Federale.",
      "tokens": [
        18897,
        374,
        4872,
        369,
        2107,
        2752,
        262,
        17282,
        19987,
        13,
        19239,
        1106,
        23228,
        1895,
        578,
        3662,
        13,
        19239,
        1106,
        871,
        4007,
        401,
        2513,
        1930,
        48394,
        308,
        635,
        479,
        292,
        2790,
        68,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.3843455655234201,
      "compression_ratio": 1.5321888412017168,
      "no_speech_prob": 1.7159548804102087e-7
    },
    {
      "id": 478,
      "seek": 325284,
      "start": 3252.84,
      "end": 3261.88,
      "text": " Hai diviso il gemello Siamese. Ho diviso il gemello Siamese. Hai tirato fuori tutta la",
      "tokens": [
        24055,
        3414,
        19227,
        1930,
        7173,
        11216,
        318,
        2918,
        1130,
        13,
        3631,
        3414,
        19227,
        1930,
        7173,
        11216,
        318,
        2918,
        1130,
        13,
        24055,
        13807,
        2513,
        8536,
        7386,
        3672,
        1328,
        635
      ],
      "temperature": 0,
      "avg_logprob": -0.30719889624644137,
      "compression_ratio": 1.6108597285067874,
      "no_speech_prob": 0.000002948009068859392
    },
    {
      "id": 479,
      "seek": 325284,
      "start": 3261.88,
      "end": 3266.84,
      "text": " parte di Federation perché era strettamente accopiata a Mercurius. Sì, era un unico progetto.",
      "tokens": [
        6975,
        1026,
        27237,
        14303,
        4249,
        342,
        14313,
        3439,
        1317,
        404,
        72,
        3274,
        257,
        18897,
        374,
        4872,
        13,
        318,
        4749,
        11,
        4249,
        517,
        517,
        2789,
        447,
        847,
        1353,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.30719889624644137,
      "compression_ratio": 1.6108597285067874,
      "no_speech_prob": 0.000002948009068859392
    },
    {
      "id": 480,
      "seek": 325284,
      "start": 3266.84,
      "end": 3274.6400000000003,
      "text": " Adesso se vuoi usare la Federazione e vuoi costruire un Gateway, devi usare un modulo",
      "tokens": [
        1999,
        5557,
        369,
        9732,
        4869,
        505,
        543,
        635,
        45545,
        12928,
        308,
        9732,
        4869,
        2063,
        894,
        621,
        517,
        48394,
        11,
        31219,
        505,
        543,
        517,
        1072,
        13455
      ],
      "temperature": 0,
      "avg_logprob": -0.30719889624644137,
      "compression_ratio": 1.6108597285067874,
      "no_speech_prob": 0.000002948009068859392
    },
    {
      "id": 481,
      "seek": 325284,
      "start": 3274.6400000000003,
      "end": 3280.48,
      "text": " a parte. Il codice è quello, non è che l'ho riscritto. Soprattutto sulla Federazione,",
      "tokens": [
        257,
        6975,
        13,
        4416,
        17656,
        573,
        4873,
        22813,
        11,
        2107,
        4873,
        947,
        287,
        6,
        1289,
        2253,
        10757,
        34924,
        13,
        318,
        404,
        81,
        48285,
        33625,
        45545,
        12928,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.30719889624644137,
      "compression_ratio": 1.6108597285067874,
      "no_speech_prob": 0.000002948009068859392
    },
    {
      "id": 482,
      "seek": 328048,
      "start": 3280.48,
      "end": 3285.4,
      "text": " come ne male, è quello. Sul Gateway l'ho estratto e ho dovuto fare un po' di ritocchi",
      "tokens": [
        808,
        408,
        7133,
        11,
        4873,
        22813,
        13,
        35897,
        48394,
        287,
        6,
        1289,
        42746,
        1353,
        308,
        1106,
        30870,
        8262,
        11994,
        517,
        714,
        6,
        1026,
        11289,
        905,
        8036
      ],
      "temperature": 0,
      "avg_logprob": -0.25139441419003616,
      "compression_ratio": 1.6628352490421456,
      "no_speech_prob": 1.073816164876007e-7
    },
    {
      "id": 483,
      "seek": 328048,
      "start": 3285.4,
      "end": 3289.84,
      "text": " perché non era sufficiente estrarlo. Cioè ci sono andato un po' a fondo, ma è stato",
      "tokens": [
        14303,
        2107,
        4249,
        11563,
        68,
        871,
        5352,
        752,
        13,
        383,
        35983,
        6983,
        9259,
        293,
        2513,
        517,
        714,
        6,
        257,
        38101,
        11,
        463,
        4873,
        29657
      ],
      "temperature": 0,
      "avg_logprob": -0.25139441419003616,
      "compression_ratio": 1.6628352490421456,
      "no_speech_prob": 1.073816164876007e-7
    },
    {
      "id": 484,
      "seek": 328048,
      "start": 3289.84,
      "end": 3295,
      "text": " molto utile perché questo mi ha permesso di andare nel core. Infatti dopo che l'ho",
      "tokens": [
        16394,
        2839,
        794,
        14303,
        10263,
        2752,
        324,
        4784,
        5557,
        1026,
        42742,
        15373,
        4965,
        13,
        11537,
        21515,
        35196,
        947,
        287,
        6,
        1289
      ],
      "temperature": 0,
      "avg_logprob": -0.25139441419003616,
      "compression_ratio": 1.6628352490421456,
      "no_speech_prob": 1.073816164876007e-7
    },
    {
      "id": 485,
      "seek": 328048,
      "start": 3295,
      "end": 3299.76,
      "text": " fatta, che sapevo esattamente che cosa succedeva lì dentro, ho iniziato a prendere a mano",
      "tokens": [
        4046,
        1328,
        11,
        947,
        601,
        494,
        3080,
        785,
        1591,
        3439,
        947,
        10163,
        1965,
        29815,
        2757,
        287,
        4749,
        10856,
        11,
        1106,
        294,
        24300,
        2513,
        257,
        9866,
        323,
        257,
        18384
      ],
      "temperature": 0,
      "avg_logprob": -0.25139441419003616,
      "compression_ratio": 1.6628352490421456,
      "no_speech_prob": 1.073816164876007e-7
    },
    {
      "id": 486,
      "seek": 328048,
      "start": 3299.76,
      "end": 3305,
      "text": " un po' di issue aperte sulla Federazione e a chiuderle perché poi sapevo esattamente",
      "tokens": [
        517,
        714,
        6,
        1026,
        2734,
        43139,
        975,
        33625,
        45545,
        12928,
        308,
        257,
        13228,
        28230,
        306,
        14303,
        19260,
        601,
        494,
        3080,
        785,
        1591,
        3439
      ],
      "temperature": 0,
      "avg_logprob": -0.25139441419003616,
      "compression_ratio": 1.6628352490421456,
      "no_speech_prob": 1.073816164876007e-7
    },
    {
      "id": 487,
      "seek": 330500,
      "start": 3305,
      "end": 3313.36,
      "text": " che il problema era da risolvere. Dopo ho iniziato a chiudere un po' di task.",
      "tokens": [
        947,
        1930,
        12395,
        4249,
        1120,
        2253,
        401,
        5887,
        13,
        42657,
        78,
        1106,
        294,
        24300,
        2513,
        257,
        13228,
        532,
        323,
        517,
        714,
        6,
        1026,
        5633,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.3824334986069623,
      "compression_ratio": 1.441988950276243,
      "no_speech_prob": 1.826623474698863e-7
    },
    {
      "id": 488,
      "seek": 330500,
      "start": 3313.36,
      "end": 3319.24,
      "text": " A fare un po' di pulizia. Tra l'altro io ci ho lavorato per un progetto, ci tengo ancora",
      "tokens": [
        316,
        11994,
        517,
        714,
        6,
        1026,
        8331,
        590,
        654,
        13,
        5403,
        287,
        6,
        47484,
        19785,
        6983,
        1106,
        29241,
        2513,
        680,
        517,
        447,
        847,
        1353,
        11,
        6983,
        13989,
        30656
      ],
      "temperature": 0,
      "avg_logprob": -0.3824334986069623,
      "compression_ratio": 1.441988950276243,
      "no_speech_prob": 1.826623474698863e-7
    },
    {
      "id": 489,
      "seek": 330500,
      "start": 3319.24,
      "end": 3331.56,
      "text": " a ricordarlo, era cercare di utilizzare Mercurius come Gateway per diverse API, tra cui un API",
      "tokens": [
        257,
        21040,
        765,
        19457,
        11,
        4249,
        10146,
        5685,
        1026,
        40355,
        543,
        18897,
        374,
        4872,
        808,
        48394,
        680,
        9521,
        9362,
        11,
        944,
        22929,
        517,
        9362
      ],
      "temperature": 0,
      "avg_logprob": -0.3824334986069623,
      "compression_ratio": 1.441988950276243,
      "no_speech_prob": 1.826623474698863e-7
    },
    {
      "id": 490,
      "seek": 333156,
      "start": 3331.56,
      "end": 3340.52,
      "text": " non federata di Strapi, quindi probabilmente conosci quel ticket che hai girato in azienda",
      "tokens": [
        2107,
        38024,
        3274,
        1026,
        745,
        4007,
        72,
        11,
        15727,
        31959,
        4082,
        49892,
        537,
        7178,
        10550,
        947,
        21822,
        14703,
        2513,
        294,
        7883,
        30498
      ],
      "temperature": 0,
      "avg_logprob": -0.28017589781019425,
      "compression_ratio": 1.4545454545454546,
      "no_speech_prob": 1.216794487390871e-7
    },
    {
      "id": 491,
      "seek": 333156,
      "start": 3340.52,
      "end": 3346.96,
      "text": " per un po', forse è il nostro amico Simone al quale mandiamo un grande abbraccio, è",
      "tokens": [
        680,
        517,
        714,
        6098,
        337,
        405,
        4873,
        1930,
        35779,
        669,
        2789,
        41652,
        419,
        421,
        1220,
        7411,
        7415,
        517,
        8883,
        410,
        1443,
        326,
        8529,
        11,
        4873
      ],
      "temperature": 0,
      "avg_logprob": -0.28017589781019425,
      "compression_ratio": 1.4545454545454546,
      "no_speech_prob": 1.216794487390871e-7
    },
    {
      "id": 492,
      "seek": 333156,
      "start": 3346.96,
      "end": 3354.68,
      "text": " stato l'ultimo a cui ci ha lavorato, è stata un po' una follia. Però di lì io per esempio",
      "tokens": [
        29657,
        287,
        6,
        723,
        6934,
        257,
        22929,
        6983,
        324,
        29241,
        2513,
        11,
        4873,
        49554,
        517,
        714,
        6,
        2002,
        25483,
        654,
        13,
        20533,
        1026,
        287,
        4749,
        19785,
        680,
        33627
      ],
      "temperature": 0,
      "avg_logprob": -0.28017589781019425,
      "compression_ratio": 1.4545454545454546,
      "no_speech_prob": 1.216794487390871e-7
    },
    {
      "id": 493,
      "seek": 333156,
      "start": 3354.68,
      "end": 3359.96,
      "text": " ho approcciato al mondo della Federazione GraphQL. Ce lo vuoi raccontare in breve?",
      "tokens": [
        1106,
        2075,
        66,
        537,
        2513,
        419,
        40499,
        11618,
        45545,
        12928,
        21884,
        13695,
        13,
        8257,
        450,
        9732,
        4869,
        4129,
        9000,
        543,
        294,
        48517,
        30
      ],
      "temperature": 0,
      "avg_logprob": -0.28017589781019425,
      "compression_ratio": 1.4545454545454546,
      "no_speech_prob": 1.216794487390871e-7
    },
    {
      "id": 494,
      "seek": 335996,
      "start": 3359.96,
      "end": 3367.4,
      "text": " Allora io l'ho scoperto in questo ultimo progetto che ho fatto, molto grosso, non sapevo neanche",
      "tokens": [
        1057,
        3252,
        19785,
        287,
        6,
        1289,
        795,
        404,
        13098,
        294,
        10263,
        3725,
        6934,
        447,
        847,
        1353,
        947,
        1106,
        23228,
        11,
        16394,
        18638,
        539,
        11,
        2107,
        601,
        494,
        3080,
        408,
        22806
      ],
      "temperature": 0,
      "avg_logprob": -0.31372702658713403,
      "compression_ratio": 1.6636363636363636,
      "no_speech_prob": 4.205130821333114e-8
    },
    {
      "id": 495,
      "seek": 335996,
      "start": 3367.4,
      "end": 3372.68,
      "text": " certo poi come si poteva fare perché io era poi un po' che non lavoravo con GraphQL,",
      "tokens": [
        22261,
        19260,
        808,
        1511,
        280,
        1370,
        2757,
        11994,
        14303,
        19785,
        4249,
        19260,
        517,
        714,
        6,
        947,
        2107,
        29241,
        25713,
        416,
        21884,
        13695,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.31372702658713403,
      "compression_ratio": 1.6636363636363636,
      "no_speech_prob": 4.205130821333114e-8
    },
    {
      "id": 496,
      "seek": 335996,
      "start": 3372.68,
      "end": 3378.6,
      "text": " perché appunto lavorando nella consulenza e lavorando un anno, un anno e mezzo sui progetti",
      "tokens": [
        14303,
        724,
        24052,
        29241,
        1806,
        23878,
        1014,
        425,
        23691,
        308,
        29241,
        1806,
        517,
        46277,
        11,
        517,
        46277,
        308,
        385,
        35130,
        459,
        72,
        447,
        847,
        7317
      ],
      "temperature": 0,
      "avg_logprob": -0.31372702658713403,
      "compression_ratio": 1.6636363636363636,
      "no_speech_prob": 4.205130821333114e-8
    },
    {
      "id": 497,
      "seek": 335996,
      "start": 3378.6,
      "end": 3384.2400000000002,
      "text": " finisci su un progetto che non lo usa e tu semplicemente sei fuori completamente da quella",
      "tokens": [
        962,
        271,
        537,
        459,
        517,
        447,
        847,
        1353,
        947,
        2107,
        450,
        29909,
        308,
        2604,
        4361,
        4770,
        16288,
        10842,
        8536,
        7386,
        28381,
        1120,
        32234
      ],
      "temperature": 0,
      "avg_logprob": -0.31372702658713403,
      "compression_ratio": 1.6636363636363636,
      "no_speech_prob": 4.205130821333114e-8
    },
    {
      "id": 498,
      "seek": 338424,
      "start": 3384.24,
      "end": 3391.3999999999996,
      "text": " consulenza. Quindi per me era una novità abbastanza, quindi ho avuto occasione di rimetterci le",
      "tokens": [
        1014,
        425,
        23691,
        13,
        32534,
        680,
        385,
        4249,
        2002,
        572,
        10398,
        1467,
        16903,
        525,
        20030,
        11,
        15727,
        1106,
        1305,
        8262,
        15319,
        5328,
        1026,
        15982,
        27296,
        537,
        476
      ],
      "temperature": 0,
      "avg_logprob": -0.3211720710576967,
      "compression_ratio": 1.456989247311828,
      "no_speech_prob": 6.615592695879968e-8
    },
    {
      "id": 499,
      "seek": 338424,
      "start": 3391.3999999999996,
      "end": 3399.56,
      "text": " mani e la federazione sì ne avevo sentito parlare ma non l'avevo mai fatta. Che cosa",
      "tokens": [
        587,
        72,
        308,
        635,
        38024,
        12928,
        49267,
        408,
        3472,
        3080,
        2279,
        3528,
        13734,
        543,
        463,
        2107,
        287,
        6,
        946,
        3080,
        12698,
        283,
        1591,
        64,
        13,
        3351,
        10163
      ],
      "temperature": 0,
      "avg_logprob": -0.3211720710576967,
      "compression_ratio": 1.456989247311828,
      "no_speech_prob": 6.615592695879968e-8
    },
    {
      "id": 500,
      "seek": 338424,
      "start": 3399.56,
      "end": 3405.7999999999997,
      "text": " vuol dire fare la federazione? Vuol dire che tu invece di dichiarare un unico schema, un",
      "tokens": [
        9732,
        401,
        1264,
        11994,
        635,
        38024,
        12928,
        30,
        37703,
        401,
        1264,
        947,
        2604,
        36344,
        1026,
        10390,
        9448,
        543,
        517,
        517,
        2789,
        34078,
        11,
        517
      ],
      "temperature": 0,
      "avg_logprob": -0.3211720710576967,
      "compression_ratio": 1.456989247311828,
      "no_speech_prob": 6.615592695879968e-8
    },
    {
      "id": 501,
      "seek": 340580,
      "start": 3405.8,
      "end": 3414.84,
      "text": " server, tu hai ogni server, chiamiamolo service visto che è il termine utilizzato, ogni service",
      "tokens": [
        7154,
        11,
        2604,
        21822,
        33189,
        7154,
        11,
        417,
        2918,
        2918,
        7902,
        2643,
        17558,
        947,
        4873,
        1930,
        1433,
        533,
        40355,
        2513,
        11,
        33189,
        2643
      ],
      "temperature": 0,
      "avg_logprob": -0.21324366017391808,
      "compression_ratio": 1.6666666666666667,
      "no_speech_prob": 5.203497153161152e-7
    },
    {
      "id": 502,
      "seek": 340580,
      "start": 3414.84,
      "end": 3425.32,
      "text": " ha il suo schema, qualcuno di questi oggetti nello schema hanno delle chiavi dichiarate",
      "tokens": [
        324,
        1930,
        34197,
        34078,
        11,
        32101,
        12638,
        1026,
        29729,
        5360,
        847,
        7317,
        408,
        1913,
        34078,
        26595,
        16485,
        45793,
        4917,
        10390,
        9448,
        473
      ],
      "temperature": 0,
      "avg_logprob": -0.21324366017391808,
      "compression_ratio": 1.6666666666666667,
      "no_speech_prob": 5.203497153161152e-7
    },
    {
      "id": 503,
      "seek": 340580,
      "start": 3425.32,
      "end": 3431.88,
      "text": " esterne o delle entità che sono dichiarate come esterne e che lui si limita a sapere",
      "tokens": [
        871,
        48135,
        277,
        16485,
        948,
        12445,
        947,
        9259,
        10390,
        9448,
        473,
        808,
        871,
        48135,
        308,
        947,
        8783,
        1511,
        2364,
        2786,
        257,
        18985,
        323
      ],
      "temperature": 0,
      "avg_logprob": -0.21324366017391808,
      "compression_ratio": 1.6666666666666667,
      "no_speech_prob": 5.203497153161152e-7
    },
    {
      "id": 504,
      "seek": 343188,
      "start": 3431.88,
      "end": 3438.84,
      "text": " che esistono, nulla più, e a eventualmente fornirne l'identificativo e poi c'è un sistema",
      "tokens": [
        947,
        785,
        468,
        8957,
        11,
        18184,
        64,
        10589,
        11,
        308,
        257,
        33160,
        4082,
        337,
        77,
        347,
        716,
        287,
        6,
        1078,
        1089,
        18586,
        308,
        19260,
        269,
        6,
        1462,
        517,
        13245
      ],
      "temperature": 0,
      "avg_logprob": -0.24807771614619664,
      "compression_ratio": 1.639269406392694,
      "no_speech_prob": 2.2033270852261921e-7
    },
    {
      "id": 505,
      "seek": 343188,
      "start": 3438.84,
      "end": 3446.04,
      "text": " di gateway che altro non fa che interrogare tutti questi servizi che tu dichiari all'inizio",
      "tokens": [
        1026,
        28532,
        947,
        40924,
        2107,
        2050,
        947,
        24871,
        543,
        19822,
        29729,
        1658,
        24300,
        947,
        2604,
        10390,
        72,
        3504,
        439,
        6,
        9328,
        1004
      ],
      "temperature": 0,
      "avg_logprob": -0.24807771614619664,
      "compression_ratio": 1.639269406392694,
      "no_speech_prob": 2.2033270852261921e-7
    },
    {
      "id": 506,
      "seek": 343188,
      "start": 3446.04,
      "end": 3453.28,
      "text": " e lui uno per uno prende questo schema, lo mergea in un'unica grande schema e fa vedere",
      "tokens": [
        308,
        8783,
        8526,
        680,
        8526,
        9866,
        68,
        10263,
        34078,
        11,
        450,
        22183,
        64,
        294,
        517,
        6,
        409,
        2262,
        8883,
        34078,
        308,
        2050,
        35373
      ],
      "temperature": 0,
      "avg_logprob": -0.24807771614619664,
      "compression_ratio": 1.639269406392694,
      "no_speech_prob": 2.2033270852261921e-7
    },
    {
      "id": 507,
      "seek": 343188,
      "start": 3453.28,
      "end": 3458.7200000000003,
      "text": " all'utente finale un'unica macchina che lo usa appunto come se fosse un'unica macchina.",
      "tokens": [
        439,
        6,
        325,
        1576,
        23510,
        517,
        6,
        409,
        2262,
        7912,
        339,
        1426,
        947,
        450,
        29909,
        724,
        24052,
        808,
        369,
        24528,
        517,
        6,
        409,
        2262,
        7912,
        339,
        1426,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.24807771614619664,
      "compression_ratio": 1.639269406392694,
      "no_speech_prob": 2.2033270852261921e-7
    },
    {
      "id": 508,
      "seek": 345872,
      "start": 3458.72,
      "end": 3478.64,
      "text": " E queste cose ti permette di fare grandi cose, ti permette di avere un bel storm di nodi",
      "tokens": [
        462,
        35455,
        30261,
        8757,
        4784,
        3007,
        1026,
        11994,
        45155,
        30261,
        11,
        8757,
        4784,
        3007,
        1026,
        37914,
        517,
        989,
        7679,
        1026,
        15224,
        72
      ],
      "temperature": 0,
      "avg_logprob": -0.3525494556037747,
      "compression_ratio": 1.4126984126984128,
      "no_speech_prob": 2.457990433413215e-7
    },
    {
      "id": 509,
      "seek": 345872,
      "start": 3478.64,
      "end": 3487.8399999999997,
      "text": " su Kubernetes per esempio in cui i due servizi che hanno bisogno di potenza hanno 24 o 36",
      "tokens": [
        459,
        23145,
        680,
        33627,
        294,
        22929,
        741,
        3462,
        1658,
        24300,
        947,
        26595,
        40505,
        1771,
        1026,
        1847,
        23691,
        26595,
        4022,
        277,
        8652
      ],
      "temperature": 0,
      "avg_logprob": -0.3525494556037747,
      "compression_ratio": 1.4126984126984128,
      "no_speech_prob": 2.457990433413215e-7
    },
    {
      "id": 510,
      "seek": 348784,
      "start": 3487.84,
      "end": 3494.08,
      "text": " nodi e il servizetto che non viene usato poca niente, ogni tanto, gli mette solo uno senza",
      "tokens": [
        15224,
        72,
        308,
        1930,
        1658,
        590,
        23778,
        947,
        2107,
        19561,
        505,
        2513,
        714,
        496,
        297,
        8413,
        11,
        33189,
        10331,
        11,
        17161,
        1131,
        975,
        6944,
        8526,
        36208
      ],
      "temperature": 0,
      "avg_logprob": -0.35346785688822246,
      "compression_ratio": 1.6188340807174888,
      "no_speech_prob": 1.8846034777197929e-7
    },
    {
      "id": 511,
      "seek": 348784,
      "start": 3494.08,
      "end": 3500.28,
      "text": " dover necessariamente replicare un'unica macchina con una serie di vantaggi mostruosa, perché",
      "tokens": [
        360,
        331,
        2688,
        45149,
        3248,
        299,
        543,
        517,
        6,
        409,
        2262,
        7912,
        339,
        1426,
        416,
        2002,
        23030,
        1026,
        371,
        394,
        46893,
        881,
        894,
        6447,
        11,
        14303
      ],
      "temperature": 0,
      "avg_logprob": -0.35346785688822246,
      "compression_ratio": 1.6188340807174888,
      "no_speech_prob": 1.8846034777197929e-7
    },
    {
      "id": 512,
      "seek": 348784,
      "start": 3500.28,
      "end": 3505.8,
      "text": " di nuovo quando si parla di progetti di un certo tipo si parla di team, non è che tu",
      "tokens": [
        1026,
        49348,
        7770,
        1511,
        971,
        875,
        1026,
        447,
        847,
        7317,
        1026,
        517,
        22261,
        9746,
        1511,
        971,
        875,
        1026,
        1469,
        11,
        2107,
        4873,
        947,
        2604
      ],
      "temperature": 0,
      "avg_logprob": -0.35346785688822246,
      "compression_ratio": 1.6188340807174888,
      "no_speech_prob": 1.8846034777197929e-7
    },
    {
      "id": 513,
      "seek": 348784,
      "start": 3505.8,
      "end": 3512.28,
      "text": " lavori su un prodotto, non è che se tu vai, cito Booking perché l'abbiamo citato prima,",
      "tokens": [
        20923,
        7386,
        459,
        517,
        15792,
        18838,
        11,
        2107,
        4873,
        947,
        369,
        2604,
        4405,
        11,
        269,
        3528,
        9476,
        278,
        14303,
        287,
        6,
        10797,
        7415,
        4814,
        2513,
        19507,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.35346785688822246,
      "compression_ratio": 1.6188340807174888,
      "no_speech_prob": 1.8846034777197929e-7
    },
    {
      "id": 514,
      "seek": 351228,
      "start": 3512.28,
      "end": 3519.32,
      "text": " non è che Booking è un team dietro, Booking avrà 10 team, 15 team, 20 team che fanno",
      "tokens": [
        2107,
        4873,
        947,
        9476,
        278,
        4873,
        517,
        1469,
        6339,
        340,
        11,
        9476,
        278,
        1305,
        39212,
        1266,
        1469,
        11,
        2119,
        1469,
        11,
        945,
        1469,
        947,
        283,
        13484
      ],
      "temperature": 0,
      "avg_logprob": -0.30384324391682943,
      "compression_ratio": 1.6108597285067874,
      "no_speech_prob": 3.486161403998267e-8
    },
    {
      "id": 515,
      "seek": 351228,
      "start": 3519.32,
      "end": 3528.28,
      "text": " cose. Io quando lavoravo in KPLM, la home page di KPLM all'epoca ci saranno stati almeno",
      "tokens": [
        30261,
        13,
        19239,
        7770,
        29241,
        25713,
        294,
        591,
        21593,
        44,
        11,
        635,
        1280,
        3028,
        1026,
        591,
        21593,
        44,
        439,
        6,
        595,
        24035,
        6983,
        13782,
        13484,
        2219,
        72,
        419,
        43232
      ],
      "temperature": 0,
      "avg_logprob": -0.30384324391682943,
      "compression_ratio": 1.6108597285067874,
      "no_speech_prob": 3.486161403998267e-8
    },
    {
      "id": 516,
      "seek": 351228,
      "start": 3528.28,
      "end": 3534.0800000000004,
      "text": " 6 team, 7 team che lavoravano per buttare fuori quei dati di quella home page, fra back",
      "tokens": [
        1386,
        1469,
        11,
        1614,
        1469,
        947,
        29241,
        706,
        3730,
        680,
        6660,
        543,
        8536,
        7386,
        631,
        72,
        1137,
        72,
        1026,
        32234,
        1280,
        3028,
        11,
        6600,
        646
      ],
      "temperature": 0,
      "avg_logprob": -0.30384324391682943,
      "compression_ratio": 1.6108597285067874,
      "no_speech_prob": 3.486161403998267e-8
    },
    {
      "id": 517,
      "seek": 351228,
      "start": 3534.0800000000004,
      "end": 3539.4,
      "text": " end, fra front end, c'era un gruppo che faccia solo l'header, c'era tutta una serie di cose",
      "tokens": [
        917,
        11,
        6600,
        1868,
        917,
        11,
        269,
        6,
        1663,
        517,
        47477,
        78,
        947,
        1915,
        2755,
        6944,
        287,
        6,
        1934,
        260,
        11,
        269,
        6,
        1663,
        3672,
        1328,
        2002,
        23030,
        1026,
        30261
      ],
      "temperature": 0,
      "avg_logprob": -0.30384324391682943,
      "compression_ratio": 1.6108597285067874,
      "no_speech_prob": 3.486161403998267e-8
    },
    {
      "id": 518,
      "seek": 353940,
      "start": 3539.4,
      "end": 3544.12,
      "text": " e questa è la normalità, quindi nel back end avere la possibilità ognuno di lavorare",
      "tokens": [
        308,
        16540,
        4873,
        635,
        2710,
        12445,
        11,
        15727,
        15373,
        646,
        917,
        37914,
        635,
        24145,
        12445,
        277,
        4568,
        12638,
        1026,
        29241,
        543
      ],
      "temperature": 0,
      "avg_logprob": -0.30949268526243934,
      "compression_ratio": 1.5866666666666667,
      "no_speech_prob": 1.2237626023647863e-8
    },
    {
      "id": 519,
      "seek": 353940,
      "start": 3544.12,
      "end": 3550.96,
      "text": " sul proprio porticello fa una bella differenza e non è solo una questione di dimensione",
      "tokens": [
        17603,
        28203,
        2436,
        573,
        1913,
        2050,
        2002,
        312,
        3505,
        743,
        23691,
        308,
        2107,
        4873,
        6944,
        2002,
        1168,
        68,
        1026,
        10139,
        68
      ],
      "temperature": 0,
      "avg_logprob": -0.30949268526243934,
      "compression_ratio": 1.5866666666666667,
      "no_speech_prob": 1.2237626023647863e-8
    },
    {
      "id": 520,
      "seek": 353940,
      "start": 3550.96,
      "end": 3561.88,
      "text": " del progetto. Quando tu hai 10 team di 3 persone, 1, 4, stiamo sul piccolo back end, quando",
      "tokens": [
        1103,
        447,
        847,
        1353,
        13,
        18725,
        2604,
        21822,
        1266,
        1469,
        1026,
        805,
        29944,
        11,
        502,
        11,
        1017,
        11,
        342,
        7415,
        17603,
        13363,
        46086,
        646,
        917,
        11,
        7770
      ],
      "temperature": 0,
      "avg_logprob": -0.30949268526243934,
      "compression_ratio": 1.5866666666666667,
      "no_speech_prob": 1.2237626023647863e-8
    },
    {
      "id": 521,
      "seek": 353940,
      "start": 3561.88,
      "end": 3568.84,
      "text": " tu hai 3 per 10, 30 persone che lavorano sulla stessa home page, poi puoi fare le merge,",
      "tokens": [
        2604,
        21822,
        805,
        680,
        1266,
        11,
        2217,
        29944,
        947,
        29241,
        3730,
        33625,
        342,
        8391,
        1280,
        3028,
        11,
        19260,
        2362,
        4869,
        11994,
        476,
        22183,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.30949268526243934,
      "compression_ratio": 1.5866666666666667,
      "no_speech_prob": 1.2237626023647863e-8
    },
    {
      "id": 522,
      "seek": 356884,
      "start": 3568.84,
      "end": 3577.1600000000003,
      "text": " si diverte, c'è tutta una serie di lanciare i test di una code base in cui lavorano 10",
      "tokens": [
        1511,
        18558,
        975,
        11,
        269,
        6,
        1462,
        3672,
        1328,
        2002,
        23030,
        1026,
        9326,
        537,
        543,
        741,
        1500,
        1026,
        2002,
        3089,
        3096,
        294,
        22929,
        29241,
        3730,
        1266
      ],
      "temperature": 0,
      "avg_logprob": -0.29516735592403925,
      "compression_ratio": 1.543103448275862,
      "no_speech_prob": 4.450843107406399e-7
    },
    {
      "id": 523,
      "seek": 356884,
      "start": 3577.1600000000003,
      "end": 3585.96,
      "text": " team, aspetti un'ora e mezza per finirne tutti, quindi ci sono tutta una serie di vantaggi",
      "tokens": [
        1469,
        11,
        16817,
        12495,
        517,
        6,
        3252,
        308,
        385,
        26786,
        680,
        962,
        347,
        716,
        19822,
        11,
        15727,
        6983,
        9259,
        3672,
        1328,
        2002,
        23030,
        1026,
        371,
        394,
        46893
      ],
      "temperature": 0,
      "avg_logprob": -0.29516735592403925,
      "compression_ratio": 1.543103448275862,
      "no_speech_prob": 4.450843107406399e-7
    },
    {
      "id": 524,
      "seek": 356884,
      "start": 3585.96,
      "end": 3590.32,
      "text": " nello spezzettare e quindi la federazione ci dà quello che è fondamentalmente un sistema",
      "tokens": [
        408,
        1913,
        768,
        4313,
        3093,
        543,
        308,
        15727,
        635,
        38024,
        12928,
        6983,
        274,
        1467,
        22813,
        947,
        4873,
        9557,
        44538,
        4082,
        517,
        13245
      ],
      "temperature": 0,
      "avg_logprob": -0.29516735592403925,
      "compression_ratio": 1.543103448275862,
      "no_speech_prob": 4.450843107406399e-7
    },
    {
      "id": 525,
      "seek": 356884,
      "start": 3590.32,
      "end": 3598.44,
      "text": " di microservizi su GraphQL. Mercurius la fa bene, è ai livelli di completezza di quello",
      "tokens": [
        1026,
        15547,
        1978,
        24300,
        459,
        21884,
        13695,
        13,
        18897,
        374,
        4872,
        635,
        2050,
        2537,
        11,
        4873,
        9783,
        1621,
        16320,
        1026,
        715,
        2631,
        4371,
        2394,
        1026,
        22813
      ],
      "temperature": 0,
      "avg_logprob": -0.29516735592403925,
      "compression_ratio": 1.543103448275862,
      "no_speech_prob": 4.450843107406399e-7
    },
    {
      "id": 526,
      "seek": 359844,
      "start": 3598.44,
      "end": 3604.08,
      "text": " di Apollo? No, non lo è, perché l'hanno inventata quelli di Apollo, cioè le specifiche",
      "tokens": [
        1026,
        25187,
        30,
        883,
        11,
        2107,
        450,
        4873,
        11,
        14303,
        287,
        6,
        71,
        13484,
        7962,
        3274,
        631,
        16320,
        1026,
        25187,
        11,
        41827,
        476,
        1608,
        351,
        9304
      ],
      "temperature": 0,
      "avg_logprob": -0.38904502868652346,
      "compression_ratio": 1.7243243243243243,
      "no_speech_prob": 7.690374559388147e-7
    },
    {
      "id": 527,
      "seek": 359844,
      "start": 3604.08,
      "end": 3612,
      "text": " della federazione. Mercurius non implementa la federazione, implementa la federazione",
      "tokens": [
        11618,
        38024,
        12928,
        13,
        18897,
        374,
        4872,
        2107,
        4445,
        64,
        635,
        38024,
        12928,
        11,
        4445,
        64,
        635,
        38024,
        12928
      ],
      "temperature": 0,
      "avg_logprob": -0.38904502868652346,
      "compression_ratio": 1.7243243243243243,
      "no_speech_prob": 7.690374559388147e-7
    },
    {
      "id": 528,
      "seek": 359844,
      "start": 3612,
      "end": 3616.04,
      "text": " V1 di Apollo, quindi l'hanno inventata quelli di Apollo.",
      "tokens": [
        691,
        16,
        1026,
        25187,
        11,
        15727,
        287,
        6,
        71,
        13484,
        7962,
        3274,
        631,
        16320,
        1026,
        25187,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.38904502868652346,
      "compression_ratio": 1.7243243243243243,
      "no_speech_prob": 7.690374559388147e-7
    },
    {
      "id": 529,
      "seek": 359844,
      "start": 3616.04,
      "end": 3622.12,
      "text": " Ero finito delle specifiche per capire come cazzo funziona, ed è vero che stavano nel",
      "tokens": [
        462,
        340,
        962,
        3528,
        16485,
        1608,
        351,
        9304,
        680,
        1410,
        621,
        808,
        269,
        921,
        4765,
        49345,
        21758,
        11,
        308,
        67,
        4873,
        1306,
        78,
        947,
        342,
        706,
        3730,
        15373
      ],
      "temperature": 0,
      "avg_logprob": -0.38904502868652346,
      "compression_ratio": 1.7243243243243243,
      "no_speech_prob": 7.690374559388147e-7
    },
    {
      "id": 530,
      "seek": 362212,
      "start": 3622.12,
      "end": 3628.56,
      "text": " sito di Apollo. E ci sono cose, per esempio c'è qualcosina che non viene implementata,",
      "tokens": [
        1394,
        78,
        1026,
        25187,
        13,
        462,
        6983,
        9259,
        30261,
        11,
        680,
        33627,
        269,
        6,
        1462,
        4101,
        6877,
        1426,
        947,
        2107,
        19561,
        4445,
        3274,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.31905257911012885,
      "compression_ratio": 1.6322869955156951,
      "no_speech_prob": 3.4125798720197054e-7
    },
    {
      "id": 531,
      "seek": 362212,
      "start": 3628.56,
      "end": 3634.04,
      "text": " serve, io non ne ho avuto bisogno, le due volte in cui avevo bisogno di fare una roba",
      "tokens": [
        4596,
        11,
        19785,
        2107,
        408,
        1106,
        1305,
        8262,
        40505,
        1771,
        11,
        476,
        3462,
        37801,
        294,
        22929,
        3472,
        3080,
        40505,
        1771,
        1026,
        11994,
        2002,
        3870,
        64
      ],
      "temperature": 0,
      "avg_logprob": -0.31905257911012885,
      "compression_ratio": 1.6322869955156951,
      "no_speech_prob": 3.4125798720197054e-7
    },
    {
      "id": 532,
      "seek": 362212,
      "start": 3634.04,
      "end": 3642.52,
      "text": " che non era implementata, ce l'ha sia implementata. No, no, no, ci ho girato attorno senza problemi,",
      "tokens": [
        947,
        2107,
        4249,
        4445,
        3274,
        11,
        1769,
        287,
        6,
        1641,
        1511,
        64,
        4445,
        3274,
        13,
        883,
        11,
        572,
        11,
        572,
        11,
        6983,
        1106,
        14703,
        2513,
        951,
        21998,
        36208,
        1154,
        72,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.31905257911012885,
      "compression_ratio": 1.6322869955156951,
      "no_speech_prob": 3.4125798720197054e-7
    },
    {
      "id": 533,
      "seek": 362212,
      "start": 3642.52,
      "end": 3648.12,
      "text": " ancora all'epoca non avevo questa conoscenza della code base per metterci le mani, adesso",
      "tokens": [
        30656,
        439,
        6,
        595,
        24035,
        2107,
        3472,
        3080,
        16540,
        416,
        10466,
        23691,
        11618,
        3089,
        3096,
        680,
        1131,
        391,
        537,
        476,
        587,
        72,
        11,
        39552
      ],
      "temperature": 0,
      "avg_logprob": -0.31905257911012885,
      "compression_ratio": 1.6322869955156951,
      "no_speech_prob": 3.4125798720197054e-7
    },
    {
      "id": 534,
      "seek": 364812,
      "start": 3648.12,
      "end": 3652.3199999999997,
      "text": " probabilmente andrei a sistemare le cose. Però, per esempio, Mercurius fa la federazione",
      "tokens": [
        31959,
        4082,
        293,
        10271,
        257,
        45758,
        543,
        476,
        30261,
        13,
        20533,
        11,
        680,
        33627,
        11,
        18897,
        374,
        4872,
        2050,
        635,
        38024,
        12928
      ],
      "temperature": 0,
      "avg_logprob": -0.2628857704900926,
      "compression_ratio": 1.6901960784313725,
      "no_speech_prob": 7.112414550647372e-7
    },
    {
      "id": 535,
      "seek": 364812,
      "start": 3652.3199999999997,
      "end": 3657.92,
      "text": " sulle subscription, che Apollo non faceva, adesso non so se nel frattempo l'hanno implementata.",
      "tokens": [
        459,
        2447,
        17231,
        11,
        947,
        25187,
        2107,
        1851,
        2757,
        11,
        39552,
        2107,
        370,
        369,
        15373,
        431,
        1591,
        443,
        2259,
        287,
        6,
        71,
        13484,
        4445,
        3274,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.2628857704900926,
      "compression_ratio": 1.6901960784313725,
      "no_speech_prob": 7.112414550647372e-7
    },
    {
      "id": 536,
      "seek": 364812,
      "start": 3657.92,
      "end": 3662.7999999999997,
      "text": " Però tu puoi fare federazione sulle subscription usando Mercurius.",
      "tokens": [
        20533,
        2604,
        2362,
        4869,
        11994,
        38024,
        12928,
        459,
        2447,
        17231,
        29798,
        18897,
        374,
        4872,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.2628857704900926,
      "compression_ratio": 1.6901960784313725,
      "no_speech_prob": 7.112414550647372e-7
    },
    {
      "id": 537,
      "seek": 364812,
      "start": 3662.7999999999997,
      "end": 3669.3599999999997,
      "text": " Rapidamente, giusto per chi non lo sapesse, quando si parla di GraphQL si parla di query",
      "tokens": [
        44580,
        3439,
        11,
        1735,
        48260,
        680,
        13228,
        2107,
        450,
        18985,
        7357,
        11,
        7770,
        1511,
        971,
        875,
        1026,
        21884,
        13695,
        1511,
        971,
        875,
        1026,
        14581
      ],
      "temperature": 0,
      "avg_logprob": -0.2628857704900926,
      "compression_ratio": 1.6901960784313725,
      "no_speech_prob": 7.112414550647372e-7
    },
    {
      "id": 538,
      "seek": 364812,
      "start": 3669.3599999999997,
      "end": 3676.7999999999997,
      "text": " e mutation. Con le query accedi ai dati, con le mutation istruisci il server GraphQL per",
      "tokens": [
        308,
        27960,
        13,
        2656,
        476,
        14581,
        696,
        1232,
        72,
        9783,
        1137,
        72,
        11,
        416,
        476,
        27960,
        1418,
        894,
        271,
        537,
        1930,
        7154,
        21884,
        13695,
        680
      ],
      "temperature": 0,
      "avg_logprob": -0.2628857704900926,
      "compression_ratio": 1.6901960784313725,
      "no_speech_prob": 7.112414550647372e-7
    },
    {
      "id": 539,
      "seek": 367680,
      "start": 3676.8,
      "end": 3681.36,
      "text": " fare delle azioni. Non sto dicendo con la zappa, però fondamentalmente questo fanno.",
      "tokens": [
        11994,
        16485,
        7883,
        15273,
        13,
        8774,
        22784,
        14285,
        3999,
        416,
        635,
        710,
        25637,
        11,
        12673,
        9557,
        44538,
        4082,
        10263,
        283,
        13484,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.2459960253733509,
      "compression_ratio": 1.568888888888889,
      "no_speech_prob": 9.931191158329966e-8
    },
    {
      "id": 540,
      "seek": 367680,
      "start": 3681.36,
      "end": 3689.84,
      "text": " In più c'è un terzo elemento che è un query, possiamo vederlo come un query, che",
      "tokens": [
        682,
        10589,
        269,
        6,
        1462,
        517,
        1796,
        4765,
        47961,
        947,
        4873,
        517,
        14581,
        11,
        44758,
        371,
        10020,
        752,
        808,
        517,
        14581,
        11,
        947
      ],
      "temperature": 0,
      "avg_logprob": -0.2459960253733509,
      "compression_ratio": 1.568888888888889,
      "no_speech_prob": 9.931191158329966e-8
    },
    {
      "id": 541,
      "seek": 367680,
      "start": 3689.84,
      "end": 3697.1200000000003,
      "text": " in qualche modo parla attraverso un altro protocollo, possiamo immaginarlo come WebSocket,",
      "tokens": [
        294,
        38737,
        16664,
        971,
        875,
        951,
        424,
        331,
        539,
        517,
        40924,
        1742,
        905,
        22388,
        11,
        44758,
        3397,
        559,
        6470,
        752,
        808,
        9573,
        50,
        31380,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.2459960253733509,
      "compression_ratio": 1.568888888888889,
      "no_speech_prob": 9.931191158329966e-8
    },
    {
      "id": 542,
      "seek": 367680,
      "start": 3697.1200000000003,
      "end": 3702.6400000000003,
      "text": " e qui ti permette di rimanere in ascolto di qualcosa che poi ti arriva e non è strettamente",
      "tokens": [
        308,
        1956,
        8757,
        4784,
        3007,
        1026,
        15982,
        282,
        323,
        294,
        382,
        8768,
        1353,
        1026,
        42400,
        947,
        19260,
        8757,
        3399,
        2757,
        308,
        2107,
        4873,
        342,
        14313,
        3439
      ],
      "temperature": 0,
      "avg_logprob": -0.2459960253733509,
      "compression_ratio": 1.568888888888889,
      "no_speech_prob": 9.931191158329966e-8
    },
    {
      "id": 543,
      "seek": 370264,
      "start": 3702.64,
      "end": 3708,
      "text": " legato a un processo di richiesta risposta. L'ho detto con la zappa, Davide.",
      "tokens": [
        1676,
        2513,
        257,
        517,
        27939,
        1026,
        4593,
        38804,
        2253,
        79,
        8638,
        13,
        441,
        6,
        1289,
        41031,
        416,
        635,
        710,
        25637,
        11,
        413,
        706,
        482,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.3392559424378818,
      "compression_ratio": 1.5171102661596958,
      "no_speech_prob": 0.000008939646249928046
    },
    {
      "id": 544,
      "seek": 370264,
      "start": 3708,
      "end": 3714.64,
      "text": " Esatto, è un sistema di push che si usa con GraphQL. Tu ti metti in ascolto e lui ti manda",
      "tokens": [
        2313,
        37491,
        11,
        4873,
        517,
        13245,
        1026,
        2944,
        947,
        1511,
        29909,
        416,
        21884,
        13695,
        13,
        7836,
        8757,
        1131,
        7317,
        294,
        382,
        8768,
        1353,
        308,
        8783,
        8757,
        7411,
        64
      ],
      "temperature": 0,
      "avg_logprob": -0.3392559424378818,
      "compression_ratio": 1.5171102661596958,
      "no_speech_prob": 0.000008939646249928046
    },
    {
      "id": 545,
      "seek": 370264,
      "start": 3714.64,
      "end": 3717.68,
      "text": " i dati quando questi arrivano, quando vengono modificati.",
      "tokens": [
        741,
        1137,
        72,
        7770,
        29729,
        30697,
        3730,
        11,
        7770,
        371,
        1501,
        8957,
        1072,
        1089,
        6908,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.3392559424378818,
      "compression_ratio": 1.5171102661596958,
      "no_speech_prob": 0.000008939646249928046
    },
    {
      "id": 546,
      "seek": 370264,
      "start": 3717.68,
      "end": 3723.92,
      "text": " Esatto, è molto figo, devo dire che io l'ho utilizzato. Non mi è, in tutta sincerità,",
      "tokens": [
        2313,
        37491,
        11,
        4873,
        16394,
        2147,
        78,
        11,
        49717,
        1264,
        947,
        19785,
        287,
        6,
        1289,
        40355,
        2513,
        13,
        8774,
        2752,
        4873,
        11,
        294,
        3672,
        1328,
        30220,
        12445,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.3392559424378818,
      "compression_ratio": 1.5171102661596958,
      "no_speech_prob": 0.000008939646249928046
    },
    {
      "id": 547,
      "seek": 370264,
      "start": 3723.92,
      "end": 3729.16,
      "text": " ancora ben chiaro come farlo scalare, però su quello potremmo aprirci un capitolo.",
      "tokens": [
        30656,
        3271,
        47454,
        78,
        808,
        1400,
        752,
        15664,
        543,
        11,
        12673,
        459,
        22813,
        1847,
        265,
        2174,
        78,
        10992,
        347,
        537,
        517,
        33807,
        7902,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.3392559424378818,
      "compression_ratio": 1.5171102661596958,
      "no_speech_prob": 0.000008939646249928046
    },
    {
      "id": 548,
      "seek": 372916,
      "start": 3729.16,
      "end": 3734.64,
      "text": " Io non ho una grande esperienza su quelle, perché al solito, non so, sai un po' la teoria,",
      "tokens": [
        19239,
        2107,
        1106,
        2002,
        8883,
        10045,
        42331,
        459,
        29237,
        11,
        14303,
        419,
        1404,
        3528,
        11,
        2107,
        370,
        11,
        32417,
        517,
        714,
        6,
        635,
        535,
        8172,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.353784405210483,
      "compression_ratio": 1.691029900332226,
      "no_speech_prob": 0.000001308174660152872
    },
    {
      "id": 549,
      "seek": 372916,
      "start": 3734.64,
      "end": 3738.6,
      "text": " ma poi i progetti in cui l'hai usato non l'hai usato e quindi…",
      "tokens": [
        463,
        19260,
        741,
        447,
        847,
        7317,
        294,
        22929,
        287,
        6,
        18230,
        505,
        2513,
        2107,
        287,
        6,
        18230,
        505,
        2513,
        308,
        15727,
        1260
      ],
      "temperature": 0,
      "avg_logprob": -0.353784405210483,
      "compression_ratio": 1.691029900332226,
      "no_speech_prob": 0.000001308174660152872
    },
    {
      "id": 550,
      "seek": 372916,
      "start": 3738.6,
      "end": 3743.64,
      "text": " Esatto, non l'ho visto utilizzato in tanti progetti grossi in produzione, però forse",
      "tokens": [
        2313,
        37491,
        11,
        2107,
        287,
        6,
        1289,
        17558,
        40355,
        2513,
        294,
        256,
        11520,
        447,
        847,
        7317,
        11367,
        72,
        294,
        1082,
        19706,
        11,
        12673,
        337,
        405
      ],
      "temperature": 0,
      "avg_logprob": -0.353784405210483,
      "compression_ratio": 1.691029900332226,
      "no_speech_prob": 0.000001308174660152872
    },
    {
      "id": 551,
      "seek": 372916,
      "start": 3743.64,
      "end": 3748.48,
      "text": " è un mio limit e quindi la mia domanda è sempre quella, sì vabbè, ma se scala, come",
      "tokens": [
        4873,
        517,
        29908,
        4948,
        308,
        15727,
        635,
        21290,
        3285,
        5575,
        4873,
        9553,
        32234,
        11,
        49267,
        371,
        10797,
        1462,
        11,
        463,
        369,
        795,
        5159,
        11,
        808
      ],
      "temperature": 0,
      "avg_logprob": -0.353784405210483,
      "compression_ratio": 1.691029900332226,
      "no_speech_prob": 0.000001308174660152872
    },
    {
      "id": 552,
      "seek": 372916,
      "start": 3748.48,
      "end": 3753.64,
      "text": " scala e quanto scala? Ma questo forse è un po' una nostra deformazione professionale.",
      "tokens": [
        795,
        5159,
        308,
        17820,
        795,
        5159,
        30,
        4042,
        421,
        18465,
        337,
        405,
        4873,
        517,
        714,
        6,
        2002,
        34311,
        36094,
        12928,
        7032,
        1220,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.353784405210483,
      "compression_ratio": 1.691029900332226,
      "no_speech_prob": 0.000001308174660152872
    },
    {
      "id": 553,
      "seek": 372916,
      "start": 3753.64,
      "end": 3758.12,
      "text": " Prima di fare una cosa ci facciamo questo tipo di domande. Però voglio arrivare a un'altra",
      "tokens": [
        2114,
        4775,
        1026,
        11994,
        2002,
        10163,
        6983,
        1915,
        42052,
        10263,
        9746,
        1026,
        3285,
        11123,
        13,
        20533,
        31273,
        19987,
        30697,
        543,
        257,
        517,
        6,
        38865
      ],
      "temperature": 0,
      "avg_logprob": -0.353784405210483,
      "compression_ratio": 1.691029900332226,
      "no_speech_prob": 0.000001308174660152872
    },
    {
      "id": 554,
      "seek": 375812,
      "start": 3758.12,
      "end": 3764.3199999999997,
      "text": " cosa. Abbiamo parlato di GraphQL, abbiamo parlato di REST, ma il nuovo arrivato nella",
      "tokens": [
        10163,
        13,
        32673,
        7415,
        13734,
        2513,
        1026,
        21884,
        13695,
        11,
        22815,
        13734,
        2513,
        1026,
        497,
        14497,
        11,
        463,
        1930,
        49348,
        30697,
        2513,
        23878
      ],
      "temperature": 0,
      "avg_logprob": -0.18134826211368335,
      "compression_ratio": 1.4301075268817205,
      "no_speech_prob": 0.0000011544581184352865
    },
    {
      "id": 555,
      "seek": 375812,
      "start": 3764.3199999999997,
      "end": 3777.3199999999997,
      "text": " stanza è di RCP. Quindi il nuovo protocollo RCP tipizzato per TypeScript per fare la chiamata",
      "tokens": [
        342,
        20030,
        4873,
        1026,
        497,
        20049,
        13,
        32534,
        1930,
        49348,
        1742,
        905,
        22388,
        497,
        20049,
        4125,
        8072,
        2513,
        680,
        15576,
        14237,
        680,
        11994,
        635,
        417,
        2918,
        3274
      ],
      "temperature": 0,
      "avg_logprob": -0.18134826211368335,
      "compression_ratio": 1.4301075268817205,
      "no_speech_prob": 0.0000011544581184352865
    },
    {
      "id": 556,
      "seek": 375812,
      "start": 3777.3199999999997,
      "end": 3782.7799999999997,
      "text": " remota di funzioni con i tipi scerati tra back-end e front-end. Non voglio affrontare",
      "tokens": [
        890,
        5377,
        1026,
        49345,
        15273,
        416,
        741,
        4125,
        72,
        262,
        1776,
        6908,
        944,
        646,
        12,
        521,
        308,
        1868,
        12,
        521,
        13,
        8774,
        31273,
        19987,
        2096,
        10001,
        543
      ],
      "temperature": 0,
      "avg_logprob": -0.18134826211368335,
      "compression_ratio": 1.4301075268817205,
      "no_speech_prob": 0.0000011544581184352865
    },
    {
      "id": 557,
      "seek": 378278,
      "start": 3782.78,
      "end": 3789.0400000000004,
      "text": " il problema perché lo faremo in una prossima puntata, però una cosa che io ho sempre sentito",
      "tokens": [
        1930,
        12395,
        14303,
        450,
        11994,
        3280,
        294,
        2002,
        48794,
        4775,
        18212,
        3274,
        11,
        12673,
        2002,
        10163,
        947,
        19785,
        1106,
        9553,
        2279,
        3528
      ],
      "temperature": 0,
      "avg_logprob": -0.1895616513873459,
      "compression_ratio": 1.6727272727272726,
      "no_speech_prob": 5.15223028685341e-8
    },
    {
      "id": 558,
      "seek": 378278,
      "start": 3789.0400000000004,
      "end": 3798.32,
      "text": " a pelle dei protocolli RCP è che nascondono tutta la parte di operazioni sulla rete che",
      "tokens": [
        257,
        520,
        2447,
        13874,
        1742,
        905,
        1833,
        72,
        497,
        20049,
        4873,
        947,
        297,
        4806,
        684,
        8957,
        3672,
        1328,
        635,
        6975,
        1026,
        2208,
        27569,
        33625,
        319,
        975,
        947
      ],
      "temperature": 0,
      "avg_logprob": -0.1895616513873459,
      "compression_ratio": 1.6727272727272726,
      "no_speech_prob": 5.15223028685341e-8
    },
    {
      "id": 559,
      "seek": 378278,
      "start": 3798.32,
      "end": 3804.8,
      "text": " nel meccanismo di comunicazione inseriscono della complessità, della complessità dettata",
      "tokens": [
        15373,
        385,
        1914,
        282,
        6882,
        1026,
        31710,
        12928,
        1028,
        260,
        271,
        45846,
        11618,
        1209,
        442,
        12445,
        11,
        11618,
        1209,
        442,
        12445,
        1141,
        83,
        3274
      ],
      "temperature": 0,
      "avg_logprob": -0.1895616513873459,
      "compression_ratio": 1.6727272727272726,
      "no_speech_prob": 5.15223028685341e-8
    },
    {
      "id": 560,
      "seek": 378278,
      "start": 3804.8,
      "end": 3811.1200000000003,
      "text": " dal fatto che qualunque tipo di comunicazione sulla rete non è solo ti chiedo e mi rispondi,",
      "tokens": [
        11702,
        23228,
        947,
        4101,
        409,
        1077,
        9746,
        1026,
        31710,
        12928,
        33625,
        319,
        975,
        2107,
        4873,
        6944,
        8757,
        417,
        36035,
        308,
        2752,
        2253,
        79,
        684,
        72,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.1895616513873459,
      "compression_ratio": 1.6727272727272726,
      "no_speech_prob": 5.15223028685341e-8
    },
    {
      "id": 561,
      "seek": 381112,
      "start": 3811.12,
      "end": 3816.48,
      "text": " ma ci sono una serie di variabili che non controlliamo e che possono finire per darci",
      "tokens": [
        463,
        6983,
        9259,
        2002,
        23030,
        1026,
        3034,
        455,
        2312,
        947,
        2107,
        45159,
        7415,
        308,
        947,
        43857,
        962,
        621,
        680,
        4072,
        537
      ],
      "temperature": 0,
      "avg_logprob": -0.24348095209911616,
      "compression_ratio": 1.5309734513274336,
      "no_speech_prob": 3.8070120922384376e-7
    },
    {
      "id": 562,
      "seek": 381112,
      "start": 3816.48,
      "end": 3823.2799999999997,
      "text": " dei problemi. È GraphQL un po' questo tipo di complessità, specie con la federazione,",
      "tokens": [
        13874,
        1154,
        72,
        13,
        34495,
        21884,
        13695,
        517,
        714,
        6,
        10263,
        9746,
        1026,
        1209,
        442,
        12445,
        11,
        1608,
        414,
        416,
        635,
        38024,
        12928,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.24348095209911616,
      "compression_ratio": 1.5309734513274336,
      "no_speech_prob": 3.8070120922384376e-7
    },
    {
      "id": 563,
      "seek": 381112,
      "start": 3823.2799999999997,
      "end": 3829.52,
      "text": " quindi dove ho un nodo GraphQL a cui chiedo che poi si occupa in modo quasi da reverse",
      "tokens": [
        15727,
        23287,
        1106,
        517,
        15224,
        78,
        21884,
        13695,
        257,
        22929,
        417,
        36035,
        947,
        19260,
        1511,
        8073,
        64,
        294,
        16664,
        20954,
        1120,
        9943
      ],
      "temperature": 0,
      "avg_logprob": -0.24348095209911616,
      "compression_ratio": 1.5309734513274336,
      "no_speech_prob": 3.8070120922384376e-7
    },
    {
      "id": 564,
      "seek": 381112,
      "start": 3829.52,
      "end": 3837.2799999999997,
      "text": " proxy possiamo immaginarlo, di distribuire queste richieste per porzioni nei nodi che",
      "tokens": [
        29690,
        44758,
        3397,
        559,
        6470,
        752,
        11,
        1026,
        4400,
        43612,
        35455,
        4593,
        6495,
        68,
        680,
        1515,
        89,
        15273,
        34517,
        15224,
        72,
        947
      ],
      "temperature": 0,
      "avg_logprob": -0.24348095209911616,
      "compression_ratio": 1.5309734513274336,
      "no_speech_prob": 3.8070120922384376e-7
    },
    {
      "id": 565,
      "seek": 383728,
      "start": 3837.28,
      "end": 3842.6400000000003,
      "text": " mi devono rispondere un po' astrae, nasconde tutta la complessità anche diretta che c'è",
      "tokens": [
        2752,
        1905,
        8957,
        2253,
        79,
        33447,
        517,
        714,
        6,
        5357,
        424,
        68,
        11,
        297,
        4806,
        7259,
        3672,
        1328,
        635,
        1209,
        442,
        12445,
        11585,
        48196,
        1328,
        947,
        269,
        6,
        1462
      ],
      "temperature": 0,
      "avg_logprob": -0.25123367780520595,
      "compression_ratio": 1.453551912568306,
      "no_speech_prob": 8.10560223385437e-8
    },
    {
      "id": 566,
      "seek": 383728,
      "start": 3842.6400000000003,
      "end": 3853.1200000000003,
      "text": " tra i vari nodi. E quindi alla fine la mia domanda è come possiamo osservare quello",
      "tokens": [
        944,
        741,
        3034,
        15224,
        72,
        13,
        462,
        15727,
        11591,
        2489,
        635,
        21290,
        3285,
        5575,
        4873,
        808,
        44758,
        19508,
        1978,
        543,
        22813
      ],
      "temperature": 0,
      "avg_logprob": -0.25123367780520595,
      "compression_ratio": 1.453551912568306,
      "no_speech_prob": 8.10560223385437e-8
    },
    {
      "id": 567,
      "seek": 383728,
      "start": 3853.1200000000003,
      "end": 3862.1200000000003,
      "text": " che succede in un ecosistema GraphQL federato? Cioè come possiamo essere consapevoli delle",
      "tokens": [
        947,
        1965,
        29815,
        294,
        517,
        11007,
        468,
        5619,
        21884,
        13695,
        38024,
        2513,
        30,
        383,
        35983,
        808,
        44758,
        19799,
        1014,
        41153,
        85,
        9384,
        16485
      ],
      "temperature": 0,
      "avg_logprob": -0.25123367780520595,
      "compression_ratio": 1.453551912568306,
      "no_speech_prob": 8.10560223385437e-8
    },
    {
      "id": 568,
      "seek": 386212,
      "start": 3862.12,
      "end": 3871.3199999999997,
      "text": " merdate che da sistemisti possiamo aver fatto o dei lentezze dirette o dei problemi di questo",
      "tokens": [
        3551,
        17393,
        947,
        1120,
        45758,
        45308,
        44758,
        18247,
        23228,
        277,
        13874,
        287,
        1576,
        89,
        1381,
        1264,
        46739,
        277,
        13874,
        1154,
        72,
        1026,
        10263
      ],
      "temperature": 0,
      "avg_logprob": -0.3056373369126093,
      "compression_ratio": 1.4352331606217616,
      "no_speech_prob": 3.2563028184995346e-7
    },
    {
      "id": 569,
      "seek": 386212,
      "start": 3871.3199999999997,
      "end": 3879.92,
      "text": " tipo? Allora lì ci sono un po' di cose. Intanto bisogna avere dei DevOps coi controcasti,",
      "tokens": [
        9746,
        30,
        1057,
        3252,
        287,
        4749,
        6983,
        9259,
        517,
        714,
        6,
        1026,
        30261,
        13,
        5681,
        5857,
        40505,
        629,
        37914,
        13874,
        43051,
        598,
        72,
        660,
        340,
        3734,
        72,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.3056373369126093,
      "compression_ratio": 1.4352331606217616,
      "no_speech_prob": 3.2563028184995346e-7
    },
    {
      "id": 570,
      "seek": 386212,
      "start": 3879.92,
      "end": 3888.2799999999997,
      "text": " che è una cosa che spesso si sottovaluta, ma quello del DevOps bravo è un ruolo essenziale",
      "tokens": [
        947,
        4873,
        2002,
        10163,
        947,
        637,
        5557,
        1511,
        43754,
        3337,
        12093,
        11,
        463,
        22813,
        1103,
        43051,
        1548,
        3080,
        4873,
        517,
        5420,
        7902,
        2097,
        11368,
        25051
      ],
      "temperature": 0,
      "avg_logprob": -0.3056373369126093,
      "compression_ratio": 1.4352331606217616,
      "no_speech_prob": 3.2563028184995346e-7
    },
    {
      "id": 571,
      "seek": 388828,
      "start": 3888.28,
      "end": 3894.1200000000003,
      "text": " e quando hai un DevOps bravo tu tutte queste cose le sai perché c'è qualcuno che ti",
      "tokens": [
        308,
        7770,
        21822,
        517,
        43051,
        1548,
        3080,
        2604,
        38632,
        35455,
        30261,
        476,
        32417,
        14303,
        269,
        6,
        1462,
        32101,
        12638,
        947,
        8757
      ],
      "temperature": 0,
      "avg_logprob": -0.27104940169896835,
      "compression_ratio": 1.4606741573033708,
      "no_speech_prob": 1.8266202062022785e-7
    },
    {
      "id": 572,
      "seek": 388828,
      "start": 3894.1200000000003,
      "end": 3899.84,
      "text": " ha messo su un sistema come si deve. Poi sta nello sviluppatore andare a vedergli questi",
      "tokens": [
        324,
        2082,
        78,
        459,
        517,
        13245,
        808,
        1511,
        17761,
        13,
        430,
        4869,
        11135,
        408,
        1913,
        17342,
        388,
        10504,
        43148,
        42742,
        257,
        371,
        10020,
        41443,
        29729
      ],
      "temperature": 0,
      "avg_logprob": -0.27104940169896835,
      "compression_ratio": 1.4606741573033708,
      "no_speech_prob": 1.8266202062022785e-7
    },
    {
      "id": 573,
      "seek": 388828,
      "start": 3899.84,
      "end": 3907.76,
      "text": " dati perché usando tool come Datadog, si chiama, Grafana, c'è soprattutto una serie",
      "tokens": [
        1137,
        72,
        14303,
        29798,
        2290,
        808,
        9315,
        345,
        664,
        11,
        1511,
        13228,
        2404,
        11,
        8985,
        69,
        2095,
        11,
        269,
        6,
        1462,
        50002,
        2002,
        23030
      ],
      "temperature": 0,
      "avg_logprob": -0.27104940169896835,
      "compression_ratio": 1.4606741573033708,
      "no_speech_prob": 1.8266202062022785e-7
    },
    {
      "id": 574,
      "seek": 390776,
      "start": 3907.76,
      "end": 3919.84,
      "text": " di tool che fanno tante cose, ma sul sistema AWS stesso con Ray, come si chiama, qualcosa",
      "tokens": [
        1026,
        2290,
        947,
        283,
        13484,
        256,
        2879,
        30261,
        11,
        463,
        17603,
        13245,
        17650,
        44413,
        416,
        10883,
        11,
        808,
        1511,
        13228,
        2404,
        11,
        42400
      ],
      "temperature": 0,
      "avg_logprob": -0.4936800457182385,
      "compression_ratio": 1.39,
      "no_speech_prob": 5.8382418899327604e-8
    },
    {
      "id": 575,
      "seek": 390776,
      "start": 3919.84,
      "end": 3926.4,
      "text": " di realistico. Non lo so, da un anno che uso Azure, quindi posso dire. Sono anche certificato",
      "tokens": [
        1026,
        957,
        468,
        2789,
        13,
        8774,
        450,
        370,
        11,
        1120,
        517,
        46277,
        947,
        22728,
        11969,
        11,
        15727,
        22501,
        1264,
        13,
        48344,
        11585,
        12378,
        2513
      ],
      "temperature": 0,
      "avg_logprob": -0.4936800457182385,
      "compression_ratio": 1.39,
      "no_speech_prob": 5.8382418899327604e-8
    },
    {
      "id": 576,
      "seek": 390776,
      "start": 3926.4,
      "end": 3936,
      "text": " AWS, il nome proprio non me lo ricordo. Vabbè frega, ci sono più di 300 servizi AWS. Esatto,",
      "tokens": [
        17650,
        11,
        1930,
        19003,
        28203,
        2107,
        385,
        450,
        21040,
        23872,
        13,
        691,
        10797,
        1462,
        2130,
        3680,
        11,
        6983,
        9259,
        10589,
        1026,
        6641,
        1658,
        24300,
        17650,
        13,
        2313,
        37491,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.4936800457182385,
      "compression_ratio": 1.39,
      "no_speech_prob": 5.8382418899327604e-8
    },
    {
      "id": 577,
      "seek": 393600,
      "start": 3936,
      "end": 3940.44,
      "text": " però tramite CloudWatch, c'è un sistema che si chiama Ray, è qualcosa che ti permette",
      "tokens": [
        12673,
        25749,
        642,
        8061,
        36204,
        11,
        269,
        6,
        1462,
        517,
        13245,
        947,
        1511,
        13228,
        2404,
        10883,
        11,
        4873,
        42400,
        947,
        8757,
        4784,
        3007
      ],
      "temperature": 0,
      "avg_logprob": -0.3665002822875977,
      "compression_ratio": 1.6173285198555956,
      "no_speech_prob": 5.453278504319314e-7
    },
    {
      "id": 578,
      "seek": 393600,
      "start": 3940.44,
      "end": 3946.04,
      "text": " di vedere il giro che ha fatto la richiesta per i vari servizi. Comunque con Datadog,",
      "tokens": [
        1026,
        35373,
        1930,
        1735,
        340,
        947,
        324,
        23228,
        635,
        4593,
        38804,
        680,
        741,
        3034,
        1658,
        24300,
        13,
        2432,
        409,
        1077,
        416,
        9315,
        345,
        664,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.3665002822875977,
      "compression_ratio": 1.6173285198555956,
      "no_speech_prob": 5.453278504319314e-7
    },
    {
      "id": 579,
      "seek": 393600,
      "start": 3946.04,
      "end": 3951.6,
      "text": " con Celsion, sono tutti una serie di servizi che ti danno le risposte. Quindi con Datadog",
      "tokens": [
        416,
        383,
        1625,
        313,
        11,
        9259,
        19822,
        2002,
        23030,
        1026,
        1658,
        24300,
        947,
        8757,
        3594,
        78,
        476,
        2253,
        23744,
        68,
        13,
        32534,
        416,
        9315,
        345,
        664
      ],
      "temperature": 0,
      "avg_logprob": -0.3665002822875977,
      "compression_ratio": 1.6173285198555956,
      "no_speech_prob": 5.453278504319314e-7
    },
    {
      "id": 580,
      "seek": 393600,
      "start": 3951.6,
      "end": 3958.8,
      "text": " tu puoi andare a vedere, dato una richiesta, qual è l'acquiri MySQL, se il sistema ha",
      "tokens": [
        2604,
        2362,
        4869,
        42742,
        257,
        35373,
        11,
        46971,
        2002,
        4593,
        38804,
        11,
        4101,
        4873,
        287,
        6,
        326,
        358,
        12988,
        1222,
        39934,
        11,
        369,
        1930,
        13245,
        324
      ],
      "temperature": 0,
      "avg_logprob": -0.3665002822875977,
      "compression_ratio": 1.6173285198555956,
      "no_speech_prob": 5.453278504319314e-7
    },
    {
      "id": 581,
      "seek": 393600,
      "start": 3958.8,
      "end": 3965.48,
      "text": " fatto bene, che ti ha rallentato con la richiesta. Può fare anche App Insight. Esatto, immagino",
      "tokens": [
        23228,
        2537,
        11,
        947,
        8757,
        324,
        31552,
        317,
        2513,
        416,
        635,
        4593,
        38804,
        13,
        13605,
        4293,
        11994,
        11585,
        3132,
        9442,
        397,
        13,
        2313,
        37491,
        11,
        3397,
        559,
        2982
      ],
      "temperature": 0,
      "avg_logprob": -0.3665002822875977,
      "compression_ratio": 1.6173285198555956,
      "no_speech_prob": 5.453278504319314e-7
    },
    {
      "id": 582,
      "seek": 396548,
      "start": 3965.48,
      "end": 3972.84,
      "text": " che lo facciano. Cioè, sono alla base queste cose. Quindi le metriche, tutta una roba essenziale.",
      "tokens": [
        947,
        450,
        1915,
        537,
        3730,
        13,
        383,
        35983,
        11,
        9259,
        11591,
        3096,
        35455,
        30261,
        13,
        32534,
        476,
        1131,
        81,
        9304,
        11,
        3672,
        1328,
        2002,
        3870,
        64,
        2097,
        11368,
        25051,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.36909678830938825,
      "compression_ratio": 1.5659574468085107,
      "no_speech_prob": 2.102431153616635e-7
    },
    {
      "id": 583,
      "seek": 396548,
      "start": 3972.84,
      "end": 3980.88,
      "text": " Adesso sto lavorando, lo ricito, con Simone Sanfratello, che è molto bravo da questo",
      "tokens": [
        1999,
        5557,
        22784,
        29241,
        1806,
        11,
        450,
        21040,
        3528,
        11,
        416,
        41652,
        5271,
        5779,
        473,
        1913,
        11,
        947,
        4873,
        16394,
        1548,
        3080,
        1120,
        10263
      ],
      "temperature": 0,
      "avg_logprob": -0.36909678830938825,
      "compression_ratio": 1.5659574468085107,
      "no_speech_prob": 2.102431153616635e-7
    },
    {
      "id": 584,
      "seek": 396548,
      "start": 3980.88,
      "end": 3988.2400000000002,
      "text": " punto di vista. Cioè lui su metriche, sull'andare a dire a questi sistemi che cosa sta succedendo",
      "tokens": [
        14326,
        1026,
        22553,
        13,
        383,
        35983,
        8783,
        459,
        1131,
        81,
        9304,
        11,
        459,
        285,
        6,
        474,
        543,
        257,
        1264,
        257,
        29729,
        10555,
        13372,
        947,
        10163,
        11135,
        1965,
        1232,
        3999
      ],
      "temperature": 0,
      "avg_logprob": -0.36909678830938825,
      "compression_ratio": 1.5659574468085107,
      "no_speech_prob": 2.102431153616635e-7
    },
    {
      "id": 585,
      "seek": 396548,
      "start": 3988.2400000000002,
      "end": 3992,
      "text": " dentro il codice, è proprio una cosa che gli piace, si vede, gli brillano gli occhi",
      "tokens": [
        10856,
        1930,
        17656,
        573,
        11,
        4873,
        28203,
        2002,
        10163,
        947,
        17161,
        50062,
        11,
        1511,
        371,
        4858,
        11,
        17161,
        738,
        388,
        75,
        3730,
        17161,
        10409,
        8036
      ],
      "temperature": 0,
      "avg_logprob": -0.36909678830938825,
      "compression_ratio": 1.5659574468085107,
      "no_speech_prob": 2.102431153616635e-7
    },
    {
      "id": 586,
      "seek": 399200,
      "start": 3992,
      "end": 3998.32,
      "text": " quando ti racconta queste cose. Tanto sarà ospite prossimamentissimo, quindi lo sentirete",
      "tokens": [
        7770,
        8757,
        4129,
        9000,
        64,
        35455,
        30261,
        13,
        314,
        5857,
        41338,
        3003,
        79,
        642,
        48794,
        332,
        2466,
        34966,
        11,
        15727,
        450,
        2279,
        621,
        975
      ],
      "temperature": 0,
      "avg_logprob": -0.25208983363875426,
      "compression_ratio": 1.5027932960893855,
      "no_speech_prob": 5.043468149779073e-7
    },
    {
      "id": 587,
      "seek": 399200,
      "start": 3998.32,
      "end": 4008.72,
      "text": " tra pochissimo. Quindi lui è proprio bravo. Io per esempio sono più sull'architettura,",
      "tokens": [
        944,
        714,
        339,
        34966,
        13,
        32534,
        8783,
        4873,
        28203,
        1548,
        3080,
        13,
        19239,
        680,
        33627,
        9259,
        10589,
        459,
        285,
        6,
        1178,
        270,
        3093,
        2991,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.25208983363875426,
      "compression_ratio": 1.5027932960893855,
      "no_speech_prob": 5.043468149779073e-7
    },
    {
      "id": 588,
      "seek": 399200,
      "start": 4008.72,
      "end": 4016.44,
      "text": " su queste cose ad alto livello, lui sul basso livello è un fenomeno. Quando metti i dati",
      "tokens": [
        459,
        35455,
        30261,
        614,
        21275,
        1621,
        1913,
        11,
        8783,
        17603,
        987,
        539,
        1621,
        1913,
        4873,
        517,
        26830,
        4726,
        78,
        13,
        18725,
        1131,
        7317,
        741,
        1137,
        72
      ],
      "temperature": 0,
      "avg_logprob": -0.25208983363875426,
      "compression_ratio": 1.5027932960893855,
      "no_speech_prob": 5.043468149779073e-7
    },
    {
      "id": 589,
      "seek": 401644,
      "start": 4016.44,
      "end": 4023.04,
      "text": " come si deve, tu recuperi tutte le informazioni che ti servono. Quindi sai esattamente le",
      "tokens": [
        808,
        1511,
        17761,
        11,
        2604,
        25692,
        72,
        38632,
        476,
        1356,
        27569,
        947,
        8757,
        1658,
        8957,
        13,
        32534,
        32417,
        785,
        1591,
        3439,
        476
      ],
      "temperature": 0,
      "avg_logprob": -0.277870914392304,
      "compression_ratio": 1.6451612903225807,
      "no_speech_prob": 3.0590183541789884e-7
    },
    {
      "id": 590,
      "seek": 401644,
      "start": 4023.04,
      "end": 4030.4,
      "text": " cose come funzionano. Poi dove non ci arriva l'infrastruttura, inteso come la VU, cioè",
      "tokens": [
        30261,
        808,
        49345,
        313,
        3730,
        13,
        430,
        4869,
        23287,
        2107,
        6983,
        3399,
        2757,
        287,
        6,
        19920,
        4148,
        81,
        13478,
        2991,
        11,
        560,
        41189,
        808,
        635,
        691,
        52,
        11,
        41827
      ],
      "temperature": 0,
      "avg_logprob": -0.277870914392304,
      "compression_ratio": 1.6451612903225807,
      "no_speech_prob": 3.0590183541789884e-7
    },
    {
      "id": 591,
      "seek": 401644,
      "start": 4030.4,
      "end": 4036.2000000000003,
      "text": " l'infrastruttura, dove non ci arriva il tooling fatto da altri, ci si smazza e si inventano",
      "tokens": [
        287,
        6,
        19920,
        4148,
        81,
        13478,
        2991,
        11,
        23287,
        2107,
        6983,
        3399,
        2757,
        1930,
        46593,
        23228,
        1120,
        33707,
        11,
        6983,
        1511,
        899,
        921,
        2394,
        308,
        1511,
        7962,
        3730
      ],
      "temperature": 0,
      "avg_logprob": -0.277870914392304,
      "compression_ratio": 1.6451612903225807,
      "no_speech_prob": 3.0590183541789884e-7
    },
    {
      "id": 592,
      "seek": 401644,
      "start": 4036.2000000000003,
      "end": 4042.92,
      "text": " cose. Io ho finito il progetto dopo un anno in cui sono passato sul nuovo progetto, nei",
      "tokens": [
        30261,
        13,
        19239,
        1106,
        962,
        3528,
        1930,
        447,
        847,
        1353,
        35196,
        517,
        46277,
        294,
        22929,
        9259,
        1320,
        2513,
        17603,
        49348,
        447,
        847,
        1353,
        11,
        34517
      ],
      "temperature": 0,
      "avg_logprob": -0.277870914392304,
      "compression_ratio": 1.6451612903225807,
      "no_speech_prob": 3.0590183541789884e-7
    },
    {
      "id": 593,
      "seek": 404292,
      "start": 4042.92,
      "end": 4049.6800000000003,
      "text": " due mesi di pausa fra un progetto e l'altro, ho provato, anzi l'abbiamo fatto perché",
      "tokens": [
        3462,
        3813,
        72,
        1026,
        2502,
        20318,
        6600,
        517,
        447,
        847,
        1353,
        308,
        287,
        6,
        47484,
        11,
        1106,
        1439,
        2513,
        11,
        364,
        3992,
        287,
        6,
        10797,
        7415,
        23228,
        14303
      ],
      "temperature": 0,
      "avg_logprob": -0.2785211835588728,
      "compression_ratio": 1.6255924170616114,
      "no_speech_prob": 1.522997017389116e-8
    },
    {
      "id": 594,
      "seek": 404292,
      "start": 4049.6800000000003,
      "end": 4055.64,
      "text": " l'abbiamo costruito, ho provato a fare un po' di tooling proprio per Mercurius per",
      "tokens": [
        287,
        6,
        10797,
        7415,
        2063,
        894,
        3528,
        11,
        1106,
        1439,
        2513,
        257,
        11994,
        517,
        714,
        6,
        1026,
        46593,
        28203,
        680,
        18897,
        374,
        4872,
        680
      ],
      "temperature": 0,
      "avg_logprob": -0.2785211835588728,
      "compression_ratio": 1.6255924170616114,
      "no_speech_prob": 1.522997017389116e-8
    },
    {
      "id": 595,
      "seek": 404292,
      "start": 4055.64,
      "end": 4064,
      "text": " andare incontro a queste cose. Quindi insieme ad altri ragazzi di NearForm, noi l'azienda",
      "tokens": [
        42742,
        834,
        896,
        340,
        257,
        35455,
        30261,
        13,
        32534,
        1028,
        44940,
        614,
        33707,
        17539,
        33910,
        1026,
        22200,
        49855,
        11,
        22447,
        287,
        6,
        921,
        30498
      ],
      "temperature": 0,
      "avg_logprob": -0.2785211835588728,
      "compression_ratio": 1.6255924170616114,
      "no_speech_prob": 1.522997017389116e-8
    },
    {
      "id": 596,
      "seek": 404292,
      "start": 4064,
      "end": 4068.16,
      "text": " abbiamo questa grande possibilità di fare un sacco di open source fra un progetto e",
      "tokens": [
        22815,
        16540,
        8883,
        24145,
        12445,
        1026,
        11994,
        517,
        4899,
        1291,
        1026,
        1269,
        4009,
        6600,
        517,
        447,
        847,
        1353,
        308
      ],
      "temperature": 0,
      "avg_logprob": -0.2785211835588728,
      "compression_ratio": 1.6255924170616114,
      "no_speech_prob": 1.522997017389116e-8
    },
    {
      "id": 597,
      "seek": 406816,
      "start": 4068.16,
      "end": 4073.92,
      "text": " l'altro. Abbiamo sviluppato questi due plugin, uno che è dato una federazione e ti racconta",
      "tokens": [
        287,
        6,
        47484,
        13,
        32673,
        7415,
        17342,
        388,
        10504,
        2513,
        29729,
        3462,
        23407,
        11,
        8526,
        947,
        4873,
        46971,
        2002,
        38024,
        12928,
        308,
        8757,
        4129,
        9000,
        64
      ],
      "temperature": 0,
      "avg_logprob": -0.28988025905369047,
      "compression_ratio": 1.7325581395348837,
      "no_speech_prob": 9.422381026524818e-7
    },
    {
      "id": 598,
      "seek": 406816,
      "start": 4073.92,
      "end": 4078.52,
      "text": " com'è fatta, cosa che in Mercurius mancava, non so se c'è in Apollo, ma adesso abbiamo",
      "tokens": [
        395,
        6,
        1462,
        4046,
        1328,
        11,
        10163,
        947,
        294,
        18897,
        374,
        4872,
        587,
        496,
        2757,
        11,
        2107,
        370,
        369,
        269,
        6,
        1462,
        294,
        25187,
        11,
        463,
        39552,
        22815
      ],
      "temperature": 0,
      "avg_logprob": -0.28988025905369047,
      "compression_ratio": 1.7325581395348837,
      "no_speech_prob": 9.422381026524818e-7
    },
    {
      "id": 599,
      "seek": 406816,
      "start": 4078.52,
      "end": 4085.2,
      "text": " un sistema che è dato una federazione e ti dice il dato X in quale servizio è sviluppato.",
      "tokens": [
        517,
        13245,
        947,
        4873,
        46971,
        2002,
        38024,
        12928,
        308,
        8757,
        10313,
        1930,
        46971,
        1783,
        294,
        421,
        1220,
        1658,
        590,
        1004,
        4873,
        17342,
        388,
        10504,
        2513,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.28988025905369047,
      "compression_ratio": 1.7325581395348837,
      "no_speech_prob": 9.422381026524818e-7
    },
    {
      "id": 600,
      "seek": 406816,
      "start": 4085.2,
      "end": 4091.6,
      "text": " Sembra una scematta, ma per chi ci lavora è essenziale, soprattutto quando hai cinque",
      "tokens": [
        14421,
        6198,
        2002,
        262,
        384,
        15677,
        1328,
        11,
        463,
        680,
        13228,
        6983,
        20923,
        3252,
        4873,
        2097,
        11368,
        25051,
        11,
        50002,
        7770,
        21822,
        6539,
        1077
      ],
      "temperature": 0,
      "avg_logprob": -0.28988025905369047,
      "compression_ratio": 1.7325581395348837,
      "no_speech_prob": 9.422381026524818e-7
    },
    {
      "id": 601,
      "seek": 406816,
      "start": 4091.6,
      "end": 4095.72,
      "text": " team che lavorano ai nodi. Tu vedi una cosa e dici ma chi è che lavora a questo nodo,",
      "tokens": [
        1469,
        947,
        29241,
        3730,
        9783,
        15224,
        72,
        13,
        7836,
        371,
        10323,
        2002,
        10163,
        308,
        274,
        8787,
        463,
        13228,
        4873,
        947,
        20923,
        3252,
        257,
        10263,
        15224,
        78,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.28988025905369047,
      "compression_ratio": 1.7325581395348837,
      "no_speech_prob": 9.422381026524818e-7
    },
    {
      "id": 602,
      "seek": 409572,
      "start": 4095.72,
      "end": 4098.88,
      "text": " dà un'occhiata che lo chiedo a tizio. Quella è una roba che tu se vedi solo lo schema",
      "tokens": [
        274,
        1467,
        517,
        6,
        905,
        8036,
        3274,
        947,
        450,
        417,
        36035,
        257,
        256,
        590,
        1004,
        13,
        4493,
        3505,
        4873,
        2002,
        3870,
        64,
        947,
        2604,
        369,
        371,
        10323,
        6944,
        450,
        34078
      ],
      "temperature": 0,
      "avg_logprob": -0.3796225526279077,
      "compression_ratio": 1.6654275092936803,
      "no_speech_prob": 5.57089059327609e-8
    },
    {
      "id": 603,
      "seek": 409572,
      "start": 4098.88,
      "end": 4103.36,
      "text": " finale non sai dove questa cosa viene fatta. Questa è la prima parte, prima cosa che abbiamo",
      "tokens": [
        23510,
        2107,
        32417,
        23287,
        16540,
        10163,
        19561,
        4046,
        1328,
        13,
        2326,
        7841,
        4873,
        635,
        19507,
        6975,
        11,
        19507,
        10163,
        947,
        22815
      ],
      "temperature": 0,
      "avg_logprob": -0.3796225526279077,
      "compression_ratio": 1.6654275092936803,
      "no_speech_prob": 5.57089059327609e-8
    },
    {
      "id": 604,
      "seek": 409572,
      "start": 4103.36,
      "end": 4111.96,
      "text": " fatto e l'altra, una cosa essenziale, è che abbiamo fatto un plugin che ti restituisce",
      "tokens": [
        23228,
        308,
        287,
        6,
        38865,
        11,
        2002,
        10163,
        2097,
        11368,
        25051,
        11,
        4873,
        947,
        22815,
        23228,
        517,
        23407,
        947,
        8757,
        1472,
        6380,
        49596
      ],
      "temperature": 0,
      "avg_logprob": -0.3796225526279077,
      "compression_ratio": 1.6654275092936803,
      "no_speech_prob": 5.57089059327609e-8
    },
    {
      "id": 605,
      "seek": 409572,
      "start": 4111.96,
      "end": 4116.4,
      "text": " le metriche della query che hai fatto. Quindi tu fai una query e gli dici esattamente che",
      "tokens": [
        476,
        1131,
        81,
        9304,
        11618,
        14581,
        947,
        21822,
        23228,
        13,
        32534,
        2604,
        283,
        1301,
        2002,
        14581,
        308,
        17161,
        274,
        8787,
        785,
        1591,
        3439,
        947
      ],
      "temperature": 0,
      "avg_logprob": -0.3796225526279077,
      "compression_ratio": 1.6654275092936803,
      "no_speech_prob": 5.57089059327609e-8
    },
    {
      "id": 606,
      "seek": 409572,
      "start": 4116.4,
      "end": 4124.5599999999995,
      "text": " cosa è successo all'interno del sistema, che il resolver X ci ha messo 200 millisecondi",
      "tokens": [
        10163,
        4873,
        2245,
        78,
        439,
        6,
        5106,
        1771,
        1103,
        13245,
        11,
        947,
        1930,
        34480,
        1783,
        6983,
        324,
        2082,
        78,
        2331,
        27940,
        18882,
        72
      ],
      "temperature": 0,
      "avg_logprob": -0.3796225526279077,
      "compression_ratio": 1.6654275092936803,
      "no_speech_prob": 5.57089059327609e-8
    },
    {
      "id": 607,
      "seek": 412456,
      "start": 4124.56,
      "end": 4128.92,
      "text": " ed è stato chiamato cinque volte. Tu dici come mai è stato chiamato cinque volte e",
      "tokens": [
        1257,
        4873,
        29657,
        417,
        2918,
        2513,
        6539,
        1077,
        37801,
        13,
        7836,
        274,
        8787,
        808,
        12698,
        4873,
        29657,
        417,
        2918,
        2513,
        6539,
        1077,
        37801,
        308
      ],
      "temperature": 0,
      "avg_logprob": -0.2728556597674334,
      "compression_ratio": 1.7768924302788844,
      "no_speech_prob": 0.0000067480382313078735
    },
    {
      "id": 608,
      "seek": 412456,
      "start": 4128.92,
      "end": 4133.4400000000005,
      "text": " vai a vedere e dici che qualcosa non è andato. Allora vai un po' a sistemare e ti rendi",
      "tokens": [
        4405,
        257,
        35373,
        308,
        274,
        8787,
        947,
        42400,
        2107,
        4873,
        293,
        2513,
        13,
        1057,
        3252,
        4405,
        517,
        714,
        6,
        257,
        45758,
        543,
        308,
        8757,
        6125,
        72
      ],
      "temperature": 0,
      "avg_logprob": -0.2728556597674334,
      "compression_ratio": 1.7768924302788844,
      "no_speech_prob": 0.0000067480382313078735
    },
    {
      "id": 609,
      "seek": 412456,
      "start": 4133.4400000000005,
      "end": 4139.68,
      "text": " conto che spesso i problemi sono nel codice. Oppure vedi che il resolver ci mette troppo",
      "tokens": [
        660,
        78,
        947,
        637,
        5557,
        741,
        1154,
        72,
        9259,
        15373,
        17656,
        573,
        13,
        15666,
        540,
        371,
        10323,
        947,
        1930,
        34480,
        6983,
        1131,
        975,
        4495,
        27000
      ],
      "temperature": 0,
      "avg_logprob": -0.2728556597674334,
      "compression_ratio": 1.7768924302788844,
      "no_speech_prob": 0.0000067480382313078735
    },
    {
      "id": 610,
      "seek": 412456,
      "start": 4139.68,
      "end": 4144.4800000000005,
      "text": " tempo e viene chiamato troppo spesso e quindi sai che quel resolver deve andare a sistemare",
      "tokens": [
        8972,
        308,
        19561,
        417,
        2918,
        2513,
        4495,
        27000,
        637,
        5557,
        308,
        15727,
        32417,
        947,
        7178,
        34480,
        17761,
        42742,
        257,
        45758,
        543
      ],
      "temperature": 0,
      "avg_logprob": -0.2728556597674334,
      "compression_ratio": 1.7768924302788844,
      "no_speech_prob": 0.0000067480382313078735
    },
    {
      "id": 611,
      "seek": 412456,
      "start": 4144.4800000000005,
      "end": 4149.04,
      "text": " le cose. Torniamo a quello che ti avevo detto inizialmente su GraphQL, il fatto che tu puoi",
      "tokens": [
        476,
        30261,
        13,
        314,
        1865,
        7415,
        257,
        22813,
        947,
        8757,
        3472,
        3080,
        41031,
        294,
        590,
        831,
        4082,
        459,
        21884,
        13695,
        11,
        1930,
        23228,
        947,
        2604,
        2362,
        4869
      ],
      "temperature": 0,
      "avg_logprob": -0.2728556597674334,
      "compression_ratio": 1.7768924302788844,
      "no_speech_prob": 0.0000067480382313078735
    },
    {
      "id": 612,
      "seek": 414904,
      "start": 4149.04,
      "end": 4156.12,
      "text": " concentrare su un singolo resolver è una grande cosa da questo punto di vista perché",
      "tokens": [
        5512,
        35559,
        459,
        517,
        1522,
        7902,
        34480,
        4873,
        2002,
        8883,
        10163,
        1120,
        10263,
        14326,
        1026,
        22553,
        14303
      ],
      "temperature": 0,
      "avg_logprob": -0.3159187835397072,
      "compression_ratio": 1.5283842794759825,
      "no_speech_prob": 5.0723535593988345e-8
    },
    {
      "id": 613,
      "seek": 414904,
      "start": 4156.12,
      "end": 4162.12,
      "text": " riduci la complessità. Esatto, riduci la complessità. Poi a volte risolvi con una",
      "tokens": [
        3973,
        34144,
        635,
        1209,
        442,
        12445,
        13,
        2313,
        37491,
        11,
        3973,
        34144,
        635,
        1209,
        442,
        12445,
        13,
        430,
        4869,
        257,
        37801,
        2253,
        401,
        4917,
        416,
        2002
      ],
      "temperature": 0,
      "avg_logprob": -0.3159187835397072,
      "compression_ratio": 1.5283842794759825,
      "no_speech_prob": 5.0723535593988345e-8
    },
    {
      "id": 614,
      "seek": 414904,
      "start": 4162.12,
      "end": 4169.48,
      "text": " bella cache, tutti i sanity ti aiutano, quindi Redis è un sidecar che deve essere sempre",
      "tokens": [
        312,
        3505,
        19459,
        11,
        19822,
        741,
        47892,
        8757,
        9783,
        325,
        3730,
        11,
        15727,
        4477,
        271,
        4873,
        517,
        1252,
        6166,
        947,
        17761,
        19799,
        9553
      ],
      "temperature": 0,
      "avg_logprob": -0.3159187835397072,
      "compression_ratio": 1.5283842794759825,
      "no_speech_prob": 5.0723535593988345e-8
    },
    {
      "id": 615,
      "seek": 414904,
      "start": 4169.48,
      "end": 4176.92,
      "text": " lì. Tra l'altro un saluto a Salvatore che ci sta ascoltando. Io ricordo ancora una frase",
      "tokens": [
        287,
        4749,
        13,
        5403,
        287,
        6,
        47484,
        517,
        1845,
        8262,
        257,
        5996,
        23352,
        418,
        947,
        6983,
        11135,
        15526,
        4837,
        1806,
        13,
        19239,
        21040,
        23872,
        30656,
        2002,
        38406
      ],
      "temperature": 0,
      "avg_logprob": -0.3159187835397072,
      "compression_ratio": 1.5283842794759825,
      "no_speech_prob": 5.0723535593988345e-8
    },
    {
      "id": 616,
      "seek": 417692,
      "start": 4176.92,
      "end": 4183.04,
      "text": " di Matteo Collina che disse se io dovessi dire su cosa è basato il 90% della mia carriera",
      "tokens": [
        1026,
        47544,
        78,
        4586,
        1426,
        947,
        17581,
        369,
        19785,
        30870,
        442,
        72,
        1264,
        459,
        10163,
        4873,
        987,
        2513,
        1930,
        4289,
        4,
        11618,
        21290,
        1032,
        470,
        1663
      ],
      "temperature": 0,
      "avg_logprob": -0.2930688967649964,
      "compression_ratio": 1.421875,
      "no_speech_prob": 1.0087580903928028e-7
    },
    {
      "id": 617,
      "seek": 417692,
      "start": 4183.04,
      "end": 4189.8,
      "text": " in termini di performance rispondere con una sola parola, Redis. Ha scritto anche un tweet",
      "tokens": [
        294,
        1433,
        3812,
        1026,
        3389,
        2253,
        79,
        33447,
        416,
        2002,
        34162,
        971,
        4711,
        11,
        4477,
        271,
        13,
        4064,
        5918,
        34924,
        11585,
        517,
        15258
      ],
      "temperature": 0,
      "avg_logprob": -0.2930688967649964,
      "compression_ratio": 1.421875,
      "no_speech_prob": 1.0087580903928028e-7
    },
    {
      "id": 618,
      "seek": 417692,
      "start": 4189.8,
      "end": 4201.64,
      "text": " su questo. Io l'ho sempre usato molto, molto, diciamo di paro dei Redis. Un po' come scelta",
      "tokens": [
        459,
        10263,
        13,
        19239,
        287,
        6,
        1289,
        9553,
        505,
        2513,
        16394,
        11,
        16394,
        11,
        14285,
        7415,
        1026,
        971,
        78,
        13874,
        4477,
        271,
        13,
        1156,
        714,
        6,
        808,
        795,
        338,
        1328
      ],
      "temperature": 0,
      "avg_logprob": -0.2930688967649964,
      "compression_ratio": 1.421875,
      "no_speech_prob": 1.0087580903928028e-7
    },
    {
      "id": 619,
      "seek": 420164,
      "start": 4201.64,
      "end": 4209.52,
      "text": " imposta, come sistema di cache. L'ultimo anno, sempre per il discorso che la nostra esperienza",
      "tokens": [
        704,
        8638,
        11,
        808,
        13245,
        1026,
        19459,
        13,
        441,
        6,
        723,
        6934,
        46277,
        11,
        9553,
        680,
        1930,
        2983,
        284,
        539,
        947,
        635,
        34311,
        10045,
        42331
      ],
      "temperature": 0,
      "avg_logprob": -0.27044294576729294,
      "compression_ratio": 1.6470588235294117,
      "no_speech_prob": 1.0087573798500671e-7
    },
    {
      "id": 620,
      "seek": 420164,
      "start": 4209.52,
      "end": 4213.320000000001,
      "text": " ce la facciamo poi in base al progetto in cui lavoriamo, perché è lì che si fa la",
      "tokens": [
        1769,
        635,
        1915,
        42052,
        19260,
        294,
        3096,
        419,
        447,
        847,
        1353,
        294,
        22929,
        29241,
        7415,
        11,
        14303,
        4873,
        287,
        4749,
        947,
        1511,
        2050,
        635
      ],
      "temperature": 0,
      "avg_logprob": -0.27044294576729294,
      "compression_ratio": 1.6470588235294117,
      "no_speech_prob": 1.0087573798500671e-7
    },
    {
      "id": 621,
      "seek": 420164,
      "start": 4213.320000000001,
      "end": 4219.5,
      "text": " differenza. L'ultimo progetto in cui ho fatto questa federazione, in cui ho lavorato a stretto",
      "tokens": [
        743,
        23691,
        13,
        441,
        6,
        723,
        6934,
        447,
        847,
        1353,
        294,
        22929,
        1106,
        23228,
        16540,
        38024,
        12928,
        11,
        294,
        22929,
        1106,
        29241,
        2513,
        257,
        27678,
        1353
      ],
      "temperature": 0,
      "avg_logprob": -0.27044294576729294,
      "compression_ratio": 1.6470588235294117,
      "no_speech_prob": 1.0087573798500671e-7
    },
    {
      "id": 622,
      "seek": 420164,
      "start": 4219.5,
      "end": 4230.280000000001,
      "text": " contatto con Matteo per un bel po', il progetto era gigante e a Redis ho imparato, grazie",
      "tokens": [
        660,
        37491,
        416,
        47544,
        78,
        680,
        517,
        989,
        714,
        6098,
        1930,
        447,
        847,
        1353,
        4249,
        8741,
        2879,
        308,
        257,
        4477,
        271,
        1106,
        704,
        289,
        2513,
        11,
        1295,
        3283
      ],
      "temperature": 0,
      "avg_logprob": -0.27044294576729294,
      "compression_ratio": 1.6470588235294117,
      "no_speech_prob": 1.0087573798500671e-7
    },
    {
      "id": 623,
      "seek": 423028,
      "start": 4230.28,
      "end": 4236.599999999999,
      "text": " a Matteo che mi ha spinto un po' a fare certe cose, ho imparato a usarlo molto bene e non",
      "tokens": [
        257,
        47544,
        78,
        947,
        2752,
        324,
        637,
        17246,
        517,
        714,
        6,
        257,
        11994,
        5351,
        68,
        30261,
        11,
        1106,
        704,
        289,
        2513,
        257,
        14745,
        752,
        16394,
        2537,
        308,
        2107
      ],
      "temperature": 0,
      "avg_logprob": -0.3092445308326656,
      "compression_ratio": 1.7722772277227723,
      "no_speech_prob": 0.000004860405169893056
    },
    {
      "id": 624,
      "seek": 423028,
      "start": 4236.599999999999,
      "end": 4241.4,
      "text": " a usarlo nel senso che non so usare Redis, ho imparato a sfruttarlo molto bene, questa",
      "tokens": [
        257,
        505,
        19457,
        15373,
        3151,
        539,
        947,
        2107,
        370,
        505,
        543,
        4477,
        271,
        11,
        1106,
        704,
        289,
        2513,
        257,
        262,
        5779,
        13478,
        19457,
        16394,
        2537,
        11,
        16540
      ],
      "temperature": 0,
      "avg_logprob": -0.3092445308326656,
      "compression_ratio": 1.7722772277227723,
      "no_speech_prob": 0.000004860405169893056
    },
    {
      "id": 625,
      "seek": 423028,
      "start": 4241.4,
      "end": 4251.4,
      "text": " è la parte importante. E' una cosa che dovrebbe far parte di una, cioè Redis dovrebbe esserci",
      "tokens": [
        4873,
        635,
        6975,
        9416,
        13,
        462,
        6,
        2002,
        10163,
        947,
        30870,
        39487,
        1400,
        6975,
        1026,
        2002,
        11,
        41827,
        4477,
        271,
        30870,
        39487,
        2097,
        260,
        537
      ],
      "temperature": 0,
      "avg_logprob": -0.3092445308326656,
      "compression_ratio": 1.7722772277227723,
      "no_speech_prob": 0.000004860405169893056
    },
    {
      "id": 626,
      "seek": 423028,
      "start": 4251.4,
      "end": 4256.759999999999,
      "text": " non il progetto. Un progetto che non ha un Redis è un progetto che non ha bisogno di",
      "tokens": [
        2107,
        1930,
        447,
        847,
        1353,
        13,
        1156,
        447,
        847,
        1353,
        947,
        2107,
        324,
        517,
        4477,
        271,
        4873,
        517,
        447,
        847,
        1353,
        947,
        2107,
        324,
        40505,
        1771,
        1026
      ],
      "temperature": 0,
      "avg_logprob": -0.3092445308326656,
      "compression_ratio": 1.7722772277227723,
      "no_speech_prob": 0.000004860405169893056
    },
    {
      "id": 627,
      "seek": 425676,
      "start": 4256.76,
      "end": 4262.4800000000005,
      "text": " performance, perché performance non intendo che deve restituire in 20 millisecondi una",
      "tokens": [
        3389,
        11,
        14303,
        3389,
        2107,
        560,
        3999,
        947,
        17761,
        1472,
        6380,
        621,
        294,
        945,
        27940,
        18882,
        72,
        2002
      ],
      "temperature": 0,
      "avg_logprob": -0.3731809898659035,
      "compression_ratio": 1.4873949579831933,
      "no_speech_prob": 0.000005594313734036405
    },
    {
      "id": 628,
      "seek": 425676,
      "start": 4262.4800000000005,
      "end": 4272.16,
      "text": " pagina, però spesso e volentieri... Se c'è complessità c'è fai dati, bom bom bom,",
      "tokens": [
        11812,
        1426,
        11,
        12673,
        637,
        5557,
        308,
        1996,
        317,
        45980,
        485,
        1100,
        269,
        6,
        1462,
        1209,
        442,
        12445,
        269,
        6,
        1462,
        283,
        1301,
        1137,
        72,
        11,
        7957,
        7957,
        7957,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.3731809898659035,
      "compression_ratio": 1.4873949579831933,
      "no_speech_prob": 0.000005594313734036405
    },
    {
      "id": 629,
      "seek": 425676,
      "start": 4272.16,
      "end": 4278.320000000001,
      "text": " un po' di carico probabilmente. Esatto, avere un luogo in cui tu dici io non mi preoccupo",
      "tokens": [
        517,
        714,
        6,
        1026,
        1032,
        2789,
        31959,
        4082,
        13,
        2313,
        37491,
        11,
        37914,
        517,
        10438,
        23515,
        294,
        22929,
        2604,
        274,
        8787,
        19785,
        2107,
        2752,
        44388,
        78
      ],
      "temperature": 0,
      "avg_logprob": -0.3731809898659035,
      "compression_ratio": 1.4873949579831933,
      "no_speech_prob": 0.000005594313734036405
    },
    {
      "id": 630,
      "seek": 425676,
      "start": 4278.320000000001,
      "end": 4284.76,
      "text": " di quanto ci mette a dare questo dato, perché tanto so che mi refresha ogni mezz'ora e io",
      "tokens": [
        1026,
        17820,
        6983,
        1131,
        975,
        257,
        8955,
        10263,
        46971,
        11,
        14303,
        10331,
        370,
        947,
        2752,
        17368,
        1641,
        33189,
        385,
        4313,
        6,
        3252,
        308,
        19785
      ],
      "temperature": 0,
      "avg_logprob": -0.3731809898659035,
      "compression_ratio": 1.4873949579831933,
      "no_speech_prob": 0.000005594313734036405
    },
    {
      "id": 631,
      "seek": 428476,
      "start": 4284.76,
      "end": 4291.360000000001,
      "text": " quindi posso permettermi ogni 10 minuti di andarlo a prendere da Redis e me ne frego",
      "tokens": [
        15727,
        22501,
        20696,
        391,
        3057,
        33189,
        1266,
        13951,
        72,
        1026,
        293,
        19457,
        257,
        9866,
        323,
        1120,
        4477,
        271,
        308,
        385,
        408,
        2130,
        1571
      ],
      "temperature": 0,
      "avg_logprob": -0.39413976669311523,
      "compression_ratio": 1.5493562231759657,
      "no_speech_prob": 5.7477279824524885e-8
    },
    {
      "id": 632,
      "seek": 428476,
      "start": 4291.360000000001,
      "end": 4297.76,
      "text": " di quanto costi il reale peso. Ed è una cosa che ti fa programmare con molto più leggerezza",
      "tokens": [
        1026,
        17820,
        2063,
        72,
        1930,
        957,
        68,
        28149,
        13,
        3977,
        4873,
        2002,
        10163,
        947,
        8757,
        2050,
        1461,
        15455,
        416,
        16394,
        10589,
        1676,
        432,
        17693,
        2394
      ],
      "temperature": 0,
      "avg_logprob": -0.39413976669311523,
      "compression_ratio": 1.5493562231759657,
      "no_speech_prob": 5.7477279824524885e-8
    },
    {
      "id": 633,
      "seek": 428476,
      "start": 4297.76,
      "end": 4303.400000000001,
      "text": " da questo punto di vista. Con il cash bisogna sempre invalidarla nelle problematiche, però",
      "tokens": [
        1120,
        10263,
        14326,
        1026,
        22553,
        13,
        2656,
        741,
        75,
        6388,
        40505,
        629,
        9553,
        1048,
        304,
        48133,
        875,
        46350,
        1154,
        267,
        9304,
        11,
        12673
      ],
      "temperature": 0,
      "avg_logprob": -0.39413976669311523,
      "compression_ratio": 1.5493562231759657,
      "no_speech_prob": 5.7477279824524885e-8
    },
    {
      "id": 634,
      "seek": 428476,
      "start": 4303.400000000001,
      "end": 4312.84,
      "text": " non è sempre necessario invalidarla, perché spesso sono label, spesso sono cose, ci sono",
      "tokens": [
        2107,
        4873,
        9553,
        2688,
        4912,
        1048,
        304,
        48133,
        875,
        11,
        14303,
        637,
        5557,
        9259,
        7645,
        11,
        637,
        5557,
        9259,
        30261,
        11,
        6983,
        9259
      ],
      "temperature": 0,
      "avg_logprob": -0.39413976669311523,
      "compression_ratio": 1.5493562231759657,
      "no_speech_prob": 5.7477279824524885e-8
    },
    {
      "id": 635,
      "seek": 431284,
      "start": 4312.84,
      "end": 4319.04,
      "text": " altri controlli successivamente che nel caso il valore non sia giusto ti risolvono il problema.",
      "tokens": [
        33707,
        1583,
        16320,
        2245,
        23957,
        947,
        15373,
        9666,
        1930,
        1323,
        418,
        2107,
        25176,
        1735,
        48260,
        8757,
        2253,
        401,
        85,
        8957,
        1930,
        12395,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.2719170366014753,
      "compression_ratio": 1.5381355932203389,
      "no_speech_prob": 1.2952698114077066e-7
    },
    {
      "id": 636,
      "seek": 431284,
      "start": 4319.04,
      "end": 4326.4400000000005,
      "text": " Tanto c'è sempre un caso in cui per quanto ci mettiamo non potremo mai... Va tutto in",
      "tokens": [
        314,
        5857,
        269,
        6,
        1462,
        9553,
        517,
        9666,
        294,
        22929,
        680,
        17820,
        6983,
        27812,
        7415,
        2107,
        1847,
        44172,
        12698,
        485,
        16822,
        23048,
        294
      ],
      "temperature": 0,
      "avg_logprob": -0.2719170366014753,
      "compression_ratio": 1.5381355932203389,
      "no_speech_prob": 1.2952698114077066e-7
    },
    {
      "id": 637,
      "seek": 431284,
      "start": 4326.4400000000005,
      "end": 4332.52,
      "text": " vacca! L'utente carica la pagina, va a mangiare, torna e c'ha una pagina piena di dati che",
      "tokens": [
        2842,
        496,
        0,
        441,
        6,
        325,
        1576,
        1032,
        2262,
        635,
        11812,
        1426,
        11,
        2773,
        257,
        587,
        7834,
        543,
        11,
        3930,
        629,
        308,
        269,
        6,
        1641,
        2002,
        11812,
        1426,
        26274,
        64,
        1026,
        1137,
        72,
        947
      ],
      "temperature": 0,
      "avg_logprob": -0.2719170366014753,
      "compression_ratio": 1.5381355932203389,
      "no_speech_prob": 1.2952698114077066e-7
    },
    {
      "id": 638,
      "seek": 431284,
      "start": 4332.52,
      "end": 4338.76,
      "text": " non funzionano più, perché nel frattempo sono cambiati e committa una forma di qualcosa",
      "tokens": [
        2107,
        49345,
        313,
        3730,
        10589,
        11,
        14303,
        15373,
        431,
        1591,
        443,
        2259,
        9259,
        19569,
        6908,
        308,
        800,
        21870,
        2002,
        8366,
        1026,
        42400
      ],
      "temperature": 0,
      "avg_logprob": -0.2719170366014753,
      "compression_ratio": 1.5381355932203389,
      "no_speech_prob": 1.2952698114077066e-7
    },
    {
      "id": 639,
      "seek": 433876,
      "start": 4338.76,
      "end": 4345.84,
      "text": " che dietro nel frattempo è cambiato. Ma se hai usato le GraphQL Subscriptions, qualcosa",
      "tokens": [
        947,
        6339,
        340,
        15373,
        431,
        1591,
        443,
        2259,
        4873,
        19569,
        2513,
        13,
        4042,
        369,
        21822,
        505,
        2513,
        476,
        21884,
        13695,
        37471,
        34173,
        11,
        42400
      ],
      "temperature": 0,
      "avg_logprob": -0.30046037265232634,
      "compression_ratio": 1.5458515283842795,
      "no_speech_prob": 4.965280595570221e-7
    },
    {
      "id": 640,
      "seek": 433876,
      "start": 4345.84,
      "end": 4351.2,
      "text": " sotto il culo, i dati sotto il culo ti sono cambiati! Anche poi lavorando nell'ambiente",
      "tokens": [
        43754,
        1930,
        11021,
        78,
        11,
        741,
        1137,
        72,
        43754,
        1930,
        11021,
        78,
        8757,
        9259,
        19569,
        6908,
        0,
        1107,
        1876,
        19260,
        29241,
        1806,
        44666,
        6,
        2173,
        8413
      ],
      "temperature": 0,
      "avg_logprob": -0.30046037265232634,
      "compression_ratio": 1.5458515283842795,
      "no_speech_prob": 4.965280595570221e-7
    },
    {
      "id": 641,
      "seek": 433876,
      "start": 4351.2,
      "end": 4358.280000000001,
      "text": " le scopri queste cose. Io l'idea di avere una scheda di un browser aperta per dei giorni",
      "tokens": [
        476,
        795,
        404,
        470,
        35455,
        30261,
        13,
        19239,
        287,
        6,
        482,
        64,
        1026,
        37914,
        2002,
        5292,
        64,
        1026,
        517,
        11185,
        43139,
        1328,
        680,
        13874,
        36937,
        72
      ],
      "temperature": 0,
      "avg_logprob": -0.30046037265232634,
      "compression_ratio": 1.5458515283842795,
      "no_speech_prob": 4.965280595570221e-7
    },
    {
      "id": 642,
      "seek": 433876,
      "start": 4358.280000000001,
      "end": 4363.4400000000005,
      "text": " non l'avevo neanche mai considerata, ma scopri che poi in realtà ci sono e dici ma come",
      "tokens": [
        2107,
        287,
        6,
        946,
        3080,
        408,
        22806,
        12698,
        1949,
        3274,
        11,
        463,
        795,
        404,
        470,
        947,
        19260,
        294,
        47512,
        6983,
        9259,
        308,
        274,
        8787,
        463,
        808
      ],
      "temperature": 0,
      "avg_logprob": -0.30046037265232634,
      "compression_ratio": 1.5458515283842795,
      "no_speech_prob": 4.965280595570221e-7
    },
    {
      "id": 643,
      "seek": 436344,
      "start": 4363.44,
      "end": 4368.799999999999,
      "text": " ha fatto questo a mandarmi dentro questi dati che sono già tre giorni e che non ci sono",
      "tokens": [
        324,
        23228,
        10263,
        257,
        7411,
        289,
        3057,
        10856,
        29729,
        1137,
        72,
        947,
        9259,
        30469,
        2192,
        36937,
        72,
        308,
        947,
        2107,
        6983,
        9259
      ],
      "temperature": 0,
      "avg_logprob": -0.36311134215324153,
      "compression_ratio": 1.6666666666666667,
      "no_speech_prob": 2.5907063161412225e-8
    },
    {
      "id": 644,
      "seek": 436344,
      "start": 4368.799999999999,
      "end": 4374.48,
      "text": " più? Ma un utente interno dell'azienda, uno a cui abbiamo chiesto, e lui ci ha detto",
      "tokens": [
        10589,
        30,
        4042,
        517,
        2839,
        1576,
        728,
        1771,
        19781,
        6,
        921,
        30498,
        11,
        8526,
        257,
        22929,
        22815,
        417,
        6495,
        78,
        11,
        308,
        8783,
        6983,
        324,
        41031
      ],
      "temperature": 0,
      "avg_logprob": -0.36311134215324153,
      "compression_ratio": 1.6666666666666667,
      "no_speech_prob": 2.5907063161412225e-8
    },
    {
      "id": 645,
      "seek": 436344,
      "start": 4374.48,
      "end": 4378.679999999999,
      "text": " che la pagina era riaperta da una settimana e io semplicemente a un certo punto ho deciso",
      "tokens": [
        947,
        635,
        11812,
        1426,
        4249,
        367,
        654,
        610,
        1328,
        1120,
        2002,
        5584,
        36497,
        308,
        19785,
        4361,
        4770,
        16288,
        257,
        517,
        22261,
        14326,
        1106,
        18206,
        78
      ],
      "temperature": 0,
      "avg_logprob": -0.36311134215324153,
      "compression_ratio": 1.6666666666666667,
      "no_speech_prob": 2.5907063161412225e-8
    },
    {
      "id": 646,
      "seek": 436344,
      "start": 4378.679999999999,
      "end": 4385.599999999999,
      "text": " di dire ok. Guarda, caso di stamattina, giusto per farci",
      "tokens": [
        1026,
        1264,
        3133,
        13,
        11549,
        64,
        11,
        9666,
        1026,
        29682,
        1591,
        1426,
        11,
        1735,
        48260,
        680,
        1400,
        537
      ],
      "temperature": 0,
      "avg_logprob": -0.36311134215324153,
      "compression_ratio": 1.6666666666666667,
      "no_speech_prob": 2.5907063161412225e-8
    },
    {
      "id": 647,
      "seek": 436344,
      "start": 4385.599999999999,
      "end": 4392.24,
      "text": " una risata insieme, caso di stamattina io sto lavorando un sistema e non posso dire",
      "tokens": [
        2002,
        2253,
        3274,
        1028,
        44940,
        11,
        9666,
        1026,
        29682,
        1591,
        1426,
        19785,
        22784,
        29241,
        1806,
        517,
        13245,
        308,
        2107,
        22501,
        1264
      ],
      "temperature": 0,
      "avg_logprob": -0.36311134215324153,
      "compression_ratio": 1.6666666666666667,
      "no_speech_prob": 2.5907063161412225e-8
    },
    {
      "id": 648,
      "seek": 439224,
      "start": 4392.24,
      "end": 4398.28,
      "text": " praticamente niente su quello di cui sto lavorando, però in breve, sto lavorando un sistema che",
      "tokens": [
        45734,
        297,
        8413,
        459,
        22813,
        1026,
        22929,
        22784,
        29241,
        1806,
        11,
        12673,
        294,
        48517,
        11,
        22784,
        29241,
        1806,
        517,
        13245,
        947
      ],
      "temperature": 0,
      "avg_logprob": -0.31107001926587974,
      "compression_ratio": 1.7194570135746607,
      "no_speech_prob": 6.412059150306959e-8
    },
    {
      "id": 649,
      "seek": 439224,
      "start": 4398.28,
      "end": 4403.2,
      "text": " ti dà, c'è una pagina prodotto, immaginate una pagina prodotto anche se non è un prodotto",
      "tokens": [
        8757,
        274,
        1467,
        11,
        269,
        6,
        1462,
        2002,
        11812,
        1426,
        15792,
        18838,
        11,
        3397,
        559,
        13923,
        2002,
        11812,
        1426,
        15792,
        18838,
        11585,
        369,
        2107,
        4873,
        517,
        15792,
        18838
      ],
      "temperature": 0,
      "avg_logprob": -0.31107001926587974,
      "compression_ratio": 1.7194570135746607,
      "no_speech_prob": 6.412059150306959e-8
    },
    {
      "id": 650,
      "seek": 439224,
      "start": 4403.2,
      "end": 4412.24,
      "text": " e tu devi fare tipo un booking su quel prodotto e cribio ho iniziato a vedere tutti i 404,",
      "tokens": [
        308,
        2604,
        31219,
        11994,
        9746,
        517,
        34424,
        459,
        7178,
        15792,
        18838,
        308,
        12815,
        65,
        1004,
        1106,
        294,
        24300,
        2513,
        257,
        35373,
        19822,
        741,
        3356,
        19,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.31107001926587974,
      "compression_ratio": 1.7194570135746607,
      "no_speech_prob": 6.412059150306959e-8
    },
    {
      "id": 651,
      "seek": 439224,
      "start": 4412.24,
      "end": 4421.4,
      "text": " 404, 404, 404, non capivo perché ed era praticamente, cos'era successo? Una persona si era aperto,",
      "tokens": [
        3356,
        19,
        11,
        3356,
        19,
        11,
        3356,
        19,
        11,
        2107,
        1410,
        6340,
        14303,
        1257,
        4249,
        45734,
        11,
        3792,
        6,
        1663,
        2245,
        78,
        30,
        15491,
        12184,
        1511,
        4249,
        43139,
        1353,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.31107001926587974,
      "compression_ratio": 1.7194570135746607,
      "no_speech_prob": 6.412059150306959e-8
    },
    {
      "id": 652,
      "seek": 442140,
      "start": 4421.4,
      "end": 4429.719999999999,
      "text": " 50 prodotti, ok 50, 100 tab, uno con ogni prodotto, che non erano prodotti, questi prodotti",
      "tokens": [
        2625,
        15792,
        37514,
        11,
        3133,
        2625,
        11,
        2319,
        4421,
        11,
        8526,
        416,
        33189,
        15792,
        18838,
        11,
        947,
        2107,
        1189,
        3730,
        15792,
        37514,
        11,
        29729,
        15792,
        37514
      ],
      "temperature": 0,
      "avg_logprob": -0.24613068320534445,
      "compression_ratio": 1.675,
      "no_speech_prob": 1.866015608698035e-8
    },
    {
      "id": 653,
      "seek": 442140,
      "start": 4429.719999999999,
      "end": 4436.5199999999995,
      "text": " sono scaduti, quindi non erano più presenti e lui ha praticamente lanciato, non so se",
      "tokens": [
        9259,
        795,
        345,
        29161,
        11,
        15727,
        2107,
        1189,
        3730,
        10589,
        1974,
        72,
        308,
        8783,
        324,
        45734,
        9326,
        537,
        2513,
        11,
        2107,
        370,
        369
      ],
      "temperature": 0,
      "avg_logprob": -0.24613068320534445,
      "compression_ratio": 1.675,
      "no_speech_prob": 1.866015608698035e-8
    },
    {
      "id": 654,
      "seek": 442140,
      "start": 4436.5199999999995,
      "end": 4444.599999999999,
      "text": " fosse uno script o qualcosa, uno schedula call per tutti i prodotti e quindi io guardando",
      "tokens": [
        24528,
        8526,
        5755,
        277,
        42400,
        11,
        8526,
        5292,
        3780,
        818,
        680,
        19822,
        741,
        15792,
        37514,
        308,
        15727,
        19785,
        6290,
        1806
      ],
      "temperature": 0,
      "avg_logprob": -0.24613068320534445,
      "compression_ratio": 1.675,
      "no_speech_prob": 1.866015608698035e-8
    },
    {
      "id": 655,
      "seek": 444460,
      "start": 4444.6,
      "end": 4452.320000000001,
      "text": " le app in site ho detto ma questo è un coglione, invece no, se tu pensi a livello di business,",
      "tokens": [
        476,
        724,
        294,
        3621,
        1106,
        41031,
        463,
        10263,
        4873,
        517,
        598,
        7191,
        5328,
        11,
        36344,
        572,
        11,
        369,
        2604,
        6099,
        72,
        257,
        1621,
        1913,
        1026,
        1606,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.2754476588705312,
      "compression_ratio": 1.5294117647058822,
      "no_speech_prob": 1.8086044661913547e-8
    },
    {
      "id": 656,
      "seek": 444460,
      "start": 4452.320000000001,
      "end": 4459.4400000000005,
      "text": " io mi immagino, sai ho avuto un'azienda turistica per un po' di tempo e io mi ricordo la mia",
      "tokens": [
        19785,
        2752,
        3397,
        559,
        2982,
        11,
        32417,
        1106,
        1305,
        8262,
        517,
        6,
        921,
        30498,
        3243,
        468,
        2262,
        680,
        517,
        714,
        6,
        1026,
        8972,
        308,
        19785,
        2752,
        21040,
        23872,
        635,
        21290
      ],
      "temperature": 0,
      "avg_logprob": -0.2754476588705312,
      "compression_ratio": 1.5294117647058822,
      "no_speech_prob": 1.8086044661913547e-8
    },
    {
      "id": 657,
      "seek": 444460,
      "start": 4459.4400000000005,
      "end": 4466.6,
      "text": " socia quando organizzava delle vacanze importanti, teneva anche per una o due settimane le pagine",
      "tokens": [
        370,
        2755,
        7770,
        4645,
        89,
        4061,
        16485,
        2842,
        282,
        1381,
        1021,
        72,
        11,
        256,
        1450,
        2757,
        11585,
        680,
        2002,
        277,
        3462,
        5584,
        332,
        1929,
        476,
        280,
        10260
      ],
      "temperature": 0,
      "avg_logprob": -0.2754476588705312,
      "compression_ratio": 1.5294117647058822,
      "no_speech_prob": 1.8086044661913547e-8
    },
    {
      "id": 658,
      "seek": 446660,
      "start": 4466.6,
      "end": 4476.320000000001,
      "text": " degli hotel aperti o le pagine dei servizi da bookare aperti, bookare non credo si dica",
      "tokens": [
        32079,
        7622,
        43139,
        7317,
        277,
        476,
        280,
        10260,
        13874,
        1658,
        24300,
        1120,
        272,
        1212,
        543,
        43139,
        7317,
        11,
        1446,
        543,
        2107,
        3864,
        78,
        1511,
        14285,
        64
      ],
      "temperature": 0,
      "avg_logprob": -0.5324192444483439,
      "compression_ratio": 1.4943820224719102,
      "no_speech_prob": 3.028839401508776e-8
    },
    {
      "id": 659,
      "seek": 446660,
      "start": 4476.320000000001,
      "end": 4482.76,
      "text": " neppure, ma facciamo finire. No, bookie, boo boo boo e bookare. Prenottare, prenottare.",
      "tokens": [
        408,
        427,
        540,
        11,
        463,
        1915,
        42052,
        962,
        621,
        13,
        883,
        11,
        1446,
        414,
        11,
        23113,
        23113,
        23113,
        308,
        1446,
        543,
        13,
        430,
        1095,
        310,
        83,
        543,
        11,
        659,
        2247,
        83,
        543,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.5324192444483439,
      "compression_ratio": 1.4943820224719102,
      "no_speech_prob": 3.028839401508776e-8
    },
    {
      "id": 660,
      "seek": 446660,
      "start": 4482.76,
      "end": 4491.96,
      "text": " Ormai abbiamo perso l'uso delle lingue. Esatto, no però è una cosa più che verosimile e",
      "tokens": [
        1610,
        76,
        1301,
        22815,
        868,
        78,
        287,
        6,
        24431,
        16485,
        287,
        278,
        622,
        13,
        2313,
        37491,
        11,
        572,
        12673,
        4873,
        2002,
        10163,
        10589,
        947,
        1306,
        329,
        332,
        794,
        308
      ],
      "temperature": 0,
      "avg_logprob": -0.5324192444483439,
      "compression_ratio": 1.4943820224719102,
      "no_speech_prob": 3.028839401508776e-8
    },
    {
      "id": 661,
      "seek": 449196,
      "start": 4491.96,
      "end": 4500,
      "text": " ormai l'abbiamo detto, quindi comunque questi edge case ci sono e ci saranno sempre. Guarda,",
      "tokens": [
        420,
        76,
        1301,
        287,
        6,
        10797,
        7415,
        41031,
        11,
        15727,
        45736,
        29729,
        4691,
        1389,
        6983,
        9259,
        308,
        6983,
        13782,
        13484,
        9553,
        13,
        11549,
        64,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.2708893666225197,
      "compression_ratio": 1.5125,
      "no_speech_prob": 2.5907063161412225e-8
    },
    {
      "id": 662,
      "seek": 449196,
      "start": 4500,
      "end": 4504.96,
      "text": " io direi che potrei rimanere ore a parlare di questo, l'abbiamo anche già fatto. Vi",
      "tokens": [
        19785,
        1264,
        72,
        947,
        1847,
        10271,
        15982,
        282,
        323,
        20865,
        257,
        13734,
        543,
        1026,
        10263,
        11,
        287,
        6,
        10797,
        7415,
        11585,
        30469,
        23228,
        13,
        6626
      ],
      "temperature": 0,
      "avg_logprob": -0.2708893666225197,
      "compression_ratio": 1.5125,
      "no_speech_prob": 2.5907063161412225e-8
    },
    {
      "id": 663,
      "seek": 449196,
      "start": 4504.96,
      "end": 4510.4800000000005,
      "text": " faccio giusto l'ultima domanda sul topic perché è una domanda che mi sta molto a cuore. Qualche",
      "tokens": [
        1915,
        8529,
        1735,
        48260,
        287,
        6,
        723,
        4775,
        3285,
        5575,
        17603,
        4829,
        14303,
        4873,
        2002,
        3285,
        5575,
        947,
        2752,
        11135,
        16394,
        257,
        2702,
        418,
        13,
        13616,
        1876
      ],
      "temperature": 0,
      "avg_logprob": -0.2708893666225197,
      "compression_ratio": 1.5125,
      "no_speech_prob": 2.5907063161412225e-8
    },
    {
      "id": 664,
      "seek": 449196,
      "start": 4510.4800000000005,
      "end": 4518.2,
      "text": " tempo fa ho lavorato su un API GraphQL, ero già in Irform ed era un grosso e-commerce,",
      "tokens": [
        8972,
        2050,
        1106,
        29241,
        2513,
        459,
        517,
        9362,
        21884,
        13695,
        11,
        1189,
        78,
        30469,
        294,
        9151,
        837,
        1257,
        4249,
        517,
        18638,
        539,
        308,
        12,
        26926,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.2708893666225197,
      "compression_ratio": 1.5125,
      "no_speech_prob": 2.5907063161412225e-8
    },
    {
      "id": 665,
      "seek": 451820,
      "start": 4518.2,
      "end": 4526.32,
      "text": " medio grosso e-commerce che faceva parecchi ordini al secondo e avevo un API GraphQL.",
      "tokens": [
        22123,
        18638,
        539,
        308,
        12,
        26926,
        947,
        1851,
        2757,
        7448,
        66,
        8036,
        4792,
        3812,
        419,
        41601,
        308,
        3472,
        3080,
        517,
        9362,
        21884,
        13695,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.27367663833330264,
      "compression_ratio": 1.3065693430656935,
      "no_speech_prob": 3.711013718543654e-8
    },
    {
      "id": 666,
      "seek": 451820,
      "start": 4526.32,
      "end": 4537.28,
      "text": " E una delle cose più rompi balle in questo API GraphQL era cercare di stabilire un contratto",
      "tokens": [
        462,
        2002,
        16485,
        30261,
        10589,
        7438,
        22630,
        2594,
        68,
        294,
        10263,
        9362,
        21884,
        13695,
        4249,
        10146,
        5685,
        1026,
        11652,
        621,
        517,
        40944,
        1353
      ],
      "temperature": 0,
      "avg_logprob": -0.27367663833330264,
      "compression_ratio": 1.3065693430656935,
      "no_speech_prob": 3.711013718543654e-8
    },
    {
      "id": 667,
      "seek": 453728,
      "start": 4537.28,
      "end": 4548.8,
      "text": " tra front-end e back-end per la comunicazione dell'errore. GraphQL ha il suo modo di comunicare",
      "tokens": [
        944,
        1868,
        12,
        521,
        308,
        646,
        12,
        521,
        680,
        635,
        31710,
        12928,
        19781,
        6,
        260,
        340,
        265,
        13,
        21884,
        13695,
        324,
        1930,
        34197,
        16664,
        1026,
        31710,
        543
      ],
      "temperature": 0,
      "avg_logprob": -0.25059735224797175,
      "compression_ratio": 1.348148148148148,
      "no_speech_prob": 1.6119882673137909e-7
    },
    {
      "id": 668,
      "seek": 453728,
      "start": 4548.8,
      "end": 4558.599999999999,
      "text": " l'errore che ti dice error e poi c'hai del data non tipizzato. Ok? E mi dico ma cazzo,",
      "tokens": [
        287,
        6,
        260,
        340,
        265,
        947,
        8757,
        10313,
        6713,
        308,
        19260,
        269,
        6,
        18230,
        1103,
        1412,
        2107,
        4125,
        8072,
        2513,
        13,
        3477,
        30,
        462,
        2752,
        274,
        2789,
        463,
        269,
        921,
        4765,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.25059735224797175,
      "compression_ratio": 1.348148148148148,
      "no_speech_prob": 1.6119882673137909e-7
    },
    {
      "id": 669,
      "seek": 455860,
      "start": 4558.6,
      "end": 4569.240000000001,
      "text": " stiamo tipizzando tutto. Ok? Allora perché non tipizzare anche gli errori di dominio",
      "tokens": [
        342,
        7415,
        4125,
        8072,
        1806,
        23048,
        13,
        3477,
        30,
        1057,
        3252,
        14303,
        2107,
        4125,
        8072,
        543,
        11585,
        17161,
        6713,
        72,
        1026,
        8859,
        1004
      ],
      "temperature": 0,
      "avg_logprob": -0.2235893726348877,
      "compression_ratio": 1.3814432989690721,
      "no_speech_prob": 2.017643474516717e-8
    },
    {
      "id": 670,
      "seek": 455860,
      "start": 4569.240000000001,
      "end": 4575.400000000001,
      "text": " e dare un modo più agevole per la comunicazione di questi errori di dominio? Tanto sempre",
      "tokens": [
        308,
        8955,
        517,
        16664,
        10589,
        3205,
        3080,
        306,
        680,
        635,
        31710,
        12928,
        1026,
        29729,
        6713,
        72,
        1026,
        8859,
        1004,
        30,
        314,
        5857,
        9553
      ],
      "temperature": 0,
      "avg_logprob": -0.2235893726348877,
      "compression_ratio": 1.3814432989690721,
      "no_speech_prob": 2.017643474516717e-8
    },
    {
      "id": 671,
      "seek": 455860,
      "start": 4575.400000000001,
      "end": 4587.320000000001,
      "text": " 200 ti risponde e già questo poco mi piace. Però come posso dirlo? Cioè ad oggi nell'API",
      "tokens": [
        2331,
        8757,
        2253,
        79,
        7259,
        308,
        30469,
        10263,
        10639,
        2752,
        50062,
        13,
        20533,
        808,
        22501,
        4746,
        752,
        30,
        383,
        35983,
        614,
        34768,
        44666,
        6,
        4715,
        40
      ],
      "temperature": 0,
      "avg_logprob": -0.2235893726348877,
      "compression_ratio": 1.3814432989690721,
      "no_speech_prob": 2.017643474516717e-8
    },
    {
      "id": 672,
      "seek": 458732,
      "start": 4587.32,
      "end": 4594.48,
      "text": " GraphQL vedo una forte distinzione tra gli errori di platform, quindi legati al framework",
      "tokens": [
        21884,
        13695,
        14267,
        78,
        2002,
        23235,
        1483,
        259,
        19706,
        944,
        17161,
        6713,
        72,
        1026,
        3663,
        11,
        15727,
        1676,
        6908,
        419,
        8388
      ],
      "temperature": 0,
      "avg_logprob": -0.2702518403530121,
      "compression_ratio": 1.637037037037037,
      "no_speech_prob": 2.7852701123265433e-7
    },
    {
      "id": 673,
      "seek": 458732,
      "start": 4594.48,
      "end": 4600.679999999999,
      "text": " utilizzato, al server, all'architecture e gli errori di dominio. Secondo te hai mai",
      "tokens": [
        40355,
        2513,
        11,
        419,
        7154,
        11,
        439,
        6,
        1178,
        5739,
        540,
        308,
        17161,
        6713,
        72,
        1026,
        8859,
        1004,
        13,
        5736,
        78,
        535,
        21822,
        12698
      ],
      "temperature": 0,
      "avg_logprob": -0.2702518403530121,
      "compression_ratio": 1.637037037037037,
      "no_speech_prob": 2.7852701123265433e-7
    },
    {
      "id": 674,
      "seek": 458732,
      "start": 4600.679999999999,
      "end": 4604.5599999999995,
      "text": " vissuto questo tipo di problema? Te lo dico perché noi abbiamo dovuto fare un wrapper,",
      "tokens": [
        371,
        891,
        8262,
        10263,
        9746,
        1026,
        12395,
        30,
        1989,
        450,
        274,
        2789,
        14303,
        22447,
        22815,
        30870,
        8262,
        11994,
        517,
        46906,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.2702518403530121,
      "compression_ratio": 1.637037037037037,
      "no_speech_prob": 2.7852701123265433e-7
    },
    {
      "id": 675,
      "seek": 458732,
      "start": 4604.5599999999995,
      "end": 4609,
      "text": " abbiamo dovuto hackare il sistema degli errori di Apollo per fargli rispondere degli errori",
      "tokens": [
        22815,
        30870,
        8262,
        10339,
        543,
        1930,
        13245,
        32079,
        6713,
        72,
        1026,
        25187,
        680,
        1400,
        41443,
        2253,
        79,
        33447,
        32079,
        6713,
        72
      ],
      "temperature": 0,
      "avg_logprob": -0.2702518403530121,
      "compression_ratio": 1.637037037037037,
      "no_speech_prob": 2.7852701123265433e-7
    },
    {
      "id": 676,
      "seek": 458732,
      "start": 4609,
      "end": 4615.639999999999,
      "text": " in grazia di Dio dove potevi querare GraphQL anche dentro l'errore. Sì, il problema non",
      "tokens": [
        294,
        1295,
        40395,
        1026,
        413,
        1004,
        23287,
        280,
        1370,
        4917,
        421,
        260,
        543,
        21884,
        13695,
        11585,
        10856,
        287,
        6,
        260,
        340,
        265,
        13,
        318,
        4749,
        11,
        1930,
        12395,
        2107
      ],
      "temperature": 0,
      "avg_logprob": -0.2702518403530121,
      "compression_ratio": 1.637037037037037,
      "no_speech_prob": 2.7852701123265433e-7
    },
    {
      "id": 677,
      "seek": 461564,
      "start": 4615.64,
      "end": 4623.64,
      "text": " è tanto la struttura dell'errore, il problema è che se c'è un errore quel dato ti torna",
      "tokens": [
        4873,
        10331,
        635,
        1056,
        13478,
        2991,
        19781,
        6,
        260,
        340,
        265,
        11,
        1930,
        12395,
        4873,
        947,
        369,
        269,
        6,
        1462,
        517,
        45935,
        265,
        7178,
        46971,
        8757,
        3930,
        629
      ],
      "temperature": 0,
      "avg_logprob": -0.22580825487772624,
      "compression_ratio": 1.6540284360189574,
      "no_speech_prob": 0.0000010188016403844813
    },
    {
      "id": 678,
      "seek": 461564,
      "start": 4623.64,
      "end": 4629.04,
      "text": " nullo, questo è uno dei grandi problemi. E l'errore non è lì, ce l'hai separato,",
      "tokens": [
        18184,
        78,
        11,
        10263,
        4873,
        8526,
        13874,
        45155,
        1154,
        72,
        13,
        462,
        287,
        6,
        260,
        340,
        265,
        2107,
        4873,
        287,
        4749,
        11,
        1769,
        287,
        6,
        18230,
        3128,
        2513,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.22580825487772624,
      "compression_ratio": 1.6540284360189574,
      "no_speech_prob": 0.0000010188016403844813
    },
    {
      "id": 679,
      "seek": 461564,
      "start": 4629.04,
      "end": 4635.4800000000005,
      "text": " ce l'hai da un'altra parte e quindi è un problema. Perché se anche tu lo restituissi",
      "tokens": [
        1769,
        287,
        6,
        18230,
        1120,
        517,
        6,
        38865,
        6975,
        308,
        15727,
        4873,
        517,
        12395,
        13,
        47978,
        369,
        11585,
        2604,
        450,
        1472,
        6380,
        891,
        72
      ],
      "temperature": 0,
      "avg_logprob": -0.22580825487772624,
      "compression_ratio": 1.6540284360189574,
      "no_speech_prob": 0.0000010188016403844813
    },
    {
      "id": 680,
      "seek": 461564,
      "start": 4635.4800000000005,
      "end": 4640.4800000000005,
      "text": " lì, col fatto che non hai messo nello schema di darti indietro l'errore, lui non te lo",
      "tokens": [
        287,
        4749,
        11,
        1173,
        23228,
        947,
        2107,
        21822,
        2082,
        78,
        408,
        1913,
        34078,
        1026,
        39010,
        72,
        1016,
        1684,
        340,
        287,
        6,
        260,
        340,
        265,
        11,
        8783,
        2107,
        535,
        450
      ],
      "temperature": 0,
      "avg_logprob": -0.22580825487772624,
      "compression_ratio": 1.6540284360189574,
      "no_speech_prob": 0.0000010188016403844813
    },
    {
      "id": 681,
      "seek": 464048,
      "start": 4640.48,
      "end": 4646.24,
      "text": " dà indietro l'errore su quel punto. Perché l'unica cosa sarebbe probabilmente quella",
      "tokens": [
        274,
        1467,
        1016,
        1684,
        340,
        287,
        6,
        260,
        340,
        265,
        459,
        7178,
        14326,
        13,
        47978,
        287,
        6,
        409,
        2262,
        10163,
        38706,
        39042,
        31959,
        4082,
        32234
      ],
      "temperature": 0,
      "avg_logprob": -0.29836507524762834,
      "compression_ratio": 1.6349809885931559,
      "no_speech_prob": 9.570809424985782e-7
    },
    {
      "id": 682,
      "seek": 464048,
      "start": 4646.24,
      "end": 4651.48,
      "text": " di non so neanche se si possa fare a dire il vero. Però sì, hai ragione, non c'è",
      "tokens": [
        1026,
        2107,
        370,
        408,
        22806,
        369,
        1511,
        41564,
        11994,
        257,
        1264,
        1930,
        1306,
        78,
        13,
        20533,
        49267,
        11,
        21822,
        17539,
        5328,
        11,
        2107,
        269,
        6,
        1462
      ],
      "temperature": 0,
      "avg_logprob": -0.29836507524762834,
      "compression_ratio": 1.6349809885931559,
      "no_speech_prob": 9.570809424985782e-7
    },
    {
      "id": 683,
      "seek": 464048,
      "start": 4651.48,
      "end": 4655.919999999999,
      "text": " una soluzione, che io sappia, non c'è una soluzione. Ti dico come avevamo fatto noi,",
      "tokens": [
        2002,
        1404,
        3334,
        5328,
        11,
        947,
        19785,
        46938,
        654,
        11,
        2107,
        269,
        6,
        1462,
        2002,
        1404,
        3334,
        5328,
        13,
        20456,
        274,
        2789,
        808,
        3472,
        85,
        10502,
        23228,
        22447,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.29836507524762834,
      "compression_ratio": 1.6349809885931559,
      "no_speech_prob": 9.570809424985782e-7
    },
    {
      "id": 684,
      "seek": 464048,
      "start": 4655.919999999999,
      "end": 4660.719999999999,
      "text": " avevamo messo tutto dentro un bel try and catch, avevamo tipizzato tutti gli errori,",
      "tokens": [
        3472,
        85,
        10502,
        2082,
        78,
        23048,
        10856,
        517,
        989,
        853,
        293,
        3745,
        11,
        3472,
        85,
        10502,
        4125,
        8072,
        2513,
        19822,
        17161,
        6713,
        72,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.29836507524762834,
      "compression_ratio": 1.6349809885931559,
      "no_speech_prob": 9.570809424985782e-7
    },
    {
      "id": 685,
      "seek": 464048,
      "start": 4660.719999999999,
      "end": 4667.759999999999,
      "text": " li avevamo messi nello schema e quindi c'era il return 200, il return positivo, però di",
      "tokens": [
        375,
        3472,
        85,
        10502,
        2082,
        72,
        408,
        1913,
        34078,
        308,
        15727,
        269,
        6,
        1663,
        1930,
        2736,
        2331,
        11,
        1930,
        2736,
        44710,
        11,
        12673,
        1026
      ],
      "temperature": 0,
      "avg_logprob": -0.29836507524762834,
      "compression_ratio": 1.6349809885931559,
      "no_speech_prob": 9.570809424985782e-7
    },
    {
      "id": 686,
      "seek": 466776,
      "start": 4667.76,
      "end": 4673.4800000000005,
      "text": " un tipo errore che poi ti dovevi andare lato front end. E così abbiamo risolto. Lo mettevate",
      "tokens": [
        517,
        9746,
        45935,
        265,
        947,
        19260,
        8757,
        23287,
        4917,
        42742,
        287,
        2513,
        1868,
        917,
        13,
        462,
        23278,
        22815,
        2253,
        401,
        1353,
        13,
        6130,
        1131,
        975,
        19083
      ],
      "temperature": 0,
      "avg_logprob": -0.33552746290571234,
      "compression_ratio": 1.446808510638298,
      "no_speech_prob": 2.8453341016643208e-8
    },
    {
      "id": 687,
      "seek": 466776,
      "start": 4673.4800000000005,
      "end": 4682.8,
      "text": " nella struttura? Sì. Gli errori di dominio erano tipizzati, sì. Ma ti faccio una domanda,",
      "tokens": [
        23878,
        1056,
        13478,
        2991,
        30,
        318,
        4749,
        13,
        460,
        2081,
        6713,
        72,
        1026,
        8859,
        1004,
        1189,
        3730,
        4125,
        8072,
        6908,
        11,
        49267,
        13,
        4042,
        8757,
        1915,
        8529,
        2002,
        3285,
        5575,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.33552746290571234,
      "compression_ratio": 1.446808510638298,
      "no_speech_prob": 2.8453341016643208e-8
    },
    {
      "id": 688,
      "seek": 466776,
      "start": 4682.8,
      "end": 4689,
      "text": " tu chiedi una lista di 10 prodotti, il terzo prodotto della lista ti ha dato un errore",
      "tokens": [
        2604,
        417,
        1091,
        72,
        2002,
        27764,
        1026,
        1266,
        15792,
        37514,
        11,
        1930,
        1796,
        4765,
        15792,
        18838,
        11618,
        27764,
        8757,
        324,
        46971,
        517,
        45935,
        265
      ],
      "temperature": 0,
      "avg_logprob": -0.33552746290571234,
      "compression_ratio": 1.446808510638298,
      "no_speech_prob": 2.8453341016643208e-8
    },
    {
      "id": 689,
      "seek": 468900,
      "start": 4689,
      "end": 4698.28,
      "text": " e quindi è nullo. Perché si è rotto e quindi non c'è nessun errore. Tu hai una lista di",
      "tokens": [
        308,
        15727,
        4873,
        18184,
        78,
        13,
        47978,
        1511,
        4873,
        4297,
        1353,
        308,
        15727,
        2107,
        269,
        6,
        1462,
        39787,
        409,
        45935,
        265,
        13,
        7836,
        21822,
        2002,
        27764,
        1026
      ],
      "temperature": 0,
      "avg_logprob": -0.337024390179178,
      "compression_ratio": 1.5384615384615385,
      "no_speech_prob": 2.590704362148699e-8
    },
    {
      "id": 690,
      "seek": 468900,
      "start": 4698.28,
      "end": 4703.92,
      "text": " oggetti che possono essere un prodotto o un errore. Ah, avete fatto un bel hack per fare",
      "tokens": [
        5360,
        847,
        7317,
        947,
        43857,
        19799,
        517,
        15792,
        18838,
        277,
        517,
        45935,
        265,
        13,
        2438,
        11,
        48201,
        23228,
        517,
        989,
        10339,
        680,
        11994
      ],
      "temperature": 0,
      "avg_logprob": -0.337024390179178,
      "compression_ratio": 1.5384615384615385,
      "no_speech_prob": 2.590704362148699e-8
    },
    {
      "id": 691,
      "seek": 468900,
      "start": 4703.92,
      "end": 4712.56,
      "text": " una roba di questo tipo. Sì, era una taffazzata. Però capisci che, guarda, io è un po' che",
      "tokens": [
        2002,
        3870,
        64,
        1026,
        10263,
        9746,
        13,
        318,
        4749,
        11,
        4249,
        2002,
        1846,
        602,
        9112,
        3274,
        13,
        20533,
        1410,
        271,
        537,
        947,
        11,
        6290,
        64,
        11,
        19785,
        4873,
        517,
        714,
        6,
        947
      ],
      "temperature": 0,
      "avg_logprob": -0.337024390179178,
      "compression_ratio": 1.5384615384615385,
      "no_speech_prob": 2.590704362148699e-8
    },
    {
      "id": 692,
      "seek": 468900,
      "start": 4712.56,
      "end": 4718.68,
      "text": " ragiono col concetto di result, dove c'hai un left side che è il dato, un right side",
      "tokens": [
        17539,
        49020,
        1173,
        1588,
        23778,
        1026,
        1874,
        11,
        23287,
        269,
        6,
        18230,
        517,
        1411,
        1252,
        947,
        4873,
        1930,
        46971,
        11,
        517,
        558,
        1252
      ],
      "temperature": 0,
      "avg_logprob": -0.337024390179178,
      "compression_ratio": 1.5384615384615385,
      "no_speech_prob": 2.590704362148699e-8
    },
    {
      "id": 693,
      "seek": 471868,
      "start": 4718.68,
      "end": 4723.400000000001,
      "text": " che è l'errore. E anche in JavaScript lo uso spesso, ho una classe stupida con queste",
      "tokens": [
        947,
        4873,
        287,
        6,
        260,
        340,
        265,
        13,
        462,
        11585,
        294,
        15778,
        450,
        22728,
        637,
        5557,
        11,
        1106,
        2002,
        32400,
        342,
        1010,
        2887,
        416,
        35455
      ],
      "temperature": 0,
      "avg_logprob": -0.23879044272682884,
      "compression_ratio": 1.641255605381166,
      "no_speech_prob": 1.2824927786425633e-8
    },
    {
      "id": 694,
      "seek": 471868,
      "start": 4723.400000000001,
      "end": 4730.8,
      "text": " due proprietà e poi io faccio, nel caso di Rust, faccio un match o faccio una if in JavaScript",
      "tokens": [
        3462,
        27881,
        1467,
        308,
        19260,
        19785,
        1915,
        8529,
        11,
        15373,
        9666,
        1026,
        34952,
        11,
        1915,
        8529,
        517,
        2995,
        277,
        1915,
        8529,
        2002,
        498,
        294,
        15778
      ],
      "temperature": 0,
      "avg_logprob": -0.23879044272682884,
      "compression_ratio": 1.641255605381166,
      "no_speech_prob": 1.2824927786425633e-8
    },
    {
      "id": 695,
      "seek": 471868,
      "start": 4730.8,
      "end": 4736.280000000001,
      "text": " in modo da validare questi due casi, in modo che anche l'errore diventa parte del dominio,",
      "tokens": [
        294,
        16664,
        1120,
        7363,
        543,
        29729,
        3462,
        22567,
        11,
        294,
        16664,
        947,
        11585,
        287,
        6,
        260,
        340,
        265,
        3414,
        8938,
        6975,
        1103,
        8859,
        1004,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.23879044272682884,
      "compression_ratio": 1.641255605381166,
      "no_speech_prob": 1.2824927786425633e-8
    },
    {
      "id": 696,
      "seek": 471868,
      "start": 4736.280000000001,
      "end": 4742.64,
      "text": " specie in contesti dove il dominio è importante. Certo, sul crude della chiesa non lo farei",
      "tokens": [
        1608,
        414,
        294,
        10287,
        72,
        23287,
        1930,
        8859,
        1004,
        4873,
        9416,
        13,
        383,
        13098,
        11,
        17603,
        30796,
        11618,
        417,
        530,
        64,
        2107,
        450,
        11994,
        72
      ],
      "temperature": 0,
      "avg_logprob": -0.23879044272682884,
      "compression_ratio": 1.641255605381166,
      "no_speech_prob": 1.2824927786425633e-8
    },
    {
      "id": 697,
      "seek": 474264,
      "start": 4742.64,
      "end": 4748.92,
      "text": " manco morto, però mi immagino un sistema di booking che ti dice, immagina che stai",
      "tokens": [
        587,
        1291,
        6599,
        78,
        11,
        12673,
        2752,
        3397,
        559,
        2982,
        517,
        13245,
        1026,
        34424,
        947,
        8757,
        10313,
        11,
        3397,
        559,
        1426,
        947,
        342,
        1301
      ],
      "temperature": 0,
      "avg_logprob": -0.24172887996751435,
      "compression_ratio": 1.6717557251908397,
      "no_speech_prob": 2.631503726036044e-8
    },
    {
      "id": 698,
      "seek": 474264,
      "start": 4748.92,
      "end": 4753.56,
      "text": " facendo un pacchetto e ti dice il servizio non è raggiungibile. Ecco, quello cos'è?",
      "tokens": [
        1915,
        3999,
        517,
        15165,
        339,
        23778,
        308,
        8757,
        10313,
        1930,
        1658,
        590,
        1004,
        2107,
        4873,
        17539,
        7834,
        1063,
        30898,
        13,
        28993,
        1291,
        11,
        22813,
        3792,
        6,
        1462,
        30
      ],
      "temperature": 0,
      "avg_logprob": -0.24172887996751435,
      "compression_ratio": 1.6717557251908397,
      "no_speech_prob": 2.631503726036044e-8
    },
    {
      "id": 699,
      "seek": 474264,
      "start": 4753.56,
      "end": 4757.68,
      "text": " Un errore di piattaforma o un errore di dominio, nel caso del pacchetto? Se è un errore di",
      "tokens": [
        1156,
        45935,
        265,
        1026,
        3895,
        18405,
        837,
        64,
        277,
        517,
        45935,
        265,
        1026,
        8859,
        1004,
        11,
        15373,
        9666,
        1103,
        15165,
        339,
        23778,
        30,
        1100,
        4873,
        517,
        45935,
        265,
        1026
      ],
      "temperature": 0,
      "avg_logprob": -0.24172887996751435,
      "compression_ratio": 1.6717557251908397,
      "no_speech_prob": 2.631503726036044e-8
    },
    {
      "id": 700,
      "seek": 474264,
      "start": 4757.68,
      "end": 4764.04,
      "text": " dominio io devo poterlo trattare come oggetto di dominio, quindi devo anche poter cuerare,",
      "tokens": [
        8859,
        1004,
        19785,
        49717,
        1847,
        260,
        752,
        504,
        1591,
        543,
        808,
        5360,
        847,
        1353,
        1026,
        8859,
        1004,
        11,
        15727,
        49717,
        11585,
        1847,
        260,
        2702,
        260,
        543,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.24172887996751435,
      "compression_ratio": 1.6717557251908397,
      "no_speech_prob": 2.631503726036044e-8
    },
    {
      "id": 701,
      "seek": 474264,
      "start": 4764.04,
      "end": 4771.280000000001,
      "text": " bruttissima parola, dentro quell'errore. Ecco, una cosa che ho visto dei tool GraphQL",
      "tokens": [
        12603,
        83,
        891,
        4775,
        971,
        4711,
        11,
        10856,
        631,
        285,
        6,
        260,
        340,
        265,
        13,
        28993,
        1291,
        11,
        2002,
        10163,
        947,
        1106,
        17558,
        13874,
        2290,
        21884,
        13695
      ],
      "temperature": 0,
      "avg_logprob": -0.24172887996751435,
      "compression_ratio": 1.6717557251908397,
      "no_speech_prob": 2.631503726036044e-8
    },
    {
      "id": 702,
      "seek": 477128,
      "start": 4771.28,
      "end": 4783.4,
      "text": " è quella. Secondo me c'è un po' di lavoro a livello di specifica da fare. Allora, faccio",
      "tokens": [
        4873,
        32234,
        13,
        5736,
        78,
        385,
        269,
        6,
        1462,
        517,
        714,
        6,
        1026,
        42060,
        257,
        1621,
        1913,
        1026,
        2685,
        64,
        1120,
        11994,
        13,
        1057,
        3252,
        11,
        1915,
        8529
      ],
      "temperature": 0,
      "avg_logprob": -0.3199053577993108,
      "compression_ratio": 1.4293193717277486,
      "no_speech_prob": 7.002157644819818e-7
    },
    {
      "id": 703,
      "seek": 477128,
      "start": 4783.4,
      "end": 4790.639999999999,
      "text": " un piccolo passo indietro. Da questo punto di vista, la community. Diciamo che le specifiche",
      "tokens": [
        517,
        13363,
        46086,
        38159,
        1016,
        1684,
        340,
        13,
        3933,
        10263,
        14326,
        1026,
        22553,
        11,
        635,
        1768,
        13,
        413,
        299,
        7415,
        947,
        476,
        1608,
        351,
        9304
      ],
      "temperature": 0,
      "avg_logprob": -0.3199053577993108,
      "compression_ratio": 1.4293193717277486,
      "no_speech_prob": 7.002157644819818e-7
    },
    {
      "id": 704,
      "seek": 477128,
      "start": 4790.639999999999,
      "end": 4797.04,
      "text": " vanno molto a rilento su GraphQL, non c'è una grande pressione di farlo evolvere, questo",
      "tokens": [
        371,
        13484,
        16394,
        257,
        367,
        388,
        15467,
        459,
        21884,
        13695,
        11,
        2107,
        269,
        6,
        1462,
        2002,
        8883,
        1886,
        5328,
        1026,
        1400,
        752,
        7117,
        5887,
        11,
        10263
      ],
      "temperature": 0,
      "avg_logprob": -0.3199053577993108,
      "compression_ratio": 1.4293193717277486,
      "no_speech_prob": 7.002157644819818e-7
    },
    {
      "id": 705,
      "seek": 479704,
      "start": 4797.04,
      "end": 4802.28,
      "text": " è vero. Alla fine ci sono cose nuove, Apollo ha portato delle grandi cose, le subscription",
      "tokens": [
        4873,
        1306,
        78,
        13,
        1057,
        64,
        2489,
        6983,
        9259,
        30261,
        3822,
        1682,
        11,
        25187,
        324,
        2436,
        2513,
        16485,
        45155,
        30261,
        11,
        476,
        17231
      ],
      "temperature": 0,
      "avg_logprob": -0.34003087824041195,
      "compression_ratio": 1.546218487394958,
      "no_speech_prob": 0.000002026132051469176
    },
    {
      "id": 706,
      "seek": 479704,
      "start": 4802.28,
      "end": 4808.24,
      "text": " inizialmente erano un'idea, non esistevano, poi lentamente le hanno definite e buttate",
      "tokens": [
        294,
        590,
        831,
        4082,
        1189,
        3730,
        517,
        6,
        482,
        64,
        11,
        2107,
        785,
        8375,
        85,
        3730,
        11,
        19260,
        23556,
        3439,
        476,
        26595,
        25131,
        308,
        6660,
        473
      ],
      "temperature": 0,
      "avg_logprob": -0.34003087824041195,
      "compression_ratio": 1.546218487394958,
      "no_speech_prob": 0.000002026132051469176
    },
    {
      "id": 707,
      "seek": 479704,
      "start": 4808.24,
      "end": 4816.24,
      "text": " dentro. Non è velocissimo come sistema, quindi, come specifiche. Quindi aggiungere, fare la",
      "tokens": [
        10856,
        13,
        8774,
        4873,
        7806,
        34966,
        808,
        13245,
        11,
        15727,
        11,
        808,
        1608,
        351,
        9304,
        13,
        32534,
        42254,
        1063,
        323,
        11,
        11994,
        635
      ],
      "temperature": 0,
      "avg_logprob": -0.34003087824041195,
      "compression_ratio": 1.546218487394958,
      "no_speech_prob": 0.000002026132051469176
    },
    {
      "id": 708,
      "seek": 479704,
      "start": 4816.24,
      "end": 4823.96,
      "text": " roba che richiedi implica che tu la devi mettere in specifiche, cioè il fatto che se un'entità",
      "tokens": [
        3870,
        64,
        947,
        4593,
        1091,
        72,
        8484,
        2262,
        947,
        2604,
        635,
        31219,
        27812,
        323,
        294,
        1608,
        351,
        9304,
        11,
        41827,
        1930,
        23228,
        947,
        369,
        517,
        6,
        317,
        12445
      ],
      "temperature": 0,
      "avg_logprob": -0.34003087824041195,
      "compression_ratio": 1.546218487394958,
      "no_speech_prob": 0.000002026132051469176
    },
    {
      "id": 709,
      "seek": 482396,
      "start": 4823.96,
      "end": 4829.52,
      "text": " non viene restituita tu potresti avere l'errore sul posto. Questa è appesante come affermazione.",
      "tokens": [
        2107,
        19561,
        1472,
        270,
        1983,
        64,
        2604,
        1847,
        4149,
        72,
        37914,
        287,
        6,
        260,
        340,
        265,
        17603,
        2183,
        78,
        13,
        2326,
        7841,
        4873,
        724,
        279,
        2879,
        808,
        2096,
        966,
        12928,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.318862789967021,
      "compression_ratio": 1.6025641025641026,
      "no_speech_prob": 0.0000011015940799552482
    },
    {
      "id": 710,
      "seek": 482396,
      "start": 4829.52,
      "end": 4834.88,
      "text": " È bisogno pensarla bene. E me la devi restituire indipendentemente dal fatto che io te l'abbia",
      "tokens": [
        34495,
        40505,
        1771,
        18321,
        875,
        2537,
        13,
        462,
        385,
        635,
        31219,
        1472,
        6380,
        621,
        1016,
        647,
        521,
        317,
        16288,
        11702,
        23228,
        947,
        19785,
        535,
        287,
        6,
        10797,
        654
      ],
      "temperature": 0,
      "avg_logprob": -0.318862789967021,
      "compression_ratio": 1.6025641025641026,
      "no_speech_prob": 0.0000011015940799552482
    },
    {
      "id": 711,
      "seek": 482396,
      "start": 4834.88,
      "end": 4841.76,
      "text": " chiesta o non te l'abbia chiesta. Perché è quello il problema. Perché se tu ci pensi,",
      "tokens": [
        417,
        38804,
        277,
        2107,
        535,
        287,
        6,
        10797,
        654,
        417,
        38804,
        13,
        47978,
        4873,
        7178,
        752,
        1930,
        12395,
        13,
        47978,
        369,
        2604,
        6983,
        6099,
        72,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.318862789967021,
      "compression_ratio": 1.6025641025641026,
      "no_speech_prob": 0.0000011015940799552482
    },
    {
      "id": 712,
      "seek": 482396,
      "start": 4841.76,
      "end": 4846.4,
      "text": " dovrebbe essere abbastanza semplice, tu per ogni entità restituisci eventualmente l'errore,",
      "tokens": [
        30870,
        39487,
        19799,
        16903,
        525,
        20030,
        4361,
        564,
        573,
        11,
        2604,
        680,
        33189,
        948,
        12445,
        1472,
        6380,
        271,
        537,
        33160,
        4082,
        287,
        6,
        260,
        340,
        265,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.318862789967021,
      "compression_ratio": 1.6025641025641026,
      "no_speech_prob": 0.0000011015940799552482
    },
    {
      "id": 713,
      "seek": 484640,
      "start": 4846.4,
      "end": 4855.679999999999,
      "text": " io nel caso se ho un errore, ti mostro l'errore, si fa. Vuoi fare un hacking da questo punto",
      "tokens": [
        19785,
        15373,
        9666,
        369,
        1106,
        517,
        45935,
        265,
        11,
        8757,
        881,
        340,
        287,
        6,
        260,
        340,
        265,
        11,
        1511,
        2050,
        13,
        37703,
        4869,
        11994,
        517,
        31422,
        1120,
        10263,
        14326
      ],
      "temperature": 0,
      "avg_logprob": -0.332572705036885,
      "compression_ratio": 1.8089430894308942,
      "no_speech_prob": 0.0000025612764602556126
    },
    {
      "id": 714,
      "seek": 484640,
      "start": 4855.679999999999,
      "end": 4859.44,
      "text": " di vista, lo fai. Il problema è che se tu non lo richiedi, quell'errore lui non ti",
      "tokens": [
        1026,
        22553,
        11,
        450,
        283,
        1301,
        13,
        4416,
        12395,
        4873,
        947,
        369,
        2604,
        2107,
        450,
        4593,
        1091,
        72,
        11,
        631,
        285,
        6,
        260,
        340,
        265,
        8783,
        2107,
        8757
      ],
      "temperature": 0,
      "avg_logprob": -0.332572705036885,
      "compression_ratio": 1.8089430894308942,
      "no_speech_prob": 0.0000025612764602556126
    },
    {
      "id": 715,
      "seek": 484640,
      "start": 4859.44,
      "end": 4864.879999999999,
      "text": " da indietro. Invece, appunto, lo metto tu, di base ti da una strutturina separata con",
      "tokens": [
        1120,
        1016,
        1684,
        340,
        13,
        682,
        303,
        384,
        11,
        724,
        24052,
        11,
        450,
        1131,
        1353,
        2604,
        11,
        1026,
        3096,
        8757,
        1120,
        2002,
        1056,
        13478,
        374,
        1426,
        3128,
        3274,
        416
      ],
      "temperature": 0,
      "avg_logprob": -0.332572705036885,
      "compression_ratio": 1.8089430894308942,
      "no_speech_prob": 0.0000025612764602556126
    },
    {
      "id": 716,
      "seek": 484640,
      "start": 4864.879999999999,
      "end": 4869.639999999999,
      "text": " un elenco degli errori in cui ti dice c'è stato un errore X in questo punto, cioè ti",
      "tokens": [
        517,
        806,
        268,
        1291,
        32079,
        6713,
        72,
        294,
        22929,
        8757,
        10313,
        269,
        6,
        1462,
        29657,
        517,
        45935,
        265,
        1783,
        294,
        10263,
        14326,
        11,
        41827,
        8757
      ],
      "temperature": 0,
      "avg_logprob": -0.332572705036885,
      "compression_ratio": 1.8089430894308942,
      "no_speech_prob": 0.0000025612764602556126
    },
    {
      "id": 717,
      "seek": 484640,
      "start": 4869.639999999999,
      "end": 4874.759999999999,
      "text": " dice proprio c'è stato un errore X in questo punto, quindi tu devi anche andarti a ricostruire",
      "tokens": [
        10313,
        28203,
        269,
        6,
        1462,
        29657,
        517,
        45935,
        265,
        1783,
        294,
        10263,
        14326,
        11,
        15727,
        2604,
        31219,
        11585,
        293,
        40155,
        257,
        21040,
        555,
        894,
        621
      ],
      "temperature": 0,
      "avg_logprob": -0.332572705036885,
      "compression_ratio": 1.8089430894308942,
      "no_speech_prob": 0.0000025612764602556126
    },
    {
      "id": 718,
      "seek": 487476,
      "start": 4874.76,
      "end": 4879.4800000000005,
      "text": " dove c'è stato questo errore, che il 90% delle volte te ne freghi e dici c'è stato",
      "tokens": [
        23287,
        269,
        6,
        1462,
        29657,
        10263,
        45935,
        265,
        11,
        947,
        1930,
        4289,
        4,
        16485,
        37801,
        535,
        408,
        2130,
        70,
        4954,
        308,
        274,
        8787,
        269,
        6,
        1462,
        29657
      ],
      "temperature": 0,
      "avg_logprob": -0.28804558721081963,
      "compression_ratio": 1.5145228215767634,
      "no_speech_prob": 1.1793579801633314e-7
    },
    {
      "id": 719,
      "seek": 487476,
      "start": 4879.4800000000005,
      "end": 4888.8,
      "text": " un errore e non mostri i dati. Ma se sei federato ti puoi anche sparare in bocca. Quello è",
      "tokens": [
        517,
        45935,
        265,
        308,
        2107,
        881,
        470,
        741,
        1137,
        72,
        13,
        4042,
        369,
        10842,
        38024,
        2513,
        8757,
        2362,
        4869,
        11585,
        45954,
        543,
        294,
        748,
        22394,
        13,
        4493,
        1913,
        4873
      ],
      "temperature": 0,
      "avg_logprob": -0.28804558721081963,
      "compression_ratio": 1.5145228215767634,
      "no_speech_prob": 1.1793579801633314e-7
    },
    {
      "id": 720,
      "seek": 487476,
      "start": 4888.8,
      "end": 4895.56,
      "text": " un altro problema. Però bene o male gli errori vengono riportati. Davide non ti sento più,",
      "tokens": [
        517,
        40924,
        12395,
        13,
        20533,
        2537,
        277,
        7133,
        17161,
        6713,
        72,
        371,
        1501,
        8957,
        12782,
        477,
        6908,
        13,
        3724,
        482,
        2107,
        8757,
        2279,
        78,
        10589,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.28804558721081963,
      "compression_ratio": 1.5145228215767634,
      "no_speech_prob": 1.1793579801633314e-7
    },
    {
      "id": 721,
      "seek": 487476,
      "start": 4895.56,
      "end": 4904.72,
      "text": " è successo qualcosa. Non ti sento. Sei sparito completamente. Allora datemi giusto un secondo,",
      "tokens": [
        4873,
        2245,
        78,
        42400,
        13,
        8774,
        8757,
        2279,
        78,
        13,
        49229,
        45954,
        3528,
        28381,
        13,
        1057,
        3252,
        1137,
        13372,
        1735,
        48260,
        517,
        41601,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.28804558721081963,
      "compression_ratio": 1.5145228215767634,
      "no_speech_prob": 1.1793579801633314e-7
    },
    {
      "id": 722,
      "seek": 490472,
      "start": 4904.72,
      "end": 4912.12,
      "text": " prima di andare in chiusura, perché abbiamo un momentino che non possiamo sboccare.",
      "tokens": [
        50364,
        19507,
        1026,
        42742,
        294,
        417,
        4872,
        2991,
        11,
        14303,
        22815,
        517,
        1623,
        2982,
        947,
        2107,
        44758,
        262,
        65,
        905,
        5685,
        13,
        50734
      ],
      "temperature": 0,
      "avg_logprob": -0.5769238074620565,
      "compression_ratio": 1.0632911392405062,
      "no_speech_prob": 0.000018631619241205044
    },
    {
      "id": 723,
      "seek": 493472,
      "start": 4934.72,
      "end": 4943.64,
      "text": " È il momento di ringraziare i donatori della settimana, le persone che ci prendono sulle",
      "tokens": [
        34495,
        1930,
        9333,
        1026,
        4875,
        424,
        3992,
        543,
        741,
        500,
        39842,
        11618,
        5584,
        36497,
        11,
        476,
        29944,
        947,
        6983,
        9866,
        8957,
        459,
        2447
      ],
      "temperature": 0,
      "avg_logprob": -0.20577802363130235,
      "compression_ratio": 1.6044444444444443,
      "no_speech_prob": 0.8662324547767639
    },
    {
      "id": 724,
      "seek": 493472,
      "start": 4943.64,
      "end": 4949.240000000001,
      "text": " spalle e ci accompagnano facendo in modo che ogni settimana possiamo portare del contenuto",
      "tokens": [
        637,
        11780,
        308,
        6983,
        18037,
        4535,
        3730,
        1915,
        3999,
        294,
        16664,
        947,
        33189,
        5584,
        36497,
        44758,
        2436,
        543,
        1103,
        21795,
        8262
      ],
      "temperature": 0,
      "avg_logprob": -0.20577802363130235,
      "compression_ratio": 1.6044444444444443,
      "no_speech_prob": 0.8662324547767639
    },
    {
      "id": 725,
      "seek": 493472,
      "start": 4949.240000000001,
      "end": 4957.2,
      "text": " fresco. Abbiamo questa settimana alcuni donatori, questo perché sono un po' di più del solito",
      "tokens": [
        25235,
        1291,
        13,
        32673,
        7415,
        16540,
        5584,
        36497,
        20005,
        24307,
        500,
        39842,
        11,
        10263,
        14303,
        9259,
        517,
        714,
        6,
        1026,
        10589,
        1103,
        1404,
        3528
      ],
      "temperature": 0,
      "avg_logprob": -0.20577802363130235,
      "compression_ratio": 1.6044444444444443,
      "no_speech_prob": 0.8662324547767639
    },
    {
      "id": 726,
      "seek": 493472,
      "start": 4957.2,
      "end": 4961.92,
      "text": " perché ci siamo fermati qualche settimana per Natale. Il primo è Giovanni Italiano",
      "tokens": [
        14303,
        6983,
        33459,
        26558,
        6908,
        38737,
        5584,
        36497,
        680,
        6821,
        1220,
        13,
        4416,
        38671,
        4873,
        47089,
        35832,
        8158,
        6254
      ],
      "temperature": 0,
      "avg_logprob": -0.20577802363130235,
      "compression_ratio": 1.6044444444444443,
      "no_speech_prob": 0.8662324547767639
    },
    {
      "id": 727,
      "seek": 496192,
      "start": 4961.92,
      "end": 4968.4800000000005,
      "text": " che ha replicato una donazione facendo una donazione di 5 birre con un messaggio. Auguri",
      "tokens": [
        947,
        324,
        3248,
        299,
        2513,
        2002,
        500,
        12928,
        1915,
        3999,
        2002,
        500,
        12928,
        1026,
        1025,
        1904,
        265,
        416,
        517,
        2082,
        30763,
        13,
        6088,
        9744
      ],
      "temperature": 0,
      "avg_logprob": -0.2545343801515912,
      "compression_ratio": 1.6712328767123288,
      "no_speech_prob": 4.181185602192272e-7
    },
    {
      "id": 728,
      "seek": 496192,
      "start": 4968.4800000000005,
      "end": 4977.4800000000005,
      "text": " di buone feste e grazie per il podcast. Grazie a te Giovanni. Abbiamo anche Leonardo Sabato",
      "tokens": [
        1026,
        758,
        546,
        6633,
        68,
        308,
        1295,
        3283,
        680,
        1930,
        7367,
        13,
        8985,
        3283,
        257,
        535,
        47089,
        35832,
        13,
        32673,
        7415,
        11585,
        36523,
        13915,
        2513
      ],
      "temperature": 0,
      "avg_logprob": -0.2545343801515912,
      "compression_ratio": 1.6712328767123288,
      "no_speech_prob": 4.181185602192272e-7
    },
    {
      "id": 729,
      "seek": 496192,
      "start": 4977.4800000000005,
      "end": 4985.52,
      "text": " o Sabato, sbaglio tutti gli accenti, li metto in modo randomico, abbi pazienza di me Leonardo.",
      "tokens": [
        277,
        13915,
        2513,
        11,
        262,
        17282,
        19987,
        19822,
        17161,
        11982,
        72,
        11,
        375,
        1131,
        1353,
        294,
        16664,
        4974,
        2789,
        11,
        410,
        5614,
        30032,
        42331,
        1026,
        385,
        36523,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.2545343801515912,
      "compression_ratio": 1.6712328767123288,
      "no_speech_prob": 4.181185602192272e-7
    },
    {
      "id": 730,
      "seek": 496192,
      "start": 4985.52,
      "end": 4990.56,
      "text": " Grazie per tutto il tempo che togliete a voi per dedicarlo al podcast e soprattutto grazie",
      "tokens": [
        8985,
        3283,
        680,
        23048,
        1930,
        8972,
        947,
        281,
        7191,
        40462,
        257,
        20931,
        680,
        4172,
        7953,
        752,
        419,
        7367,
        308,
        50002,
        1295,
        3283
      ],
      "temperature": 0,
      "avg_logprob": -0.2545343801515912,
      "compression_ratio": 1.6712328767123288,
      "no_speech_prob": 4.181185602192272e-7
    },
    {
      "id": 731,
      "seek": 499056,
      "start": 4990.56,
      "end": 4997.52,
      "text": " per la condivisione. Bravi tutti, grazie a te perché fai in modo che noi possiamo veramente",
      "tokens": [
        680,
        635,
        2224,
        592,
        1991,
        68,
        13,
        4991,
        4917,
        19822,
        11,
        1295,
        3283,
        257,
        535,
        14303,
        283,
        1301,
        294,
        16664,
        947,
        22447,
        44758,
        50079
      ],
      "temperature": 0,
      "avg_logprob": -0.23713659311269786,
      "compression_ratio": 1.433862433862434,
      "no_speech_prob": 2.102431864159371e-7
    },
    {
      "id": 732,
      "seek": 499056,
      "start": 4997.52,
      "end": 5002.96,
      "text": " arrivare alle vostre orecchie ogni settimana. Ma abbiamo un altro donatore, 3 birre Andrea",
      "tokens": [
        30697,
        543,
        5430,
        28944,
        265,
        277,
        13867,
        339,
        414,
        33189,
        5584,
        36497,
        13,
        4042,
        22815,
        517,
        40924,
        500,
        43148,
        11,
        805,
        1904,
        265,
        24215
      ],
      "temperature": 0,
      "avg_logprob": -0.23713659311269786,
      "compression_ratio": 1.433862433862434,
      "no_speech_prob": 2.102431864159371e-7
    },
    {
      "id": 733,
      "seek": 499056,
      "start": 5002.96,
      "end": 5012.1,
      "text": " Quintino che insieme appunto a Leonardo e a Giovanni hanno fatto in modo che noi questa",
      "tokens": [
        2326,
        686,
        2982,
        947,
        1028,
        44940,
        724,
        24052,
        257,
        36523,
        308,
        257,
        47089,
        35832,
        26595,
        23228,
        294,
        16664,
        947,
        22447,
        16540
      ],
      "temperature": 0,
      "avg_logprob": -0.23713659311269786,
      "compression_ratio": 1.433862433862434,
      "no_speech_prob": 2.102431864159371e-7
    },
    {
      "id": 734,
      "seek": 501210,
      "start": 5012.1,
      "end": 5022.360000000001,
      "text": " settimana abbiamo potuto pubblicare l'episodio. Dicevamo, nel frattempo è esploso tutto,",
      "tokens": [
        5584,
        36497,
        22815,
        1847,
        8262,
        1535,
        11489,
        543,
        287,
        6,
        595,
        271,
        378,
        1004,
        13,
        413,
        573,
        85,
        10502,
        11,
        15373,
        431,
        1591,
        443,
        2259,
        4873,
        785,
        564,
        9869,
        23048,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.2427603077684712,
      "compression_ratio": 1.5427350427350428,
      "no_speech_prob": 2.616516496800614e-7
    },
    {
      "id": 735,
      "seek": 501210,
      "start": 5022.360000000001,
      "end": 5025.88,
      "text": " però parlavamo appunto della gestione degli errori. Secondo me comunque è uno di quegli",
      "tokens": [
        12673,
        13734,
        706,
        10502,
        724,
        24052,
        11618,
        7219,
        5328,
        32079,
        6713,
        72,
        13,
        5736,
        78,
        385,
        45736,
        4873,
        8526,
        1026,
        631,
        41443
      ],
      "temperature": 0,
      "avg_logprob": -0.2427603077684712,
      "compression_ratio": 1.5427350427350428,
      "no_speech_prob": 2.616516496800614e-7
    },
    {
      "id": 736,
      "seek": 501210,
      "start": 5025.88,
      "end": 5034.280000000001,
      "text": " ambiti dove si può aprire una bella discussione. Sì, assolutamente sì. Cioè è uno di quegli",
      "tokens": [
        3913,
        8707,
        23287,
        1511,
        26526,
        1882,
        38920,
        2002,
        312,
        3505,
        5017,
        68,
        13,
        318,
        4749,
        11,
        1256,
        2308,
        3439,
        49267,
        13,
        383,
        35983,
        4873,
        8526,
        1026,
        631,
        41443
      ],
      "temperature": 0,
      "avg_logprob": -0.2427603077684712,
      "compression_ratio": 1.5427350427350428,
      "no_speech_prob": 2.616516496800614e-7
    },
    {
      "id": 737,
      "seek": 501210,
      "start": 5034.280000000001,
      "end": 5038.84,
      "text": " ambiti in cui andrebbero affrontati un po' presi per le corne, trovare una soluzione",
      "tokens": [
        3913,
        8707,
        294,
        22929,
        293,
        22692,
        46659,
        2096,
        10001,
        6908,
        517,
        714,
        6,
        1183,
        72,
        680,
        476,
        1181,
        716,
        11,
        35449,
        543,
        2002,
        1404,
        3334,
        5328
      ],
      "temperature": 0,
      "avg_logprob": -0.2427603077684712,
      "compression_ratio": 1.5427350427350428,
      "no_speech_prob": 2.616516496800614e-7
    },
    {
      "id": 738,
      "seek": 503884,
      "start": 5038.84,
      "end": 5042.52,
      "text": " comune per tutti perché poi una volta che hai la soluzione comune, anche il tooling",
      "tokens": [
        395,
        2613,
        680,
        19822,
        14303,
        19260,
        2002,
        18765,
        947,
        21822,
        635,
        1404,
        3334,
        5328,
        395,
        2613,
        11,
        11585,
        1930,
        46593
      ],
      "temperature": 0,
      "avg_logprob": -0.2524900227567575,
      "compression_ratio": 1.6566037735849057,
      "no_speech_prob": 2.0061541761151602e-7
    },
    {
      "id": 739,
      "seek": 503884,
      "start": 5042.52,
      "end": 5047.88,
      "text": " dopo ce l'hai che ti funziona out of the box senza dover di nuovo reinventare la ruota",
      "tokens": [
        35196,
        1769,
        287,
        6,
        18230,
        947,
        8757,
        49345,
        21758,
        484,
        295,
        264,
        2424,
        36208,
        360,
        331,
        1026,
        49348,
        33477,
        543,
        635,
        5420,
        5377
      ],
      "temperature": 0,
      "avg_logprob": -0.2524900227567575,
      "compression_ratio": 1.6566037735849057,
      "no_speech_prob": 2.0061541761151602e-7
    },
    {
      "id": 740,
      "seek": 503884,
      "start": 5047.88,
      "end": 5054.32,
      "text": " tutte le volte perché poi la problematica è lì. Cioè che poi detto tra mette, secondo",
      "tokens": [
        38632,
        476,
        37801,
        14303,
        19260,
        635,
        1154,
        267,
        2262,
        4873,
        287,
        4749,
        13,
        383,
        35983,
        947,
        19260,
        41031,
        944,
        1131,
        975,
        11,
        41601
      ],
      "temperature": 0,
      "avg_logprob": -0.2524900227567575,
      "compression_ratio": 1.6566037735849057,
      "no_speech_prob": 2.0061541761151602e-7
    },
    {
      "id": 741,
      "seek": 503884,
      "start": 5054.32,
      "end": 5058.96,
      "text": " me la soluzione è anche abbastanza semplice, cioè restituisci un tipo che può essere",
      "tokens": [
        385,
        635,
        1404,
        3334,
        5328,
        4873,
        11585,
        16903,
        525,
        20030,
        4361,
        564,
        573,
        11,
        41827,
        1472,
        6380,
        271,
        537,
        517,
        9746,
        947,
        26526,
        19799
      ],
      "temperature": 0,
      "avg_logprob": -0.2524900227567575,
      "compression_ratio": 1.6566037735849057,
      "no_speech_prob": 2.0061541761151602e-7
    },
    {
      "id": 742,
      "seek": 503884,
      "start": 5058.96,
      "end": 5066.4400000000005,
      "text": " i dati o l'errore, ok? Puoi definire l'errore se tu non lo definisci all'interno del nodo",
      "tokens": [
        741,
        1137,
        72,
        277,
        287,
        6,
        260,
        340,
        265,
        11,
        3133,
        30,
        13605,
        4869,
        1561,
        621,
        287,
        6,
        260,
        340,
        265,
        369,
        2604,
        2107,
        450,
        1561,
        271,
        537,
        439,
        6,
        5106,
        1771,
        1103,
        15224,
        78
      ],
      "temperature": 0,
      "avg_logprob": -0.2524900227567575,
      "compression_ratio": 1.6566037735849057,
      "no_speech_prob": 2.0061541761151602e-7
    },
    {
      "id": 743,
      "seek": 506644,
      "start": 5066.44,
      "end": 5071.879999999999,
      "text": " no? Di GraphQL. Se tu non lo definisci in caso d'errore hai o un errore generico o null.",
      "tokens": [
        572,
        30,
        8789,
        21884,
        13695,
        13,
        1100,
        2604,
        2107,
        450,
        1561,
        271,
        537,
        294,
        9666,
        274,
        6,
        260,
        340,
        265,
        21822,
        277,
        517,
        45935,
        265,
        1337,
        2789,
        277,
        18184,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.2896228075027466,
      "compression_ratio": 1.3915343915343916,
      "no_speech_prob": 3.769455148017187e-8
    },
    {
      "id": 744,
      "seek": 506644,
      "start": 5071.879999999999,
      "end": 5078.639999999999,
      "text": " E alla fine così lo risolvi. Però diciamo che posticipiamo questa discussione magari",
      "tokens": [
        462,
        11591,
        2489,
        23278,
        450,
        2253,
        401,
        4917,
        13,
        20533,
        14285,
        7415,
        947,
        2183,
        6537,
        7415,
        16540,
        5017,
        68,
        49932
      ],
      "temperature": 0,
      "avg_logprob": -0.2896228075027466,
      "compression_ratio": 1.3915343915343916,
      "no_speech_prob": 3.769455148017187e-8
    },
    {
      "id": 745,
      "seek": 506644,
      "start": 5078.639999999999,
      "end": 5085.5199999999995,
      "text": " in uno dei nostri working group su GraphQL. Assolutamente. Abbiamo trovato il topic per",
      "tokens": [
        294,
        8526,
        13874,
        10397,
        470,
        1364,
        1594,
        459,
        21884,
        13695,
        13,
        6281,
        2308,
        3439,
        13,
        32673,
        7415,
        35449,
        2513,
        1930,
        4829,
        680
      ],
      "temperature": 0,
      "avg_logprob": -0.2896228075027466,
      "compression_ratio": 1.3915343915343916,
      "no_speech_prob": 3.769455148017187e-8
    },
    {
      "id": 746,
      "seek": 508552,
      "start": 5085.52,
      "end": 5097.040000000001,
      "text": " il prossimo working group. Io guardavo l'orologio, siamo avvantissimo con l'orario, non abbiamo",
      "tokens": [
        1930,
        48794,
        6934,
        1364,
        1594,
        13,
        19239,
        6290,
        25713,
        287,
        6,
        284,
        1132,
        1004,
        11,
        33459,
        1305,
        5219,
        34966,
        416,
        287,
        6,
        284,
        4912,
        11,
        2107,
        22815
      ],
      "temperature": 0,
      "avg_logprob": -0.2115905485957502,
      "compression_ratio": 1.5284090909090908,
      "no_speech_prob": 0.000001328768576058792
    },
    {
      "id": 747,
      "seek": 508552,
      "start": 5097.040000000001,
      "end": 5105.400000000001,
      "text": " dei nuovi donatori ahimè, però vi ricordo che abbiamo la parte nel sito di supportaci",
      "tokens": [
        13874,
        37802,
        4917,
        500,
        39842,
        3716,
        332,
        1462,
        11,
        12673,
        1932,
        21040,
        23872,
        947,
        22815,
        635,
        6975,
        15373,
        1394,
        78,
        1026,
        1406,
        22086
      ],
      "temperature": 0,
      "avg_logprob": -0.2115905485957502,
      "compression_ratio": 1.5284090909090908,
      "no_speech_prob": 0.000001328768576058792
    },
    {
      "id": 748,
      "seek": 508552,
      "start": 5105.400000000001,
      "end": 5111.4400000000005,
      "text": " se vi va buttateci un occhio e se vi va di supportarci fatelo pure e arriviamo dritti",
      "tokens": [
        369,
        1932,
        2773,
        6660,
        473,
        537,
        517,
        10409,
        31033,
        308,
        369,
        1932,
        2773,
        1026,
        1406,
        289,
        537,
        4046,
        10590,
        6075,
        308,
        30697,
        7415,
        1224,
        21558
      ],
      "temperature": 0,
      "avg_logprob": -0.2115905485957502,
      "compression_ratio": 1.5284090909090908,
      "no_speech_prob": 0.000001328768576058792
    },
    {
      "id": 749,
      "seek": 511144,
      "start": 5111.44,
      "end": 5119.24,
      "text": " dritti dritti al momento tipico e topico del nostro podcast. Il momento, il Paese dei Balocchi.",
      "tokens": [
        1224,
        21558,
        1224,
        21558,
        419,
        9333,
        4125,
        2789,
        308,
        1192,
        2789,
        1103,
        35779,
        7367,
        13,
        4416,
        9333,
        11,
        1930,
        3426,
        1130,
        13874,
        13140,
        905,
        8036,
        13
      ],
      "temperature": 0,
      "avg_logprob": -0.23796998449118742,
      "compression_ratio": 1.5113636363636365,
      "no_speech_prob": 0.0000019033835769732832
    },
    {
      "id": 750,
      "seek": 511144,
      "start": 5119.24,
      "end": 5132.24,
      "text": " Ed è questo il momento nel quale io chiedo a Davide se ha qualcosa da condividere con",
      "tokens": [
        3977,
        4873,
        10263,
        1930,
        9333,
        15373,
        421,
        1220,
        19785,
        417,
        36035,
        257,
        3724,
        482,
        369,
        324,
        42400,
        1120,
        2224,
        1843,
        323,
        416
      ],
      "temperature": 0,
      "avg_logprob": -0.23796998449118742,
      "compression_ratio": 1.5113636363636365,
      "no_speech_prob": 0.0000019033835769732832
    },
    {
      "id": 751,
      "seek": 511144,
      "start": 5132.24,
      "end": 5139.16,
      "text": " noi. Allora, ci ho pensato un po' perché sono andato un po' a guardarmi le vecchie",
      "tokens": [
        22447,
        13,
        1057,
        3252,
        11,
        6983,
        1106,
        6099,
        2513,
        517,
        714,
        6,
        14303,
        9259,
        293,
        2513,
        517,
        714,
        6,
        257,
        6290,
        289,
        3057,
        476,
        42021,
        339,
        414
      ],
      "temperature": 0,
      "avg_logprob": -0.23796998449118742,
      "compression_ratio": 1.5113636363636365,
      "no_speech_prob": 0.0000019033835769732832
    },
    {
      "id": 752,
      "seek": 513916,
      "start": 5139.16,
      "end": 5144.24,
      "text": " puntate e ho scoperto all'ultimo di questa cosa quindi me la sono dovuta un po' improvvisare,",
      "tokens": [
        18212,
        473,
        308,
        1106,
        795,
        404,
        13098,
        439,
        6,
        723,
        6934,
        1026,
        16540,
        10163,
        15727,
        385,
        635,
        9259,
        30870,
        12093,
        517,
        714,
        6,
        29424,
        4938,
        543,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.3005902299257082,
      "compression_ratio": 1.5695652173913044,
      "no_speech_prob": 0.000001679720526226447
    },
    {
      "id": 753,
      "seek": 513916,
      "start": 5144.24,
      "end": 5151.599999999999,
      "text": " ma ciò il regalo giusto. Il regalo giusto che è un libro che secondo me ogni sviluppatore",
      "tokens": [
        463,
        6983,
        4293,
        1930,
        1121,
        10334,
        1735,
        48260,
        13,
        4416,
        1121,
        10334,
        1735,
        48260,
        947,
        4873,
        517,
        29354,
        947,
        41601,
        385,
        33189,
        17342,
        388,
        10504,
        43148
      ],
      "temperature": 0,
      "avg_logprob": -0.3005902299257082,
      "compression_ratio": 1.5695652173913044,
      "no_speech_prob": 0.000001679720526226447
    },
    {
      "id": 754,
      "seek": 513916,
      "start": 5151.599999999999,
      "end": 5156,
      "text": " e ogni persona che lavora nel nostro campo dovrebbe aver letto o quantomeno aver sentito",
      "tokens": [
        308,
        33189,
        12184,
        947,
        20923,
        3252,
        15373,
        35779,
        29691,
        30870,
        39487,
        18247,
        718,
        1353,
        277,
        4426,
        4726,
        78,
        18247,
        2279,
        3528
      ],
      "temperature": 0,
      "avg_logprob": -0.3005902299257082,
      "compression_ratio": 1.5695652173913044,
      "no_speech_prob": 0.000001679720526226447
    },
    {
      "id": 755,
      "seek": 513916,
      "start": 5156,
      "end": 5161.5599999999995,
      "text": " illuminare sperando che non l'abbia già regalato qualcuno prima di me, che è Quality",
      "tokens": [
        28593,
        543,
        24152,
        1806,
        947,
        2107,
        287,
        6,
        10797,
        654,
        30469,
        1121,
        304,
        2513,
        32101,
        12638,
        19507,
        1026,
        385,
        11,
        947,
        4873,
        28892
      ],
      "temperature": 0,
      "avg_logprob": -0.3005902299257082,
      "compression_ratio": 1.5695652173913044,
      "no_speech_prob": 0.000001679720526226447
    },
    {
      "id": 756,
      "seek": 516156,
      "start": 5161.56,
      "end": 5178.64,
      "text": " Land. È la prima volta che lo sento. È un libro, potrei dire un tipo, la guida intergalattica,",
      "tokens": [
        6607,
        13,
        34495,
        635,
        19507,
        18765,
        947,
        450,
        2279,
        78,
        13,
        34495,
        517,
        29354,
        11,
        1847,
        10271,
        1264,
        517,
        9746,
        11,
        635,
        695,
        2887,
        728,
        9800,
        1591,
        2262,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.35782490746449613,
      "compression_ratio": 1.2971014492753623,
      "no_speech_prob": 3.6534789416009517e-8
    },
    {
      "id": 757,
      "seek": 516156,
      "start": 5178.64,
      "end": 5182.84,
      "text": " ma non c'entra assolutamente nulla e non è neanche di quella brillantezza. Però,",
      "tokens": [
        463,
        2107,
        269,
        6,
        317,
        424,
        1256,
        2308,
        3439,
        18184,
        64,
        308,
        2107,
        4873,
        408,
        22806,
        1026,
        32234,
        8695,
        2879,
        26786,
        13,
        20533,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.35782490746449613,
      "compression_ratio": 1.2971014492753623,
      "no_speech_prob": 3.6534789416009517e-8
    },
    {
      "id": 758,
      "seek": 518284,
      "start": 5182.84,
      "end": 5192.400000000001,
      "text": " aspetta che lo cerco per bene, Quality Land di Mark Kling che esiste in due versioni,",
      "tokens": [
        382,
        7275,
        1328,
        947,
        450,
        10146,
        1291,
        680,
        2537,
        11,
        28892,
        6607,
        1026,
        3934,
        591,
        1688,
        947,
        785,
        8375,
        294,
        3462,
        3037,
        72,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.25298100765620435,
      "compression_ratio": 1.6839622641509433,
      "no_speech_prob": 0.000002332060830667615
    },
    {
      "id": 759,
      "seek": 518284,
      "start": 5192.400000000001,
      "end": 5197.6,
      "text": " per ottimisti e per pessimisti. Il libro è lo stesso, non cambia molto, cambiano gli",
      "tokens": [
        680,
        4337,
        31208,
        45308,
        308,
        680,
        37399,
        45308,
        13,
        4416,
        29354,
        4873,
        450,
        44413,
        11,
        2107,
        18751,
        654,
        16394,
        11,
        18751,
        6254,
        17161
      ],
      "temperature": 0,
      "avg_logprob": -0.25298100765620435,
      "compression_ratio": 1.6839622641509433,
      "no_speech_prob": 0.000002332060830667615
    },
    {
      "id": 760,
      "seek": 518284,
      "start": 5197.6,
      "end": 5202.08,
      "text": " intermezzi che ci sono fra un capitale e l'altro, che in quello per ottimisti sono più ottimisti",
      "tokens": [
        728,
        19917,
        3992,
        947,
        6983,
        9259,
        6600,
        517,
        33807,
        1220,
        308,
        287,
        6,
        47484,
        11,
        947,
        294,
        22813,
        680,
        4337,
        31208,
        45308,
        9259,
        10589,
        4337,
        31208,
        45308
      ],
      "temperature": 0,
      "avg_logprob": -0.25298100765620435,
      "compression_ratio": 1.6839622641509433,
      "no_speech_prob": 0.000002332060830667615
    },
    {
      "id": 761,
      "seek": 518284,
      "start": 5202.08,
      "end": 5209.32,
      "text": " e quello per pessimisti sono un po' più pessimistici, che racconta il mondo che stiamo",
      "tokens": [
        308,
        22813,
        680,
        37399,
        45308,
        9259,
        517,
        714,
        6,
        10589,
        37399,
        3142,
        72,
        11,
        947,
        4129,
        9000,
        64,
        1930,
        40499,
        947,
        342,
        7415
      ],
      "temperature": 0,
      "avg_logprob": -0.25298100765620435,
      "compression_ratio": 1.6839622641509433,
      "no_speech_prob": 0.000002332060830667615
    },
    {
      "id": 762,
      "seek": 520932,
      "start": 5209.32,
      "end": 5216.16,
      "text": " vivendo, il mondo dei social, il mondo dei sistemi di ricerca, il mondo della delivery,",
      "tokens": [
        11005,
        3999,
        11,
        1930,
        40499,
        13874,
        2093,
        11,
        1930,
        40499,
        13874,
        10555,
        13372,
        1026,
        21040,
        36127,
        11,
        1930,
        40499,
        11618,
        8982,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.20430452578535704,
      "compression_ratio": 1.6555023923444976,
      "no_speech_prob": 1.5623942317688488e-7
    },
    {
      "id": 763,
      "seek": 520932,
      "start": 5216.16,
      "end": 5224.08,
      "text": " portato all'estremo, all'estremo dove si può arrivare. Faccio un esempio, non fai",
      "tokens": [
        2436,
        2513,
        439,
        6,
        377,
        44172,
        11,
        439,
        6,
        377,
        44172,
        23287,
        1511,
        26526,
        30697,
        543,
        13,
        17667,
        8529,
        517,
        33627,
        11,
        2107,
        283,
        1301
      ],
      "temperature": 0,
      "avg_logprob": -0.20430452578535704,
      "compression_ratio": 1.6555023923444976,
      "no_speech_prob": 1.5623942317688488e-7
    },
    {
      "id": 764,
      "seek": 520932,
      "start": 5224.08,
      "end": 5230,
      "text": " neanche più le ricerche sull'Amazon di turno, che non si chiama Amazon, il loro servizio,",
      "tokens": [
        408,
        22806,
        10589,
        476,
        21040,
        260,
        1876,
        459,
        285,
        6,
        16833,
        6317,
        1026,
        1261,
        78,
        11,
        947,
        2107,
        1511,
        13228,
        2404,
        6795,
        11,
        1930,
        28810,
        1658,
        590,
        1004,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.20430452578535704,
      "compression_ratio": 1.6555023923444976,
      "no_speech_prob": 1.5623942317688488e-7
    },
    {
      "id": 765,
      "seek": 520932,
      "start": 5230,
      "end": 5234.84,
      "text": " perché loro sanno già che cosa spedirti a casa, perché ti conoscono talmente bene",
      "tokens": [
        14303,
        28810,
        262,
        13484,
        30469,
        947,
        10163,
        637,
        292,
        2498,
        72,
        257,
        9022,
        11,
        14303,
        8757,
        49892,
        45846,
        4023,
        4082,
        2537
      ],
      "temperature": 0,
      "avg_logprob": -0.20430452578535704,
      "compression_ratio": 1.6555023923444976,
      "no_speech_prob": 1.5623942317688488e-7
    },
    {
      "id": 766,
      "seek": 523484,
      "start": 5234.84,
      "end": 5241.04,
      "text": " che lui viene lasciato, porto un esempio, dalla ragazza e dopo tre minuti gli suona",
      "tokens": [
        947,
        8783,
        19561,
        48451,
        2513,
        11,
        1515,
        1353,
        517,
        33627,
        11,
        35566,
        17539,
        921,
        2394,
        308,
        35196,
        2192,
        13951,
        72,
        17161,
        459,
        4037
      ],
      "temperature": 0,
      "avg_logprob": -0.260671591758728,
      "compression_ratio": 1.4943181818181819,
      "no_speech_prob": 1.7805648511171057e-8
    },
    {
      "id": 767,
      "seek": 523484,
      "start": 5241.04,
      "end": 5252.24,
      "text": " il campanello con uno che gli consegna sei birre. Ed è molto divertente, è un librino",
      "tokens": [
        1930,
        2255,
        282,
        11216,
        416,
        8526,
        947,
        17161,
        4425,
        70,
        629,
        10842,
        1904,
        265,
        13,
        3977,
        4873,
        16394,
        23781,
        1576,
        11,
        4873,
        517,
        4939,
        2982
      ],
      "temperature": 0,
      "avg_logprob": -0.260671591758728,
      "compression_ratio": 1.4943181818181819,
      "no_speech_prob": 1.7805648511171057e-8
    },
    {
      "id": 768,
      "seek": 523484,
      "start": 5252.24,
      "end": 5261.32,
      "text": " non troppo impegnativo, che ho veramente divorato, su un mondo distopico, però divertente,",
      "tokens": [
        2107,
        4495,
        27000,
        19643,
        4568,
        18586,
        11,
        947,
        1106,
        50079,
        11861,
        2513,
        11,
        459,
        517,
        40499,
        1483,
        404,
        2789,
        11,
        12673,
        23781,
        1576,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.260671591758728,
      "compression_ratio": 1.4943181818181819,
      "no_speech_prob": 1.7805648511171057e-8
    },
    {
      "id": 769,
      "seek": 526132,
      "start": 5261.32,
      "end": 5265.28,
      "text": " perché tipicamente il mondo distopico è un mondo triste, invece questo è un mondo",
      "tokens": [
        14303,
        4125,
        23653,
        1930,
        40499,
        1483,
        404,
        2789,
        4873,
        517,
        40499,
        33526,
        11,
        36344,
        10263,
        4873,
        517,
        40499
      ],
      "temperature": 0,
      "avg_logprob": -0.2432180633544922,
      "compression_ratio": 1.7109375,
      "no_speech_prob": 4.618433635528163e-8
    },
    {
      "id": 770,
      "seek": 526132,
      "start": 5265.28,
      "end": 5272.799999999999,
      "text": " molto allegro, molto divertente, quindi consiglio a tutti la lettura e per noi del settore ci",
      "tokens": [
        16394,
        10364,
        340,
        11,
        16394,
        23781,
        1576,
        11,
        15727,
        40233,
        19987,
        257,
        19822,
        635,
        20689,
        2991,
        308,
        680,
        22447,
        1103,
        5584,
        418,
        6983
      ],
      "temperature": 0,
      "avg_logprob": -0.2432180633544922,
      "compression_ratio": 1.7109375,
      "no_speech_prob": 4.618433635528163e-8
    },
    {
      "id": 771,
      "seek": 526132,
      "start": 5272.799999999999,
      "end": 5277.24,
      "text": " sono sorrisi in più, perché è un mondo che conosciamo anche da dietro, non solo da",
      "tokens": [
        9259,
        9359,
        5714,
        72,
        294,
        10589,
        11,
        14303,
        4873,
        517,
        40499,
        947,
        49892,
        42052,
        11585,
        1120,
        6339,
        340,
        11,
        2107,
        6944,
        1120
      ],
      "temperature": 0,
      "avg_logprob": -0.2432180633544922,
      "compression_ratio": 1.7109375,
      "no_speech_prob": 4.618433635528163e-8
    },
    {
      "id": 772,
      "seek": 526132,
      "start": 5277.24,
      "end": 5282.84,
      "text": " davanti. Esatto, no, questo me lo recupero, è super interessante, soprattutto ero alla",
      "tokens": [
        11753,
        11520,
        13,
        2313,
        37491,
        11,
        572,
        11,
        10263,
        385,
        450,
        25692,
        78,
        11,
        4873,
        1687,
        24372,
        11,
        50002,
        1189,
        78,
        11591
      ],
      "temperature": 0,
      "avg_logprob": -0.2432180633544922,
      "compression_ratio": 1.7109375,
      "no_speech_prob": 4.618433635528163e-8
    },
    {
      "id": 773,
      "seek": 526132,
      "start": 5282.84,
      "end": 5289.24,
      "text": " ricerca di una lettura leggera. Io invece, ahimè, butto al cesso tutta la leggerezza",
      "tokens": [
        21040,
        36127,
        1026,
        2002,
        20689,
        2991,
        1676,
        1321,
        64,
        13,
        19239,
        36344,
        11,
        3716,
        332,
        1462,
        11,
        457,
        1353,
        419,
        269,
        5557,
        3672,
        1328,
        635,
        30991,
        323,
        26786
      ],
      "temperature": 0,
      "avg_logprob": -0.2432180633544922,
      "compression_ratio": 1.7109375,
      "no_speech_prob": 4.618433635528163e-8
    },
    {
      "id": 774,
      "seek": 528924,
      "start": 5289.24,
      "end": 5297.639999999999,
      "text": " che ci ha portato Davide, per invece condividere con voi una roba molto figa. Allora, voi sapete,",
      "tokens": [
        947,
        6983,
        324,
        2436,
        2513,
        3724,
        482,
        11,
        680,
        36344,
        2224,
        1843,
        323,
        416,
        20931,
        2002,
        3870,
        64,
        16394,
        2147,
        64,
        13,
        1057,
        3252,
        11,
        20931,
        18985,
        3498,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.2651873826980591,
      "compression_ratio": 1.4656084656084656,
      "no_speech_prob": 5.3157830137706696e-8
    },
    {
      "id": 775,
      "seek": 528924,
      "start": 5297.639999999999,
      "end": 5304.76,
      "text": " vi ho già rotto le balle su questo che sto provando con mediocre successo, devo dire,",
      "tokens": [
        1932,
        1106,
        30469,
        4297,
        1353,
        476,
        2594,
        68,
        459,
        10263,
        947,
        22784,
        1439,
        1806,
        416,
        45415,
        2245,
        78,
        11,
        49717,
        1264,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.2651873826980591,
      "compression_ratio": 1.4656084656084656,
      "no_speech_prob": 5.3157830137706696e-8
    },
    {
      "id": 776,
      "seek": 528924,
      "start": 5304.76,
      "end": 5311.32,
      "text": " a studiare Rust per uno sviluppatore che viene dal mondo JavaScript e prima ancora dal mondo",
      "tokens": [
        257,
        972,
        72,
        543,
        34952,
        680,
        8526,
        17342,
        388,
        10504,
        43148,
        947,
        19561,
        11702,
        40499,
        15778,
        308,
        19507,
        30656,
        11702,
        40499
      ],
      "temperature": 0,
      "avg_logprob": -0.2651873826980591,
      "compression_ratio": 1.4656084656084656,
      "no_speech_prob": 5.3157830137706696e-8
    },
    {
      "id": 777,
      "seek": 531132,
      "start": 5311.32,
      "end": 5319.32,
      "text": " PHP, probabilmente tipo Rust è, non lo so, l'uomo nero dei racconti da bambino, cioè",
      "tokens": [
        47298,
        11,
        31959,
        4082,
        9746,
        34952,
        4873,
        11,
        2107,
        450,
        370,
        11,
        287,
        6,
        84,
        13395,
        297,
        2032,
        13874,
        4129,
        9000,
        72,
        1120,
        272,
        2173,
        2982,
        11,
        41827
      ],
      "temperature": 0,
      "avg_logprob": -0.20800205608745953,
      "compression_ratio": 1.4710743801652892,
      "no_speech_prob": 3.769454082203083e-8
    },
    {
      "id": 778,
      "seek": 531132,
      "start": 5319.32,
      "end": 5326.24,
      "text": " una roba che ti spaventa a morte e siccome la paura non è mai troppa, ho beccato questa",
      "tokens": [
        2002,
        3870,
        64,
        947,
        8757,
        637,
        706,
        8938,
        257,
        37392,
        308,
        33579,
        1102,
        635,
        2502,
        2991,
        2107,
        4873,
        12698,
        4495,
        34827,
        11,
        1106,
        312,
        1914,
        2513,
        16540
      ],
      "temperature": 0,
      "avg_logprob": -0.20800205608745953,
      "compression_ratio": 1.4710743801652892,
      "no_speech_prob": 3.769454082203083e-8
    },
    {
      "id": 779,
      "seek": 531132,
      "start": 5326.24,
      "end": 5333.299999999999,
      "text": " fonte, questa risorsa che è molto figa. Allora, sto studiando Rust perché voglio avvicinarmi",
      "tokens": [
        283,
        10219,
        11,
        16540,
        2253,
        38822,
        947,
        4873,
        16394,
        2147,
        64,
        13,
        1057,
        3252,
        11,
        22784,
        972,
        72,
        1806,
        34952,
        14303,
        31273,
        19987,
        1305,
        25537,
        6470,
        3057
      ],
      "temperature": 0,
      "avg_logprob": -0.20800205608745953,
      "compression_ratio": 1.4710743801652892,
      "no_speech_prob": 3.769454082203083e-8
    },
    {
      "id": 780,
      "seek": 531132,
      "start": 5333.299999999999,
      "end": 5339,
      "text": " alla system programming, però nel contempo io ho fatto alcune parti dell'università",
      "tokens": [
        11591,
        1185,
        9410,
        11,
        12673,
        15373,
        660,
        443,
        2259,
        19785,
        1106,
        23228,
        20005,
        2613,
        24408,
        19781,
        6,
        41421,
        12445
      ],
      "temperature": 0,
      "avg_logprob": -0.20800205608745953,
      "compression_ratio": 1.4710743801652892,
      "no_speech_prob": 3.769454082203083e-8
    },
    {
      "id": 781,
      "seek": 533900,
      "start": 5339,
      "end": 5345.68,
      "text": " un po' alla cazzo di cane, speciale a parte di algoritmi, nel senso, l'esame di APAL",
      "tokens": [
        517,
        714,
        6,
        11591,
        269,
        921,
        4765,
        1026,
        27518,
        11,
        2121,
        68,
        257,
        6975,
        1026,
        3501,
        50017,
        3057,
        11,
        15373,
        3151,
        539,
        11,
        287,
        6,
        279,
        529,
        1026,
        5372,
        3427
      ],
      "temperature": 0,
      "avg_logprob": -0.2582286516825358,
      "compression_ratio": 1.51931330472103,
      "no_speech_prob": 1.040779338268294e-7
    },
    {
      "id": 782,
      "seek": 533900,
      "start": 5345.68,
      "end": 5352.12,
      "text": " ho passato, ma se mi chiedete come, non lo so, non ne ho minimamente idea. Quindi ha",
      "tokens": [
        1106,
        1320,
        2513,
        11,
        463,
        369,
        2752,
        417,
        1091,
        3498,
        808,
        11,
        2107,
        450,
        370,
        11,
        2107,
        408,
        1106,
        4464,
        3439,
        1558,
        13,
        32534,
        324
      ],
      "temperature": 0,
      "avg_logprob": -0.2582286516825358,
      "compression_ratio": 1.51931330472103,
      "no_speech_prob": 1.040779338268294e-7
    },
    {
      "id": 783,
      "seek": 533900,
      "start": 5352.12,
      "end": 5357.88,
      "text": " senso per me riprendere in mano gli algoritmi e siccome sto studiando Rust, come vi dicevo,",
      "tokens": [
        3151,
        539,
        680,
        385,
        12782,
        4542,
        323,
        294,
        18384,
        17161,
        3501,
        50017,
        3057,
        308,
        33579,
        1102,
        22784,
        972,
        72,
        1806,
        34952,
        11,
        808,
        1932,
        10313,
        3080,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.2582286516825358,
      "compression_ratio": 1.51931330472103,
      "no_speech_prob": 1.040779338268294e-7
    },
    {
      "id": 784,
      "seek": 533900,
      "start": 5357.88,
      "end": 5366.2,
      "text": " ho beccato questo repository che è una figata, pazzesca. Si chiama The Algorithms, esistono",
      "tokens": [
        1106,
        312,
        1914,
        2513,
        10263,
        25841,
        947,
        4873,
        2002,
        2147,
        3274,
        11,
        280,
        9112,
        279,
        496,
        13,
        4909,
        13228,
        2404,
        440,
        35014,
        6819,
        2592,
        11,
        785,
        468,
        8957
      ],
      "temperature": 0,
      "avg_logprob": -0.2582286516825358,
      "compression_ratio": 1.51931330472103,
      "no_speech_prob": 1.040779338268294e-7
    },
    {
      "id": 785,
      "seek": 536620,
      "start": 5366.2,
      "end": 5371.639999999999,
      "text": " delle versioni in linguaggi diversi, ma la versione che vi voglio raccontare oggi è",
      "tokens": [
        16485,
        3037,
        72,
        294,
        21766,
        46893,
        6111,
        72,
        11,
        463,
        635,
        3037,
        68,
        947,
        1932,
        31273,
        19987,
        4129,
        9000,
        543,
        34768,
        4873
      ],
      "temperature": 0,
      "avg_logprob": -0.24117985338267714,
      "compression_ratio": 1.75,
      "no_speech_prob": 9.62563646567105e-8
    },
    {
      "id": 786,
      "seek": 536620,
      "start": 5371.639999999999,
      "end": 5379.8,
      "text": " la versione degli algoritmi sviluppati in Rust e ci sono gli algoritmi di ordinamento,",
      "tokens": [
        635,
        3037,
        68,
        32079,
        3501,
        50017,
        3057,
        17342,
        388,
        10504,
        6908,
        294,
        34952,
        308,
        6983,
        9259,
        17161,
        3501,
        50017,
        3057,
        1026,
        25376,
        8824,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.24117985338267714,
      "compression_ratio": 1.75,
      "no_speech_prob": 9.62563646567105e-8
    },
    {
      "id": 787,
      "seek": 536620,
      "start": 5379.8,
      "end": 5387.28,
      "text": " quelli sui grafi, gli algoritmi di dynamic programming, ci sono le data structure, ci",
      "tokens": [
        631,
        16320,
        459,
        72,
        1295,
        13325,
        11,
        17161,
        3501,
        50017,
        3057,
        1026,
        8546,
        9410,
        11,
        6983,
        9259,
        476,
        1412,
        3877,
        11,
        6983
      ],
      "temperature": 0,
      "avg_logprob": -0.24117985338267714,
      "compression_ratio": 1.75,
      "no_speech_prob": 9.62563646567105e-8
    },
    {
      "id": 788,
      "seek": 536620,
      "start": 5387.28,
      "end": 5395.44,
      "text": " sono gli algoritmi di ricerca, c'è veramente un gozziliardo. Se avete bisogno",
      "tokens": [
        9259,
        17161,
        3501,
        50017,
        3057,
        1026,
        21040,
        36127,
        11,
        269,
        6,
        1462,
        50079,
        517,
        352,
        4313,
        2312,
        12850,
        13,
        1100,
        48201,
        40505,
        1771
      ],
      "temperature": 0,
      "avg_logprob": -0.24117985338267714,
      "compression_ratio": 1.75,
      "no_speech_prob": 9.62563646567105e-8
    },
    {
      "id": 789,
      "seek": 539544,
      "start": 5395.44,
      "end": 5403.4,
      "text": " di una lettura un pochino più pesante, ecco, qua probabilmente avete il repository che",
      "tokens": [
        1026,
        2002,
        20689,
        2991,
        517,
        714,
        339,
        2982,
        10589,
        9262,
        2879,
        11,
        11437,
        1291,
        11,
        24159,
        31959,
        4082,
        48201,
        1930,
        25841,
        947
      ],
      "temperature": 0,
      "avg_logprob": -0.24644841645893298,
      "compression_ratio": 1.4946236559139785,
      "no_speech_prob": 1.0907279346383802e-7
    },
    {
      "id": 790,
      "seek": 539544,
      "start": 5403.4,
      "end": 5408.919999999999,
      "text": " fa per voi. Io comunque consiglio sempre il libro di Davide che mi sembra molto più interessante",
      "tokens": [
        2050,
        680,
        20931,
        13,
        19239,
        45736,
        40233,
        19987,
        9553,
        1930,
        29354,
        1026,
        3724,
        482,
        947,
        2752,
        20775,
        424,
        16394,
        10589,
        24372
      ],
      "temperature": 0,
      "avg_logprob": -0.24644841645893298,
      "compression_ratio": 1.4946236559139785,
      "no_speech_prob": 1.0907279346383802e-7
    },
    {
      "id": 791,
      "seek": 539544,
      "start": 5408.919999999999,
      "end": 5415.639999999999,
      "text": " del mio balocco o almeno molto più divertente, però insomma, volevo condividere con voi...",
      "tokens": [
        1103,
        29908,
        3119,
        905,
        1291,
        277,
        419,
        43232,
        16394,
        10589,
        23781,
        1576,
        11,
        12673,
        1028,
        30243,
        11,
        49877,
        3080,
        2224,
        1843,
        323,
        416,
        20931,
        485
      ],
      "temperature": 0,
      "avg_logprob": -0.24644841645893298,
      "compression_ratio": 1.4946236559139785,
      "no_speech_prob": 1.0907279346383802e-7
    },
    {
      "id": 792,
      "seek": 541564,
      "start": 5415.64,
      "end": 5430.88,
      "text": " Anche lì dipende comunque... volevo condividere esatto con voi questa cosa. Davide, mi ha",
      "tokens": [
        1107,
        1876,
        287,
        4749,
        10460,
        5445,
        45736,
        485,
        49877,
        3080,
        2224,
        1843,
        323,
        785,
        37491,
        416,
        20931,
        16540,
        10163,
        13,
        3724,
        482,
        11,
        2752,
        324
      ],
      "temperature": 0,
      "avg_logprob": -0.30210123390987004,
      "compression_ratio": 1.3875968992248062,
      "no_speech_prob": 1.6893520182748034e-7
    },
    {
      "id": 793,
      "seek": 541564,
      "start": 5430.88,
      "end": 5440.92,
      "text": " fatto superissimo piacere averti qua con noi, veramente un iper iper iper piacere. Anche",
      "tokens": [
        23228,
        1687,
        34966,
        3895,
        326,
        323,
        257,
        3281,
        72,
        24159,
        416,
        22447,
        11,
        50079,
        517,
        741,
        610,
        741,
        610,
        741,
        610,
        3895,
        326,
        323,
        13,
        1107,
        1876
      ],
      "temperature": 0,
      "avg_logprob": -0.30210123390987004,
      "compression_ratio": 1.3875968992248062,
      "no_speech_prob": 1.6893520182748034e-7
    },
    {
      "id": 794,
      "seek": 544092,
      "start": 5440.92,
      "end": 5446.56,
      "text": " a me mi sono proprio divertito un sacco, poi come già ben mi conosci, mi piace parlare,",
      "tokens": [
        257,
        385,
        2752,
        9259,
        28203,
        23781,
        3528,
        517,
        4899,
        1291,
        11,
        19260,
        808,
        30469,
        3271,
        2752,
        49892,
        537,
        11,
        2752,
        50062,
        13734,
        543,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.31005936134152295,
      "compression_ratio": 1.4972375690607735,
      "no_speech_prob": 4.075738146980257e-8
    },
    {
      "id": 795,
      "seek": 544092,
      "start": 5446.56,
      "end": 5457.28,
      "text": " quindi saremmo potuti stare qualche altra ora qua in chiacchiere. Grazie di tutto e poi",
      "tokens": [
        15727,
        38706,
        2174,
        78,
        1847,
        29161,
        22432,
        38737,
        4955,
        424,
        33714,
        24159,
        294,
        13228,
        326,
        8036,
        323,
        13,
        8985,
        3283,
        1026,
        23048,
        308,
        19260
      ],
      "temperature": 0,
      "avg_logprob": -0.31005936134152295,
      "compression_ratio": 1.4972375690607735,
      "no_speech_prob": 4.075738146980257e-8
    },
    {
      "id": 796,
      "seek": 544092,
      "start": 5457.28,
      "end": 5464.56,
      "text": " ci rivedremo presto, intanto con te spero presto di persona e poi adesso seguirò assiduamente",
      "tokens": [
        6983,
        367,
        3194,
        44172,
        16305,
        78,
        11,
        560,
        5857,
        416,
        535,
        24152,
        78,
        16305,
        78,
        1026,
        12184,
        308,
        19260,
        39552,
        18584,
        4293,
        1256,
        327,
        84,
        3439
      ],
      "temperature": 0,
      "avg_logprob": -0.31005936134152295,
      "compression_ratio": 1.4972375690607735,
      "no_speech_prob": 4.075738146980257e-8
    },
    {
      "id": 797,
      "seek": 546456,
      "start": 5464.56,
      "end": 5474.04,
      "text": " il podcast per vedere un po' le persone che passano e che sono passate. Grazie, grazie",
      "tokens": [
        1930,
        7367,
        680,
        35373,
        517,
        714,
        6,
        476,
        29944,
        947,
        1320,
        3730,
        308,
        947,
        9259,
        1320,
        473,
        13,
        8985,
        3283,
        11,
        1295,
        3283
      ],
      "temperature": 0,
      "avg_logprob": -0.23046832614474827,
      "compression_ratio": 1.511111111111111,
      "no_speech_prob": 7.690228471801674e-7
    },
    {
      "id": 798,
      "seek": 546456,
      "start": 5474.04,
      "end": 5482.92,
      "text": " di nuovo. Noi immagino che ci vedremo prestissimo, grazie davvero di essere venuti qua. Ricordo",
      "tokens": [
        1026,
        49348,
        13,
        883,
        72,
        3397,
        559,
        2982,
        947,
        6983,
        14267,
        44172,
        16305,
        34966,
        11,
        1295,
        3283,
        11753,
        39332,
        1026,
        19799,
        6138,
        29161,
        24159,
        13,
        21215,
        23872
      ],
      "temperature": 0,
      "avg_logprob": -0.23046832614474827,
      "compression_ratio": 1.511111111111111,
      "no_speech_prob": 7.690228471801674e-7
    },
    {
      "id": 799,
      "seek": 546456,
      "start": 5482.92,
      "end": 5491.360000000001,
      "text": " rapidamente abbiamo avuto con noi il super Davide Fiorello, il decano di noi nearformisti",
      "tokens": [
        7558,
        3439,
        22815,
        1305,
        8262,
        416,
        22447,
        1930,
        1687,
        3724,
        482,
        38245,
        418,
        1913,
        11,
        1930,
        979,
        3730,
        1026,
        22447,
        2651,
        837,
        45308
      ],
      "temperature": 0,
      "avg_logprob": -0.23046832614474827,
      "compression_ratio": 1.511111111111111,
      "no_speech_prob": 7.690228471801674e-7
    },
    {
      "id": 800,
      "seek": 549136,
      "start": 5491.36,
      "end": 5499.2,
      "text": " o nearformer, un abbraccio grandissimo Davide. Ringraziando di nuovo Davide, io vi ricordo",
      "tokens": [
        277,
        2651,
        837,
        260,
        11,
        517,
        410,
        1443,
        326,
        8529,
        2697,
        34966,
        3724,
        482,
        13,
        19844,
        424,
        3992,
        1806,
        1026,
        49348,
        3724,
        482,
        11,
        19785,
        1932,
        21040,
        23872
      ],
      "temperature": 0,
      "avg_logprob": -0.29748748451150875,
      "compression_ratio": 1.4894736842105263,
      "no_speech_prob": 0.0000029944242214696715
    },
    {
      "id": 801,
      "seek": 549136,
      "start": 5499.2,
      "end": 5506.679999999999,
      "text": " rapidamente i nostri contatti, infochiocciola.it, brandrepo in modo canonico, il classico, l'immancabile",
      "tokens": [
        7558,
        3439,
        741,
        10397,
        470,
        660,
        21515,
        11,
        13614,
        8036,
        905,
        537,
        4711,
        13,
        270,
        11,
        3360,
        265,
        2259,
        294,
        16664,
        21985,
        2789,
        11,
        1930,
        1508,
        2789,
        11,
        287,
        6,
        332,
        1601,
        66,
        33288
      ],
      "temperature": 0,
      "avg_logprob": -0.29748748451150875,
      "compression_ratio": 1.4894736842105263,
      "no_speech_prob": 0.0000029944242214696715
    },
    {
      "id": 802,
      "seek": 549136,
      "start": 5506.679999999999,
      "end": 5516.5199999999995,
      "text": " gruppo telegram. Se avete del tempo aprite iTunes, andate tra i podcast e metteteci una",
      "tokens": [
        47477,
        78,
        4304,
        1342,
        13,
        1100,
        48201,
        1103,
        8972,
        10992,
        642,
        33017,
        11,
        293,
        473,
        944,
        741,
        7367,
        308,
        27812,
        3498,
        537,
        2002
      ],
      "temperature": 0,
      "avg_logprob": -0.29748748451150875,
      "compression_ratio": 1.4894736842105263,
      "no_speech_prob": 0.0000029944242214696715
    },
    {
      "id": 803,
      "seek": 551652,
      "start": 5516.52,
      "end": 5523.160000000001,
      "text": " bella stellina e se vi va anche lasciateci una recensione. Potrebbe non essere importante,",
      "tokens": [
        312,
        3505,
        30787,
        1426,
        308,
        369,
        1932,
        2773,
        11585,
        48451,
        473,
        537,
        2002,
        850,
        3378,
        68,
        13,
        9145,
        39487,
        2107,
        19799,
        9416,
        11
      ],
      "temperature": 0,
      "avg_logprob": -0.2212013028702646,
      "compression_ratio": 1.6036036036036037,
      "no_speech_prob": 0.0000013287739193401649
    },
    {
      "id": 804,
      "seek": 551652,
      "start": 5523.160000000001,
      "end": 5531.8,
      "text": " ma per noi lo è, nel senso che grazie a questo riusciamo a tenere saldo la nostra posizione",
      "tokens": [
        463,
        680,
        22447,
        450,
        4873,
        11,
        15373,
        3151,
        539,
        947,
        1295,
        3283,
        257,
        10263,
        367,
        4872,
        42052,
        257,
        2064,
        323,
        1845,
        2595,
        635,
        34311,
        1366,
        35740
      ],
      "temperature": 0,
      "avg_logprob": -0.2212013028702646,
      "compression_ratio": 1.6036036036036037,
      "no_speech_prob": 0.0000013287739193401649
    },
    {
      "id": 805,
      "seek": 551652,
      "start": 5531.8,
      "end": 5539.120000000001,
      "text": " nelle classifiche di iTunes per il podcasting, che non è una questione di orgoglio o di",
      "tokens": [
        46350,
        1508,
        351,
        9304,
        1026,
        33017,
        680,
        1930,
        7367,
        278,
        11,
        947,
        2107,
        4873,
        2002,
        1168,
        68,
        1026,
        14045,
        664,
        19987,
        277,
        1026
      ],
      "temperature": 0,
      "avg_logprob": -0.2212013028702646,
      "compression_ratio": 1.6036036036036037,
      "no_speech_prob": 0.0000013287739193401649
    },
    {
      "id": 806,
      "seek": 551652,
      "start": 5539.120000000001,
      "end": 5544,
      "text": " dire noi siamo più fighi degli altri, ma è questione di raggiungere più orecchie",
      "tokens": [
        1264,
        22447,
        33459,
        10589,
        283,
        910,
        72,
        32079,
        33707,
        11,
        463,
        4873,
        1168,
        68,
        1026,
        17539,
        7834,
        1063,
        323,
        10589,
        277,
        13867,
        339,
        414
      ],
      "temperature": 0,
      "avg_logprob": -0.2212013028702646,
      "compression_ratio": 1.6036036036036037,
      "no_speech_prob": 0.0000013287739193401649
    },
    {
      "id": 807,
      "seek": 554400,
      "start": 5544,
      "end": 5548.12,
      "text": " possibili. Detto questo, appuntamento alla prossima settimana. Ciao!",
      "tokens": [
        50364,
        24145,
        72,
        13,
        4237,
        1353,
        10263,
        11,
        724,
        2760,
        8824,
        11591,
        48794,
        4775,
        5584,
        36497,
        13,
        28473,
        0,
        50570
      ],
      "temperature": 0,
      "avg_logprob": -0.43647884187244235,
      "compression_ratio": 0.9855072463768116,
      "no_speech_prob": 0.000012606793461600319
    },
    {
      "id": 808,
      "seek": 557400,
      "start": 5574,
      "end": 5580.56,
      "text": " Sottotitoli e revisione a cura di QTSS",
      "tokens": [
        50364,
        318,
        1521,
        310,
        270,
        9384,
        308,
        34218,
        68,
        257,
        1262,
        64,
        1026,
        1249,
        7327,
        50,
        50692
      ],
      "temperature": 0,
      "avg_logprob": -0.5408271153767904,
      "compression_ratio": 0.8260869565217391,
      "no_speech_prob": 0.9715107679367065
    }
  ],
  "language": "it"
}